\section{Abstract}
%
In order to map perceptual or cognitive functions onto brain areas of study
participants, researchers usually conduct dedicated experiments often
accompanied with a task (so called ``functional localizer'').
%
Nevertheless, the approach ``one paradigm, one brain function'' becomes
unfeasible if one wants to map a variety of functions in a time-efficient
manner.
%
We explored a way to project brain mapping data (statistical z-maps) from a
reference group onto individual brains of study participants after performing a
functional aligning of participants with a common model space.
%
Data were obtained from conducting a functional localizer paradigm and two
paradigms using naturalistic stimulation during functional magnetic resonance
imaging (fMRI):
%
participants took part in a task-based, block-design visual localizer, and
participants were watching an audio-visual movie and listening to the movie's
audio-description, both paradigms free of any task.
%
Based on these data, we created a common model space employing a \ac{srm}
\citep{chen2015reduced}.
%
On the one hand, the common model space allows denoising data from individuals
that were used to create the common model space.
%
On the other hand, data from left-out subjects can be aligned with the common
model space by using a (preferably short) segment of a naturalistic stimulus as
a ``diagnostic run'', a process that provides a subject-specific transformation
matrices.
%
The inverse of the acquired transformation matrices can then be used to project
data from other paradigms aligned in common space onto the brain of the left-out
subjects.
%
The general goal of the project is to assess the required length of the
diagnostic run, and to compare empirical z-maps with z-maps that were predicted
from other participants' data.
%
We present preliminary results of predicted z-maps gained from the same paradigm
as the diagnostic run as well as across paradigms.


\section{Introduction}
% brain mapping
topographic brain mapping maps brain functions, perceptual or cognitive
processes, onto brain areas.
% localizer
Usually, this is done by letting study participants perform a task during a so
called ``functional localizer''.
% problem: one localizer for one domain
The second problem is: you usually need one localizer for one domain of brain
functions
% which gets messy
Which means: if you want to map a variety of domains, you need a variety of
localizers, and then the whole approach gets time-consuming and inefficient
(localizer batteries; e.g. Thirion's work)
% problem: compliance
Even more severe in a clinical population: they mostly need the participants to
perform a task, and they are just plain boring which often leads to a diminished
compliance.


\subsection{Our approach}
% main idea
The main idead ist to predict location, size, and shape of a functional area in
an individual person based on location, size, and shape of the same functional
area in persons of a reference group.
%
Based on a prediction of a reference group's data, the diagnostic procedure
could be more reliable and valid but also more time-efficient and thus more
affordable.
%
We can do this in basically two ways:
a) we do this by performing an anatomical alignment, or
%
which is a new a approach:
%
b) we can do this by performing a functional alignment
%

but when we want to predict functional areas from anatomically aligned brains,
we are running into the problem of poor
functional-anatomical correspondence.

Poor functional-anatomical correspondence means:
even if we assume that two persons have anatomically identical brains, still,
the location, size \& shape of the functional area is probably different.


predict brain-patterns across subjects \& across an experiment?

\begin{itemize}
    \item 1. create CMS from brain patterns (during natural stimulation ) in
        reference group

    \item 2. project localizer data of reference group into CMS

    \item 3. align left-out subject with CMS using a varying amount of data (1-8
        runs je per stim.) data (parts) need to be of the same stimulation used
        to create the CMS (cause we align time-series not connectivity profiles)
        result: transformation matrix

    \item 4. project localizer data from CMS into left-out subject  using the
        inverse matrix from step 3

    \item 5. compare predicted localizer results vs. empirical results

    \item 6. compare steps 1-5 to a similar procedure using anatomical alignment

\end{itemize}

\subsubsection{anatomical alignment}

% anatomical alignment
In general, a individual's diagnostic could be based on a prediction based on
the anatomical location of the functional area in the reference group (e.g.
\citet{weiner2018defining}.
%
- anatomical alignment aligns vertices in 2-dimensional space or voxels in
3-dimensional anatomical space
%
we transform the shape of an individual brain into the shape of an average,
standard brain

\subsubsection{Shared response model}

from \citep{cohen2017computational}: ''One such technique is shared response
modeling (SRM65; see also hyperalignment66), which projects fMRI responses from
each participant into a low-dimensional space that captures temporal variance
shared across participants (Box 1; Fig. 4–5).
%
If participants are given the same stimulus or task sequence (for example, a
movie), which leads their brains through a series of cognitive states (for
example, visual, auditory, semantic), then identifying shared variance has the
effect of highlighting variance related to these states.
%
An added benefit is that SRM helps address the data starvation problem above:
because the SRM space is by definition shared across individuals, data from
multiple participants can be combined prior to MVPA or other analyses.
%
Cross-participant decoding is possible without SRM67–70 but may be limited to
cognitive states whose neural representations are coarse and thus tolerant of
misalignment.
%
Indeed, SRM improves MVPA precisely by aligning fine-grained spatial patterns
within local regions71.
%
Moreover, beyond improving alignment and increasing the sensitivity of other
analyses, the output of SRM itself can be informative.
%
For example, it has been used to estimate the dimensionality with which the
posterior medial cortex represents movies during perception and memory recall72
`` \citep{cohen2017computational}.

%
''The flip side of focusing on shared responses is to focus on responses that
are idiosyncratic to individuals.
%
Although these responses are excluded in SRM, they are not necessarily noise and
may in fact be highly reliable within participants.
%
Indeed, SRM can be used to iso- late participant-unique responses by examining
the residuals after removing shared group responses, or it can be applied
hierarchically to the residuals to identify subgroups65.
%
More generally, there is a growing trend toward investigating individual
differences as another source of meaningful variance in fMRI73.
%
Recognizing that signal exists beyond the average or shared response of a group,
such studies exploit idiosyncratic but stable responses to account for
previously unexplained variance in brain function, behavioral performance and
clinical measures70,74'' \citep{cohen2017computational}.

''Figure 4 Process for SRM. fMRI data are collected from each of m participants
experiencing the same stimulus and then organized into a matrix X (voxels by
time). Each matrix X is then factored using a probabilistic latent-factor model
into the product of a subject-specific matrix W of k brain maps (an orthogonal
basis) and a shared temporal response matrix S of size k by time. That is, for
each participant: X = W S + R, where X, W, and the residuals, R (not shown), are
subject-specific, and S is shared across participantis``
\citep{cohen2017computational}.

''After standard alignment, fMRI data can be aggregated at the group level by
averaging the values at each voxel across participants.
%
Although this reduces intersubject noise, variation in the anatomical locations
of functional signals across participants blurs estimates of their shared
responses.
%
SRM offers an alternative approach by jointly factoring each participant’s data
into a shared set of feature time series and subject-specific topographies for
each feature (Fig. 4).
%
The simplest use of SRM is for extracting a shared response across an anatomical
ROI. Used this way, SRM and related methods can yield significant gains in
sensitivity for group-level inference65,66.
%
For example, which short segment of a movie is being watched can be classified
with many times greater accuracy from fMRI data after functional versus
anatomical alignment65,71.
%
Moreover, text annotations of movie segments based on fMRI are consistently
better, across ROIs and analysis parameters, after SRM98.
%
Applying SRM to a large swath of the brain means that all voxels within the
region contribute to the final derived metric.
%
This can conflict with the goal of associating spatially local activity with
specific cognitive functions. To address such issues, SRM can be applied in
small overlapping searchlights to obtain localized metrics of shared
information71,99 \citep{cohen2017computational}``.

%
''SRM is computed using a subset of the available fMRI data, with the number of
features, k, determined using cross-validation.
%
Naturalistic stimuli such as movies and stories are often used to generate such
training data, though any study design in which participants perform the same
sequence of trials—or for which a common sequence can be spliced together from
the same set of trials—could be used (for example, a battery of cognitive
tasks).
%
SRM highlights the sources of variance elicited by the stimuli or trials that
are shared across participants in the training data.
%
Held-out test data (including from new participants) are then projected into the
shared response space for further analysis.
%
Such test data could be of the same type as the training data, for example,
allowing for decoding of new movie segments (Fig.  5).
%
Alternatively, the test data could be from controlled laboratory
experiments—there is no requirement for a common trial sequence or set, unlike
for the training data—with SRM simply replacing standard alignment in the
preprocessing pipeline.
%
As a rule of thumb, SRM will improve sensitivity for detecting a cognitive
process of interest in the test data if the training stimuli or trials strongly
and variably engage that process in a way that is reliable across participants.
%
One limitation when using SRM for preprocessing is that additional data must be
collected for training, reducing the amount of data (and potentially statistical
power) related to the principal question of the study``
\citep{cohen2017computational}.

\paragraph{procedure}
%
\begin{itemize}
    \item conduct the localizer experiment to get the brain responses in the
    reference group
    \item align the brains in that reference group anatomically to the standard
    brain
    \item also align your left-out subject to the standard brain, which gives
    you a transformation matrix
    \item then you use the inverse of that transformation matrix to project the
    data
    \item from the reference group into the anatomy of the left-out subject,
    which is essentially the prediction
\end{itemize}


\paragraph{but: functional-anatomical correspondence}
% shitty
The approach relying on merely anatomical alignment is shitty because of modest
functional-anatomical correspondence.
% explain functional-anatomical correspondence
A modest functional-anatomical correspondence means that even if one would assume
two persons had an identically shaped brain anatomy the location, size, and
shape of a functional area would still not be indetical.

% PPA example
Here, as an example the location of the ``PPA'', the Parahippocampal Place Area.
which is more active when we look at pictures of scenes or landscapes.
%
location, shape, size of the PPA in 14 subjects,
%
the brighter the voxel the more subjects have their PPA in that location.
%
location is roughly similar across persons, but still, the plot shows that there
is individual variation.


\paragraph{individual differences}
%
do you find differences in individual brain patterns or do differences stem from
differences in functional-anatomical correspondence?
%
vice versa: we might find no differences because of poor functional-anatomical
correspondence


\subsubsection{functional alignment}
%
given that we do not want to predict brain anatomy but predict an accurate
mapping of brain functions onto the brain anatomy:
%
wouldn't it make more sense to not perform an anatomical alignment but a
functional alignment?
%
Functional alignment aligns cortical patterns (time-series or connectivity
profiles) in a multi-dimensional function space.
% reference
The reference is not an average brain anatomy but a so called \ac{cms}.


\paragraph{Hyperalignment}
%
There are two algorithms dominating the field: hyperalignment and \ac{srm}


\paragraph{Shared response model}
%
\todo[inline]{why did we chose SRM?}


\paragraph{procedure}
%
You let study participants watch a naturalistic stimulus like a movie or an
audio-book...  ...which are continuous, naturally engaging, and are triggering a
wide variety of time-locked brain functions
%
From that stimuli, you get the time-series per voxel and per subject, and what
the algorithm essentially does is: it learns responses that are shared across
participants (which is the common model space)
%
but more importantly, the algorithm also learns individual transformation
matrices.
%
These transformation matrices can be used to project data into and out of the
Common Model Space.
For the prediction through a common model space, we first needed to create it
%
We let participants watch the movie and audio-description of Forrest Gump,
assuming that the naturalistic stimuli trigger, among others, brain responses
that a similar to those triggered by the functional localizer
%
Usually when aligning brain responses (and not connectivity profiles) you would
let the left-out subject watch the same and the whole stimuli that were used to
create the Common Model Space to get the transformation matrices.

between-subject cross-validation (=external validation) vs. within-subject cross-validation)
external validation (testing predictive model performance on an idependently
collected dataset)

external validation: testing a predictive model performance on an independently
collected dataset.\todo{means: use localizer TRs to predict localizer???}

\subsubsection{our current study: overview, blabla}
%
focussing on the domain of so called ``higher-visual, category-selective
areas''
%
How do you usually map these category-selective areas onto brains of individual
study participants?
%
You conduct a functional localizer experiments, lasting about 20 minutes,
%
You let the participants watch pictures of different categories, while also
letting them perform a task to force them to pay attention to picture, after
picture, after picture
%
After having collecting the data you do some statistics, and as a result you
get statistical maps, t- or z-map, that essentially tell you where the
higher-visual areas are located
%
If we need about 20 minutes for just one domain, why don’t we predict the
location of a functional area in one person from the location that we found in
other persons?


\subsubsection{our questions}
% questions of current study
In the current study, we asked ourselves two questions:
%
1) does functional alignment based on a the \ac{srm} improve the prediction
compared to a prediction based on merely anatomical alignment?
%
2) how much data do we need to align an individual subject to a/the \ac{cms} and
outperform a prediction based on anatomical alignment? Is a short diagnostic run
``sufficient''?
%
We wanted to predict the location of the PPA based on data from a reference
group
%
And we wanted to compare the prediction performance based on anatomical
alignment \& functional alignment
%
- goal: align left-out subject to model space using varying amount of data of
movie or audiobook - transform localizer results into left-out subject
%
Hence, the real question that we asked yourselves was: How many minutes of a
naturalistic stimulus are necessary to perform what we call a „partial
alignment“.
%
So, we only used parts of the naturalistic stimuli to let the algorithm learn
the transformation matrices for the left-out subject
%
and we tested which amount of data is needed to do an alignment to the common
model space that provides transformation matrices that outperform a prediction
using just an anatomical alignment.


\section{Methods}

\todo[inline]{taken from naturalistic PPA paper}

% intro
We used components of the publicly available
\href{http://www.studyforrest.org}{studyforrest.org} dataset that has
been repeatedly used by other research groups in independent studies
(\citep[e.g.,][]{ben2018hippocampal, jiahui2019predicting, hu2017decoding,
lettieri2019emotionotopy, nguyen2016integration}).
% used studies
The same participants were
% AD
a) listening to the audio-description \citep{hanke2014audiomovie} of
the movie ``Forrest Gump'',
% AV
b) watching the audio-visual movie \citep{hanke2016simultaneous}, and
% VIS
c) participating in a dedicated six-category block-design visual localizer
\citep{sengupta2016extension}.
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications. Following is a summary of the most important aspects.


\subsection{Participants}

\todo[inline]{taken from naturalistic PPA paper}

% we get the data from the naturalistic PPA paper (its subdataset)
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in\_bold3Tp2/sub-??\_task-a?movie\_run-?\_bold*.*


% AD study
In the audio-description study \citep{hanke2014audiomovie}, 20 German native
speakers (all right-handed, age 21–38 years, mean age 26.6 years, 12 male)
listened to the German audio-description \citep{ForrestGumpGermanAD} of the
movie ``Forrest Gump'' \citep{ForrestGumpMovie}.
% AV study
In the movie study \citep{hanke2016simultaneous}, 15 participants (21–39 years,
mean age 29.4, six female), a subgroup of the prior audio-description study,
watched the audio-visual movie with dubbed German audio track
\citep{ForrestGumpDVD}.
% VIS study
In the block-design localizer study \citep{sengupta2016extension}, the same 15
participants took part in a six-category block-design visual localizer.
% participants' health
All participants reported to have normal hearing, normal or corrected-to-normal
vision, and no known history of neurological disorders.
% compensation, consent and shit
In all studies, participants received monetary compensation and gave written
informed consent for their participation and for public sharing of obtained data
in anonymized form. The studies had prior approval by the Ethics Committee of
Otto-von-Guericke University of Magdeburg, Germany.


\subsection{Stimuli and Procedure}

\todo[inline]{taken from naturalistic PPA paper}

% AD & AV stimulus name & references
The German DVD release \citep{ForrestGumpDVD} of the movie ``Forrest Gump''
\citep{ForrestGumpMovie} and its temporally aligned audio-description
\citep{ForrestGumpGermanAD} served as naturalistic stimuli, with an approximate
duration of two hours, split into eight consecutive segments of
\unit[$\approx$15]{minutes}.
% AD: additional narrator
The audio-description adds another male narrator to the voice-over narration of
the main character Forrest Gump. This additional narration describes essential
aspects of the visual scenery when there is no off-screen voice, dialog, or
other relevant auditory content.
% task
For all sessions with naturalistic stimuli, participants were instructed to
inhibit physical movements except for eye-movements, and otherwise to simply
``enjoy the presentation''.
%
For details on stimulus creation and presentation see
\citet{hanke2014audiomovie, hanke2016simultaneous}.

% VIS study picture categories
Stimuli for the block-design localizer study were 24 unique grayscale images of
faces, bodies, objects, houses, outdoor scenes and scrambled images, matched in
luminance and size, that were previously used in other studies
(\citep[e.g.,][]{haxby2011common}).
% procedure: presentation & instructions
Participants performed a one-back image matching task for four block-design
runs, with two \unit[16]{s} blocks per stimulus category in each run.
%
For details on stimulus creation and presentation see
\citet{sengupta2016extension}.


\subsection{Stimulation setup}

\todo[inline]{taken from naturalistic PPA paper}

% AD
In the audio-description study, visual instructions were presented on a
rear-projection screen inside the scanner bore. During the functional scans, the
projector presented a medium gray screen with the primary purpose to illuminate
a participant's visual field in order to prevent premature fatigue.
% AV & VIS
In the movie and block-design localizer study, visual instructions and stimuli
were presented on a rear-projection screen
% screen size \unit[23.75 $\times$ 10.25]{cm}
at a viewing distance of \unit[63]{cm}, with a movie frame projection size of
approximately \unit[21.3]$^{\circ}$ $\times$ \unit[9.3]$^{\circ}$.
% angle of view: VIS
In the block-design localizer study, stimulus images were displayed at a size of
approximately \unit[10]$^{\circ}$ $\times$ \unit[10]$^{\circ}$ of visual angle.
% AD & AV: auditory stimulation
Auditory stimulation was implemented using custom in-ear (audio-description), or
over-the-ear headphones (movie), which reduced the scanner noise by at least
\unit[20–30]{dB}.


\subsection{fMRI data acquisition}

\todo[inline]{taken from naturalistic PPA paper}

Gradient-echo fMRI data for the audio-description study were acquired using a
\unit[7]{Tesla} Siemens MAGNETOM magnetic resonance scanner equipped with a 32
channel brain receive coil at \unit[2]{s} repetition time (TR) with 36 axial
slices (thickness \unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane
resolution, \unit[224]{mm} field-of-view, anterior-to-posterior phase encoding
direction) and a \unit[10]{\%} inter-slice gap, recorded in ascending order.
% slice orientation
Slices were oriented to include the ventral portions of frontal and occipital
cortex while minimizing intersection with the eyeballs.
% FOV
The field of view was centered on the approximate location of Heschl's gyrus.
% motion correction
EPI images were online-corrected for motion and geometric distortions.

% AV & VIS
In the movie and block-design localizer study, a \unit[3]{Tesla} Philips Achieva
dStream MRI scanner with a 32 channel head coil acquired gradient-echo fMRI data
at \unit[2]{s} repetition time with
% slices
35 axial slices (thickness \unit[3.0]{mm}, \unit[10]{\%} inter-slice gap) with
\unit[80 $\times$ 80]{voxels} (\unit[3.0 $\times$ 3.0]{mm} of in-plane
resolution, \unit[240]{mm} field-of-view) and an anterior-to-posterior phase
encoding direction, recorded in ascending order.
% no. of volumes
A total of 3599 volumes were recorded for each participant in each of the
naturalistic stimulus paradigms (audio-description and movie).

% visual localizer: 4 x 156 TR
A total of 624 volumes were recored for each participant across the four runs of
the visual localizer experiment


\subsection{Preprocessing}

\todo[inline]{taken from naturalistic PPA paper}

% data sources
The current analyses were carried out on the same preprocessed fMRI data
\citep{hanke2016aligned} that were used for the technical validation analysis
presented in \citet{hanke2016simultaneous}.
% exclusion of VP 10
Of those 15 participants in the studyforrest dataset that took part in all three
experiments, data of one participant were dropped due to invalid distortion
correction during scanning of the audio-description stimulus.
% preprocessing of pre-aligned data
Data were corrected for motion, aligned with and re-sliced onto a
participant-specific BOLD template image \citep{sengupta2016extension} (uniform
spatial resolution of \unit[2.5$\times$2.5$\times$2.5]{mm} for both
audio-description and movie data).
% preprocessing intro
Preprocessing was performed by FEAT v6.00 (FMRI Expert Analysis Tool
\citep{woolrich2001autocorr}) as shipped with FSL v5.0.9
(\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software Library}
\citep{smith2004fsl}) on a computer-cluster running
\href{http://neuro.debian.net}{NeuroDebian} \citep{halchenko2012open}.



\subsubsection{Naturalistic stimuli}

\todo[inline]{following procedure in study 2 might not apply to study 3}


\paragraph{first-level analysis}

% install naturalistic ppa analysis as subdataset
% comprises  "aligned-data", templates etc. as subdatasets
% datalad install -d . -s https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis

% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in_bold3Tp2/sub-??_task-a?movie_run-?_bold*.*
The same input data used in study2 were downloaded as subdataset in
\citep{haeusler2021ppadata} for the creation of the \ac{cms}.

% get motion correction parameters for AO data
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/phase1
% datalad get inputs/studyforrest-ppa-analysis/inputs/phase1/sub???/BOLD/task001\_run00?/bold\_dico\_moco.txt
%
In order to use the data of the naturalistic stimuli for the creation of the
\ac{cms}, we preprocessed them in the same way as in
\citep{haeusler2022processing} by rerunning the first-level analysis:

\todo[inline]{taken from naturalistic PPA paper}

% temporal filtering
For the present analysis, the following additional preprocessing was performed.
High-pass temporal filtering was applied to every stimulus segment using a
Gaussian-weighted least-squares straight line with a cutoff period of
\unit[150]{s} (sigma=\unit[75.0]{s}) to remove low-frequency confounds.
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
Data were spatially smoothed applying a Gaussian kernel with full width at half
maximum (FWHM) of \unit[4.0]{mm}.
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% pre-whithening
Correction for local autocorrelation in the time series (prewhitening) was
applied using FILM (FMRIB's Improved Linear Model \citep{woolrich2001autocorr})
to improve estimation efficiency.


\paragraph{second-level analysis}

%% templates and transforms
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/sub-*/bold3Tp2/;
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/templates/*

% rerun second-level analysis
The second-level analysis was rerun to obtain the (un)thresholded $z$-maps.
\todo[inline]{was this even necessary???}


\subsubsection{Visual localizer}
% install VIS dataset as subdataset
% datalad install -d . -s https://github.com/psychoinformatics-de/studyforrest-data-visualrois.git inputs/studyforrest-data-visualroi
% datalad get inputs/studyforrest-data-visualrois/src/aligned/sub-*/in\_bold3Tp2/sub-*\_task-objectcategories\_run-*\_bold.nii.gz
% datalad get inputs/studyforrest-data-visualrois/sub-*/onsets/run-*/*.txt
In order to use the data of the visual localizer for the creation of the
\ac{cms}, we preprocessed them in the same way as in the original study
\citep{sengupta2016extension} by rerunning the first-level analysis:

% condor\_submit code/despike.submit
We despiked the ``raw'' data
(inputs/studyforrest-data-visualrois/code/despike.submit).
% rerun the analysis
% ./code/generate\_1st\_level\_design.sh
% condor\_submit code/compute\_1stlvl\_glm.submit
\todo[inline]{get the parameters from the design-files; cf. with VIS paper}


\subsection{Creation of common model space via Shared response model}

% introductory stuff
\citep{chen2015reduced}: let the algorithm learn the shared responses which is
the common model space, and it also learns the transformation matrices.
%

\todo[inline]{grand\_mean\_for\_4d.py (formerly: data\_normalize\_4d.py)}

\todo[inline]{actually, not necessary anymore 'cause FSL seems to have applied
grand mean scaling to 'filtered\_func\_data.nii.gz)'}

\todo[inline]{data\_mask\_concat\_runs.py will apply scipy's
    preprocessing.StandardScaler() before concatenation anyway}

% grand mean scaling for 4d data:
% voxel values in every image are divided by the average global mean
% intensity of the whole session. This effectively removes any mean global
% differences in intensity between sessions.

% FSL User Guide:
% filtered_func_data will normally have been temporally high-pass filtered,
% it is not zero mean; the mean value for each voxel's time course has been
% added back in for various practical reasons.
% When FILM begins the linear modelling, it starts by removing this mean.

% input: 'sub-*/run-?.feat/filtered_func_data.nii.gz' of VIS, AO, AV
% -> should already grand mean scaled

%
The time series of every experiment, subject, and run that has been filtered
by FSL ('filtered\_func\_data.nii.gz') were used.
%
Given that FSL adds back the mean value for each voxel's time course at the
and of the preprocessing it was subtracted again.
%
The each voxel was multiplied by 10000%
and saved to 'sub-??\_task-*\_run-?\_bold\_filtered.nii.gz'

\subsection{ROI creation}

\todo[inline]{problem 1: grpPPA contains n=14 subject, not n-1 subjects}

\todo[inline]{problem 2: still, there are voxels outside of the PPA-mask;
probably, because of the warping procedures}

% masks-from-mni-to-bold3Tp2.py:
% - merges unilateral ROIs overlaps (already in MNI) to bilateral ROI
% - output: 'masks/in_mni/PPA_overlap_prob.nii.gz'
% - warps union of ROIs from MNI into each subjects space
% output: 'sub-*/masks/in_bold3Tp2/grp_PPA_bin.nii.gz' + audio_fov.nii.gz dilate
% the ROI masks by 1 voxel; output: 'grp_PPA_bin_dil.nii.gz'

% masks-from-mni-to-bold3Tp2.py:
% warp MNI masks into individual bold3Tp2 spaces

% masks-from-t1w-to-bold3Tp2.py:
% transforms 'inputs/tnt/sub-*/t1w/brain_seg*.nii.gz'
% into individual's bold3Tp2
% output: 'sub-*/masks/in_bold3Tp2/brain_seg*.nii.gz'

% mask-builder-voxel-counter.py:
% builds different individual masks by dilating, merging other masks
% creates a FoV of AO stimulus for every subject from 4d time-series of AO run
% output: sub-*/masks/in_bold3Tp2/audio_fov.nii.gz'
% counts the voxels
% long story short: we cannot used all gyri that contain PPA to some degree
% even if the mask by FoV of AO stimulus and individual gray matter mask

% data_mask_concat_runs.py:
For each subject, and experiment, the corresponding 4d time-series of each run
were masked with the 'grp PPA' (but undilated and not masked with individual
gray matter mask) multiplied by the individual's \ac{fov} of the
audio-description.

%
Number of voxel within the mask can be seen in \ref{tab:ppamaskvoxels}.

\begin{table*}[btp]
\caption{Number of voxels within the union of individual PPAs projected back
    into individuals' subject-space.}
\label{tab:ppamaskvoxels}
\begin{tabular}{ll}
\toprule
\textbf{Subject} & \textbf{no. of voxels} \\
\midrule
sub-01 & 1665 \tabularnewline
sub-02 & 1732 \tabularnewline
sub-03 & 1400 \tabularnewline
sub-04 & 1575 \tabularnewline
sub-05 & 1664 \tabularnewline
sub-06 & 1951 \tabularnewline
sub-14 & 1376 \tabularnewline
sub-09 & 1383 \tabularnewline
sub-15 & 1683 \tabularnewline
sub-16 & 1887 \tabularnewline
sub-17 & 1441 \tabularnewline
sub-18 & 1729 \tabularnewline
sub-19 & 1369 \tabularnewline
sub-20 & 1437 \tabularnewline
\bottomrule
\end{tabular}
\end{table*}

% scaling
Then, data of every run were scaled using scipy's preprocessing.StandardScaler()
% output: 'sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy; and:
% 'sub-*_task_visloc_run-1-4_bold-filtered.npy'
Given that the last 75TR of the last run of the audio-description were missing
in subject 04 due to WHAT?, we also dropped the last 75 TR from the other
participants data since it these TRs consisted just of the credits of the movie
anyway.
% AO + AV has 7123 TRs; not 7198 volumes anymore
As a result the data to create the CMS consisted of 3524 TRs of
audio-description, 3599 TRs of the movie and 4x156 = 624 TRs of the visual
localizer experiment.

\subsubsection{Fitting of shared response model}

\todo[inline]{check brainIAK tutorial; how to cite brainIAK?}

% data_srm_fitting.py
% output: 'test/sub-??/srm-ao-av-vis\_feat10-iter30.npz

\todo[inline]{2 kinds of flavours: 'ao \& av \& vis' vs. only 'ao \& av'}

Fore each subject, we created a \ac{cms} based on other subject's data.
%
The runs of the audio-description, movie, and visual localizer were concatenated
to one continuous and z-scored using scipy.stats.zscore().
% features
Model fitting was performed using 10 features [WHY?] and 30 iterations;
calling the fit method if brainIAK's srm object (brainiak.funcalign.srm.SRM)

% negative control; output: 'test/sub-01/srm-ao-av-shuffled\_feat10-iter30.npz'
As negative control, we also fitted a shared response model to after shuffling
the order of runs of audio-description and movie (but not visual localizer) of
every subject before fitting the model.


\paragraph{plot\_srm.py}

\todo[inline]{imo not very informative}
%
plots SRM (features*time points) created for a left-out subject; plots
time-course of top 3 responses (currently, TRs 0-800 = quarter of AO); plots
distance matrix of time points in shared space


\paragraph{plot\_corr-of-glm-and-srm.py}
%
\begin{itemize}
    \item plot correlation matrix between
    \item regressors of the naturalistic stimuli (and some combinations, e.g.
        geo\&groom, geo\&groom\&furn
    \item and shared responses
    \item at the moment, it correlates only with AO TRs (and wrongly uses
        0:3599 TRs)
    \item ToDo: at the moment in\_dir with hardcoded model 'srm-ao-av'
    ->  means: no vis, not shuffled; correlate AO regressors with correct AO
        TRs; correlate AO regressors with AV TRs; correlate AV regressors with
        AV TRs; correlate AV regressors with AO TRs; correlate AO and AV
        regressors with all TRs (+VIS?)
\end{itemize}


\subsubsection{Partial alignment}

\todo[inline]{matrices have been computed CMS created from AO+AV+VIS, but also
just from AO+AV}

\paragraph{get\_wmatrix\_for\_left-out.py}
% AO: 0-451, 0-892, 0-1330, 0-1818, 0-2280, 0-2719, 0-3261, 0-3524
% AV: 3524-3975, 3524-4416, 3524-4854, 3524-5342, 3524-5804, 3524-6243,
%     3524-6785, 3524-7123
% AO+AV: 0-7123

% input 'sub-*/sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy'
We used an increasing number of runs of the filtered audio-description and movie
data to align each left-out subject to the corresponding \ac{cms}, and obtain
the subject's transformation matrix.
%
The SRM object was sliced according to the used runs and the weight matrix was
obtained calling the SRM method srm\_sliced.transform\_subject(runsdata)
% output: f'wmatrix_{model}_feat{n_feat}_{start}-{end}.npy'


\paragraph{predict\_ppa.py}

% prediction from anatomy
We first created the template/prediction from the anatomy of other subjects.
%
We warped the results of the visual localizer (and results of the auditory
naturalistic stimulus) from each individual's subject-space (bold3Tp2) into MNI
space (grpbold3Tp2).
% output 1: 'test/masks/in_mni/sub-*_VIS-PPA.nii.gz'
% output 2: 'test/masks/in_mni/sub-*_AO-PPA.nii.gz'
The results of all subjects were warped from MNI space in to the left-out
subject's space
% output 1: 'test/sub-*/masks/in_bold3Tp2/sub-*_VIS-PPA.nii.gz'
% output 2: 'test/sub-*/masks/in_bold3Tp2/sub-*_AO-PPA.nii.gz'
The transformed z-maps were masked with mask of unions of PPAs and the
individual's \ac{fov}.
%
The mean of these z-maps served as the prediction for the left-out subject.
% output1: 'test/sub-*/predicted-VIS-PPA_from_anatomy.nii.gz'
% output2: 'test/sub-*/predicted-AO-PPA_from_anatomy.nii.gz'

% prediction from CMS
We then predicted the z-maps of the PPA from the visual localizer experiment and
the auditory naturistic stimulus.
%
The transformation matrices that were created using an increasing number of runs
of each naturalistic stimulus were used to transform a z-map templated created
in the \ac{cms} into the left-out subject's space
% aligned zmaps to shared space: (k features x t time-points); 1 time-point
% because it's a zmap no time-series) using

First, individual results from the visual localizer and auditory naturalistic
stimulus were masked with the group PPA mask in each individuals subject space
and then transformed from each individual's space into the \ac{cms} [by calling
'zmaps\_in\_cms = srm.transform(masked\_zmaps)'].
% zmap from CMS into subject space: compute dot product of left-out subject
% weight-matrix with zmap in cms
Using the inverse of the left-out subject's transformation matrix, the zmaps
from the other subjects that were projected into \ac{cms} were then projected
into the anatomy of the left-out subject.
%
Here again, the mean of these z-maps served as the prediction for the left-out
subject.

\todo[inline]{i.e.: in both cases the mean was computed in anatomical space;
this is more 'similar' than computing the mean in anatomy for prediction from
anatomy, and computing the mean in CMS for prediction from CMS; but: creating the
template in CMS does not lead to different results anyway}

% correlation between empirical z-map & predicted z-maps
Finally, we calculated the Pearson correlation coefficients between the
empirical z-maps of each subject (visual localizer and auditory PPA) and the
values that were predicted from other subjects' anatomy.
%
Further, we calculated the Pearson correlation coefficients between the
empirical z-maps of each subject (visual localizer and auditory PPA) and the
values that were predicted from using an increasing number of runs of the
naturalsitic stimuli to obtain the transformation matrices.


\subsubsection{alternative template creation}
%
lastly, we currently apply another method to get the „z-map template“ on which
the prediction is based ('test/data\_denoise-vis.py';
'test/data\_srm-vis-to-ind.py')


\section{Results}

%
For every subject, we see the correlation of z-maps that tell us the, quote
``real'' unquote, PPA and predicted PPA
%
In green, we see the correlations between empirical values from the localizer \&
the predicted values using anatomical alignment
%
In orange, we see the correlations between empirical values \& the predicted
values using parts of the movie
%
In blue, we see the correlations between empirical values \& the predicted
values using parts of the audio-description
%
I marked subject 4, because I want to show you how results look like in a
horizontal slice of the brain of subject 4
%
We have these nice blurry EPI-images and all z-maps are threshold at a value of
bigger than 2.3.
%
always in red, we can see the z-map from the localizer experiment across the
whole brain,
%
the region of interest that we used is white and the predicted values, are blue
%
The prediction using anatomical alignment and the prediction using 15 minutes of
movie data show a correlation of about .7
%
the prediction using 15 minutes of the audio-description correlates about 0 with
the empirical z-map
%
The last one is the extreme case, but it can give you an idea of how the z-maps
look in a slice of the brain


\subsection{predict\_ppa.py: outputs also the Pearson correlations}


\subsection{plot\_stripplot.py}

the lovely stripplots of correlations

\subsection{statistics\_t-test-correlations.py}

Compute if differences between kinds of prediction is significant


\subsubsection{test/statistics\_cronbachs.py}

\subsubsection{plot\_bland-altman.py}

I hate that script

% corrstats.py; not necessary anymore; calculates the statistical significant
% differences between two dependent or independent correlation coefficients






\section{Discussion}


\subsection{Discussion of current results}
%
15 min of movie watching used for functional alignment outperform prediction
using anatomical alignment
%
30 minutes of movie watching outperform 15 minutes of movie watching
%
more than 30 minutes do not lead to a significantly improved prediction
performance.
%

\subsection{ground truth}

``Predictive models based on neuroimaging data will only ever account for a
fraction of the variance.
%
Neuroimaging studies are limited by how much information the signal can capture
about the measure of interest.
%
At the same time, these studies are also limited by the chosen phenotypic
measure used.  While the success of a model is evaluated by how well it predicts
a phenotypic measure (and these phenotypic measures have to be treated as gold
standards), it is well known that such measures are not always the ground truth
but themselves suffer from confounds and noise.
%
When studying brain-behavior associations, one must keep in mind how
extraordinary it is that neuroimaging data can be distilled to approximate
phenotypic measures that reﬂect a simpliﬁcation of multiple complex features.
%
Thus, even modest results are reasonable and remarkable.
%
For a discussion on the reliability of phenotypic measures in the context of
predictive modeling, we point the interested reader to: (Dubois et al., 2018a,
2018b; Gignac and Bates, 2017) \citep{scheinost2019ten}.

\subsection{Future questions}
%

\subsubsection{Proof of concept}
%


\subsubsection{ROI vs. whole brain}
%
more (?) data vs. searchlight algorithm; connectivity;
\citep{nastase2019leveraging}


\subsubsection{predict other t-contrasts of localizer}


\subsubsection{predict other localizers}
%
e.g. retinotopy, language areas


\subsubsection{create CMS from other study}
%
we currently have a cross-subject and cross-experiment prediction,
but we do not have a (real) cross-scanner prediction
%
create a CMS from another experiment’s data,
using another scanner and hopefully more subjects
%
In case of an alignment of time-series,
that experiment needs, at least, a part of Forrest Gump as an intersection


\subsection{the vision}
%
functional atlas
%
Imagine you scan a new, unknown subject for just 15 minutes more, and you
additionally get results from a whole variety of other paradigms mapped onto
that brain
%
Results from localizers of low-level perceptual processes, but also higher-level
cognitive processes like language, memory, emotions and so on
%
And I did not write that onto the slides because it is just a vague idea:
%
If you have different common model spaces for different subgroups, you can
investigate which alignment onto which ``subgroup common model space'' results
in less error, that lets you classify to which subgroup your new subject might
belong.


\section{Conclusion}
