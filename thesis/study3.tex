\section{Abstract}
%
In order to map perceptual or cognitive functions onto brain areas of study
participants, researchers usually conduct dedicated experiments often
accompanied with a task (so called ``functional localizer'').
%
Nevertheless, the approach ``one paradigm, one brain function'' becomes
unfeasible if one wants to map a variety of functions in a time-efficient
manner.
%
We explored a way to project brain mapping data (statistical z-maps) from a
reference group onto individual brains of study participants after performing a
functional aligning of participants with a common model space.
%
Data were obtained from conducting a functional localizer paradigm and two
paradigms using naturalistic stimulation during functional magnetic resonance
imaging (fMRI):
%
participants took part in a task-based, block-design visual localizer, and
participants were watching an audio-visual movie and listening to the movie's
audio-description, both paradigms free of any task.
%
Based on these data, we created a common model space employing a \ac{srm}
\citep{chen2015reduced}.
%
On the one hand, the common model space allows denoising data from individuals
that were used to create the common model space.
%
On the other hand, data from left-out subjects can be aligned with the common
model space by using a (preferably short) segment of a naturalistic stimulus as
a ``diagnostic run'', a process that provides a subject-specific transformation
matrices.
%
The inverse of the acquired transformation matrices can then be used to project
data from other paradigms aligned in common space onto the brain of the left-out
subjects.
%
The general goal of the project is to assess the required length of the
diagnostic run, and to compare empirical z-maps with z-maps that were predicted
from other participants' data.
%
We present preliminary results of predicted z-maps gained from the same paradigm
as the diagnostic run as well as across paradigms.


\section{Introduction}
% brain mapping
topographic brain mapping maps brain functions, perceptual or cognitive
processes, onto brain areas.
% localizer
Usually, this is done by letting study participants perform a task during a so
called ``functional localizer''.
% problem: one localizer for one domain
The second problem is: you usually need one localizer for one domain of brain
functions
% which gets messy
Which means: if you want to map a variety of domains, you need a variety of
localizers, and then the whole approach gets time-consuming and inefficient
(localizer batteries; e.g. Thirion's work)
% problem: compliance
Even more severe in a clinical population: they mostly need the participants to
perform a task, and they are just plain boring which often leads to a diminished
compliance.


\subsection{Our approach}
% main idea
The main idead ist to predict location, size, and shape of a functional area in
an individual person based on location, size, and shape of the same functional
area in persons of a reference group.
%
Based on a prediction of a reference group's data, the diagnostic procedure
could be more reliable and valid but also more time-efficient and thus more
affordable.
%
We can do this in basically two ways:
a) we do this by performing an anatomical alignment, or
%
which is a new a approach:
%
b) we can do this by performing a functional alignment
%

but when we want to predict functional areas from anatomically aligned brains,
we are running into the problem of poor
functional-anatomical correspondence.

Poor functional-anatomical correspondence means:
even if we assume that two persons have anatomically identical brains, still,
the location, size \& shape of the functional area is probably different.


\subsubsection{anatomical alignment}

% anatomical alignment
In general, a individual's diagnostic could be based on a prediction based on
the anatomical location of the functional area in the reference group (e.g.
\citet{weiner2018defining}.
%
- anatomical alignment aligns vertices in 2-dimensional space or voxels in
3-dimensional anatomical space
%
we transform the shape of an individual brain into the shape of an average,
standard brain


\paragraph{procedure}
%
\begin{itemize}
    \item conduct the localizer experiment to get the brain responses in the
    reference group
    \item align the brains in that reference group anatomically to the standard
    brain
    \item also align your left-out subject to the standard brain, which gives
    you a transformation matrix
    \item then you use the inverse of that transformation matrix to project the
    data
    \item from the reference group into the anatomy of the left-out subject,
    which is essentially the prediction
\end{itemize}


\paragraph{but: functional-anatomical correspondence}
% shitty
The approach relying on merely anatomical alignment is shitty because of modest
functional-anatomical correspondence.
% explain functional-anatomical correspondence
A modest functional-anatomical correspondence means that even if one would assume
two persons had an identically shaped brain anatomy the location, size, and
shape of a functional area would still not be indetical.

% PPA example
Here, as an example the location of the ``PPA'', the Parahippocampal Place Area.
which is more active when we look at pictures of scenes or landscapes.
%
location, shape, size of the PPA in 14 subjects,
%
the brighter the voxel the more subjects have their PPA in that location.
%
location is roughly similar across persons, but still, the plot shows that there
is individual variation.


\paragraph{individual differences}
%
do you find differences in individual brain patterns or do differences stem from
differences in functional-anatomical correspondence?
%
vice versa: we might find no differences because of poor functional-anatomical
correspondence


\subsubsection{functional alignment}
%
given that we do not want to predict brain anatomy but predict an accurate
mapping of brain functions onto the brain anatomy:
%
wouldn't it make more sense to not perform an anatomical alignment but a
functional alignment?
%
Functional alignment aligns cortical patterns (time-series or connectivity
profiles) in a multi-dimensional function space.
% reference
The reference is not an average brain anatomy but a so called \ac{cms}.


\paragraph{Hyperalignment}
%
There are two algorithms dominating the field: hyperalignment and \ac{srm}


\paragraph{Shared response model}
%
\todo[inline]{why did we chose SRM?}


\paragraph{procedure}
%
You let study participants watch a naturalistic stimulus like a movie or an
audio-book...  ...which are continuous, naturally engaging, and are triggering a
wide variety of time-locked brain functions
%
From that stimuli, you get the time-series per voxel and per subject, and what
the algorithm essentially does is: it learns responses that are shared across
participants (which is the common model space)
%
but more importantly, the algorithm also learns individual transformation
matrices.
%
These transformation matrices can be used to project data into and out of the
Common Model Space.
For the prediction through a common model space, we first needed to create it
%
We let participants watch the movie and audio-description of Forrest Gump,
assuming that the naturalistic stimuli trigger, among others, brain responses
that a similar to those triggered by the functional localizer
%
Usually when aligning brain responses (and not connectivity profiles) you would
let the left-out subject watch the same and the whole stimuli that were used to
create the Common Model Space to get the transformation matrices.



\subsubsection{our current study: overview, blabla}
%
focussing on the domain of so called ``higher-visual, category-selective
areas''
%
How do you usually map these category-selective areas onto brains of individual
study participants?
%
You conduct a functional localizer experiments, lasting about 20 minutes,
%
You let the participants watch pictures of different categories, while also
letting them perform a task to force them to pay attention to picture, after
picture, after picture
%
After having collecting the data you do some statistics, and as a result you
get statistical maps, t- or z-map, that essentially tell you where the
higher-visual areas are located
%
If we need about 20 minutes for just one domain, why don’t we predict the
location of a functional area in one person from the location that we found in
other persons?


\subsubsection{our questions}
% questions of current study
In the current study, we asked ourselves two questions:
%
1) does functional alignment based on a the \ac{srm} improve the prediction
compared to a prediction based on merely anatomical alignment?
%
2) how much data do we need to align an individual subject to a/the \ac{cms} and
outperform a prediction based on anatomical alignment? Is a short diagnostic run
``sufficient''?
%
We wanted to predict the location of the PPA based on data from a reference
group
%
And we wanted to compare the prediction performance based on anatomical
alignment \& functional alignment
%
- goal: align left-out subject to model space using varying amount of data of
movie or audiobook - transform localizer results into left-out subject
%
Hence, the real question that we asked yourselves was: How many minutes of a
naturalistic stimulus are necessary to perform what we call a „partial
alignment“.
%
So, we only used parts of the naturalistic stimuli to let the algorithm learn
the transformation matrices for the left-out subject
%
and we tested which amount of data is needed to do an alignment to the common
model space that provides transformation matrices that outperform a prediction
using just an anatomical alignment.


\section{Methods}

\todo[inline]{taken from naturalistic PPA paper}

% intro
We used components of the publicly available
\href{http://www.studyforrest.org}{studyforrest.org} dataset that has
been repeatedly used by other research groups in independent studies
(\citep[e.g.,][]{ben2018hippocampal, jiahui2019predicting, hu2017decoding,
lettieri2019emotionotopy, nguyen2016integration}).
% used studies
The same participants were
% AD
a) listening to the audio-description \citep{hanke2014audiomovie} of
the movie ``Forrest Gump'',
% AV
b) watching the audio-visual movie \citep{hanke2016simultaneous}, and
% VIS
c) participating in a dedicated six-category block-design visual localizer
\citep{sengupta2016extension}.
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications. Following is a summary of the most important aspects.


\subsection{Participants}

\todo[inline]{taken from naturalistic PPA paper}

% we get the data from the naturalistic PPA paper (its subdataset)
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in\_bold3Tp2/sub-??\_task-a?movie\_run-?\_bold*.*


% AD study
In the audio-description study \citep{hanke2014audiomovie}, 20 German native
speakers (all right-handed, age 21–38 years, mean age 26.6 years, 12 male)
listened to the German audio-description \citep{ForrestGumpGermanAD} of the
movie ``Forrest Gump'' \citep{ForrestGumpMovie}.
% AV study
In the movie study \citep{hanke2016simultaneous}, 15 participants (21–39 years,
mean age 29.4, six female), a subgroup of the prior audio-description study,
watched the audio-visual movie with dubbed German audio track
\citep{ForrestGumpDVD}.
% VIS study
In the block-design localizer study \citep{sengupta2016extension}, the same 15
participants took part in a six-category block-design visual localizer.
% participants' health
All participants reported to have normal hearing, normal or corrected-to-normal
vision, and no known history of neurological disorders.
% compensation, consent and shit
In all studies, participants received monetary compensation and gave written
informed consent for their participation and for public sharing of obtained data
in anonymized form. The studies had prior approval by the Ethics Committee of
Otto-von-Guericke University of Magdeburg, Germany.


\subsection{Stimuli and Procedure}

\todo[inline]{taken from naturalistic PPA paper}

% AD & AV stimulus name & references
The German DVD release \citep{ForrestGumpDVD} of the movie ``Forrest Gump''
\citep{ForrestGumpMovie} and its temporally aligned audio-description
\citep{ForrestGumpGermanAD} served as naturalistic stimuli, with an approximate
duration of two hours, split into eight consecutive segments of
\unit[$\approx$15]{minutes}.
% AD: additional narrator
The audio-description adds another male narrator to the voice-over narration of
the main character Forrest Gump. This additional narration describes essential
aspects of the visual scenery when there is no off-screen voice, dialog, or
other relevant auditory content.
% task
For all sessions with naturalistic stimuli, participants were instructed to
inhibit physical movements except for eye-movements, and otherwise to simply
``enjoy the presentation''.
%
For details on stimulus creation and presentation see
\citet{hanke2014audiomovie, hanke2016simultaneous}.

% VIS study picture categories
Stimuli for the block-design localizer study were 24 unique grayscale images of
faces, bodies, objects, houses, outdoor scenes and scrambled images, matched in
luminance and size, that were previously used in other studies
(\citep[e.g.,][]{haxby2011common}).
% procedure: presentation & instructions
Participants performed a one-back image matching task for four block-design
runs, with two \unit[16]{s} blocks per stimulus category in each run.
%
For details on stimulus creation and presentation see
\citet{sengupta2016extension}.


\subsection{Stimulation setup}

\todo[inline]{taken from naturalistic PPA paper}

% AD
In the audio-description study, visual instructions were presented on a
rear-projection screen inside the scanner bore. During the functional scans, the
projector presented a medium gray screen with the primary purpose to illuminate
a participant's visual field in order to prevent premature fatigue.
% AV & VIS
In the movie and block-design localizer study, visual instructions and stimuli
were presented on a rear-projection screen
% screen size \unit[23.75 $\times$ 10.25]{cm}
at a viewing distance of \unit[63]{cm}, with a movie frame projection size of
approximately \unit[21.3]$^{\circ}$ $\times$ \unit[9.3]$^{\circ}$.
% angle of view: VIS
In the block-design localizer study, stimulus images were displayed at a size of
approximately \unit[10]$^{\circ}$ $\times$ \unit[10]$^{\circ}$ of visual angle.
% AD & AV: auditory stimulation
Auditory stimulation was implemented using custom in-ear (audio-description), or
over-the-ear headphones (movie), which reduced the scanner noise by at least
\unit[20–30]{dB}.


\subsection{fMRI data acquisition}

\todo[inline]{taken from naturalistic PPA paper}

Gradient-echo fMRI data for the audio-description study were acquired using a
\unit[7]{Tesla} Siemens MAGNETOM magnetic resonance scanner equipped with a 32
channel brain receive coil at \unit[2]{s} repetition time (TR) with 36 axial
slices (thickness \unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane
resolution, \unit[224]{mm} field-of-view, anterior-to-posterior phase encoding
direction) and a \unit[10]{\%} inter-slice gap, recorded in ascending order.
% slice orientation
Slices were oriented to include the ventral portions of frontal and occipital
cortex while minimizing intersection with the eyeballs.
% FOV
The field of view was centered on the approximate location of Heschl's gyrus.
% motion correction
EPI images were online-corrected for motion and geometric distortions.

% AV & VIS
In the movie and block-design localizer study, a \unit[3]{Tesla} Philips Achieva
dStream MRI scanner with a 32 channel head coil acquired gradient-echo fMRI data
at \unit[2]{s} repetition time with
% slices
35 axial slices (thickness \unit[3.0]{mm}, \unit[10]{\%} inter-slice gap) with
\unit[80 $\times$ 80]{voxels} (\unit[3.0 $\times$ 3.0]{mm} of in-plane
resolution, \unit[240]{mm} field-of-view) and an anterior-to-posterior phase
encoding direction, recorded in ascending order.
% no. of volumes
A total of 3599 volumes were recorded for each participant in each of the
naturalistic stimulus paradigms (audio-description and movie).

% visual localizer: 4 x 156 TR
A total of 624 volumes were recored for each participant across the four runs of
the visual localizer experiment


\subsection{Preprocessing}

\todo[inline]{taken from naturalistic PPA paper}

% data sources
The current analyses were carried out on the same preprocessed fMRI data
\citep{hanke2016aligned} that were used for the technical validation analysis
presented in \citet{hanke2016simultaneous}.
% exclusion of VP 10
Of those 15 participants in the studyforrest dataset that took part in all three
experiments, data of one participant were dropped due to invalid distortion
correction during scanning of the audio-description stimulus.
% preprocessing of pre-aligned data
Data were corrected for motion, aligned with and re-sliced onto a
participant-specific BOLD template image \citep{sengupta2016extension} (uniform
spatial resolution of \unit[2.5$\times$2.5$\times$2.5]{mm} for both
audio-description and movie data).
% preprocessing intro
Preprocessing was performed by FEAT v6.00 (FMRI Expert Analysis Tool
\citep{woolrich2001autocorr}) as shipped with FSL v5.0.9
(\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software Library}
\citep{smith2004fsl}) on a computer-cluster running
\href{http://neuro.debian.net}{NeuroDebian} \citep{halchenko2012open}.



\subsubsection{Naturalistic stimuli}

\todo[inline]{following procedure in study 2 might not apply to study 3}


\paragraph{first-level analysis}

% install PPA-Analyse als subdataset
% enthält "aligned-data", templates etc. als subdatasets
% datalad install -d . -s https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis

% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in_bold3Tp2/sub-??_task-a?movie_run-?_bold*.*
The same input data used in study2 werd downloaded as subdataset in
\citep{haeusler2021ppadata} for the creation of the \ac{cms}.

% get motion correction parameters for AO data
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/phase1
% datalad get inputs/studyforrest-ppa-analysis/inputs/phase1/sub???/BOLD/task001\_run00?/bold\_dico\_moco.txt
%
In order to use the data of the naturalistic stimuli for the creation of the
\ac{cms}, we preprocessed them in the same way as in
\citep{haeusler2022processing} by rerunning the first-level analysis:

\todo[inline]{taken from naturalistic PPA paper}

% temporal filtering
For the present analysis, the following additional preprocessing was performed.
High-pass temporal filtering was applied to every stimulus segment using a
Gaussian-weighted least-squares straight line with a cutoff period of
\unit[150]{s} (sigma=\unit[75.0]{s}) to remove low-frequency confounds.
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
Data were spatially smoothed applying a Gaussian kernel with full width at half
maximum (FWHM) of \unit[4.0]{mm}.
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% pre-whithening
Correction for local autocorrelation in the time series (prewhitening) was
applied using FILM (FMRIB's Improved Linear Model \citep{woolrich2001autocorr})
to improve estimation efficiency.


\paragraph{second-level analysis}

%% templates and transforms
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/sub-*/bold3Tp2/;
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/templates/*

% rerun second-level analysis
The second-level analysis was rerun to obtain the (un)thresholded $z$-maps.
\todo[inline]{was this even necessary???}


\subsubsection{Visual localizer}
% install VIS dataset as subdataset
% datalad install -d . -s https://github.com/psychoinformatics-de/studyforrest-data-visualrois.git inputs/studyforrest-data-visualroi
% datalad get inputs/studyforrest-data-visualrois/src/aligned/sub-*/in\_bold3Tp2/sub-*\_task-objectcategories\_run-*\_bold.nii.gz
% datalad get inputs/studyforrest-data-visualrois/sub-*/onsets/run-*/*.txt
In order to use the data of the visual localizer for the creation of the
\ac{cms}, we preprocessed them in the same way as in the original study
\citep{sengupta2016extension} by rerunning the first-level analysis:

% condor\_submit code/despike.submit
We despiked the ``raw'' data
(inputs/studyforrest-data-visualrois/code/despike.submit).
% rerun the analysis
% ./code/generate\_1st\_level\_design.sh
% condor\_submit code/compute\_1stlvl\_glm.submit
\todo[inline]{get the parameters from the design-files; cf. with VIS paper}


\subsection{Creation of commond model space via Shared response model}

% Eingangs blabla
\citep{chen2015reduced}: let the algorithm learn the shared responses which is
the common model space, and it also learns the transformation matrices.
%

\todo[inline]{grand\_mean\_for\_4d.py (formerly: data\_normalize\_4d.py)}

\todo[inline]{actually, not necessary anymore 'cause FSL seems to have applied
grand mean scaling to 'filtered\_func\_data.nii.gz)'}

\todo[inline]{data\_mask\_concat\_runs.py will apply scipy's
    preprocessing.StandardScaler() before concatenation anyway}

% grand mean scaling for 4d data:
% voxel values in every image are divided by the average global mean
% intensity of the whole session. This effectively removes any mean global
% differences in intensity between sessions.

% FSL User Guide:
% filtered_func_data will normally have been temporally high-pass filtered,
% it is not zero mean; the mean value for each voxel's timecourse has been
% added back in for various practical reasons.
% When FILM begins the linear modelling, it starts by removing this mean.

% input: 'sub-*/run-?.feat/filtered_func_data.nii.gz' of VIS, AO, AV
% -> should already grand mean scaled

%
The time series of every experiment, subject, and run that has been filtered
by FSL ('filtered\_func\_data.nii.gz') were used.
%
Given that FSL adds back the the mean value for each voxel's timecourse at the
and of the preprocessing it was substracted again.
%
The each voxel was multiplied by 10000%
and saved to 'sub-??\_task-*\_run-?\_bold\_filtered.nii.gz'

\subsection{ROI creation}

\todo[inline]{problem 1: grpPPA contains n=14 subject, not n-1 subjects}

\todo[inline]{problem 2: still, there are voxels outside of the PPA-mask;
prolly, because of the warping procedures}

% masks-from-mni-to-bold3Tp2.py:
% - merges unilateral ROIs overlaps (already in MNI) to bilateral ROI
% - output: 'masks/in_mni/PPA_overlap_prob.nii.gz'
% - warps union of ROIs from MNI into each subjects space
% output: 'sub-*/masks/in_bold3Tp2/grp_PPA_bin.nii.gz' + audio_fov.nii.gz dilate
% the ROI masks by 1 voxel; output: 'grp_PPA_bin_dil.nii.gz'

% masks-from-mni-to-bold3Tp2.py:
% warp MNI masks into individual bold3Tp2 spaces

% masks-from-t1w-to-bold3Tp2.py:
% transforms 'inputs/tnt/sub-*/t1w/brain_seg*.nii.gz'
% into individual's bold3Tp2
% output: 'sub-*/masks/in_bold3Tp2/brain_seg*.nii.gz'

% mask-builder-voxel-counter.py:
% builds different individual masks by dilating, merging other masks
% creates a FoV of AO stimulus for every subject from 4d time-series of AO run
% output: sub-*/masks/in_bold3Tp2/audio_fov.nii.gz'
% counts the voxels
% long story short: we cannot used all gyri that contain PPA to some degree
% even if the mask by FoV of AO stimulus and individual gray matter mask

% data_mask_concat_runs.py:
For each subject, and experiment, the corresponding 4d time-series of each run
were masked with the 'grp PPA' (but undilated and not masked with individual
gray matter mask) multiplied by the individual's FoV of the audio-description.

%
voxel for bilateral PPAs:
sub-01: 1665;
sub-02: 1732;
sub-03: 1400;
sub-04: 1575;
sub-05: 1664;
sub-06: 1951;
sub-09: 1383;
sub-14; 1376;
sub-15; 1683;
sub-16: 1887;
sub-17: 1441;
sub-18: 1729;
sub-19: 1369;
sub-20: 1437.

% scaling
Then, data of every run were scaled using scipy's preprocessing.StandardScaler()
% output: 'sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy; and:
% 'sub-*_task_visloc_run-1-4_bold-filtered.npy'
Given that the last 75TR of the last run of the audio-description were missing
in subject 04 due to WHAT?, we also dropped the last 75 TR from the other
participants data since it these TRs consisted just of the credits of the movie
anyway.
% AO + AV has 7123 TRs; not 7198 volumes anymore
As a result the data to create the CMS consisted of 3524 TRs of
audio-description, 3599 TRs of the movie and 4x156 = 624 TRs of the visual
localizer experiment.

\subsubsection{Fitting of shared response model}

\todo[inline]{check brainIAK tutorial; how to cite brainIAK?}

% data_srm_fitting.py
% output: 'test/sub-??/srm-ao-av-vis\_feat10-iter30.npz

\todo[inline]{2 kinds of flavours: 'ao \& av \& vis' vs. only 'ao \& av'}

Fore each subject, we created a \ac{cms} based on other subject's data.
%
The runs of the audio-description, movie, and visual localizer were concatenated
and z-scored usin scipy.stats.zscore().
% features
Model fitting was performed using 10 features [WHY?] and 30 iterations;
calling the fit method if brainIAK's srm object (brainiak.funcalign.srm.SRM)

% negative control; output: 'test/sub-01/srm-ao-av-shuffled\_feat10-iter30.npz'
As negative control, we also fitted a shared response model to after shuffling
the order of runs of audio-description and movie (but not visual localizer) of
every subject before fitting the model.


\paragraph{plot\_srm.py}

\todo[inline]{imo not very informative}
%
plots SRM (features*time points) created for a left-out subject; plots
time-course of top 3 respones (currently, TRs 0-800 = quarter of AO); plots
distance matrix of time points in shared space


\paragraph{plot\_corr-of-glm-and-srm.py}
%
\begin{itemize}
    \item plot correlation matrix between
    \item regressors of the naturalistic stimuli (and some combinations, e.g.
        geo\&groom, geo\&groom\&furn
    \item and shared responses
    \item at the moment, it correlates only with AO TRs (and wrongly uses
        0:3599 TRs)
    \item ToDo: at the moment in\_dir with hardcoded model 'srm-ao-av'
    ->  means: no vis, not shuffled; correlate AO regressors with correct AO
        TRs; correlate AO regressors with AV TRs; correlate AV regressors with
        AV TRs; correlate AV regressors with AO TRs; correlate AO and AV
        regressors with all TRs (+VIS?)
\end{itemize}


\subsubsection{Partial alignment}
% get_wmatrix_for_left-out.py
% AO: 0-451, 0-892, 0-1330, 0-1818, 0-2280, 0-2719, 0-3261, 0-3524
% AV: 3524-3975, 3524-4416, 3524-4854, 3524-5342, 3524-5804, 3524-6243,
%     3524-6785, 3524-7123
% AO+AV: 0-7123

% input 'sub-*/sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy'
We used an increasing number of runs of the filtered audio-description and movie
data to align the left-out subject to the \ac{cms} (done for AO+AV and AO+AV+VIS).
%
The SRM object was sliced according to the used runs and the weight matrix was
obtained calling the SRM methd srm\_sliced.transform\_subject(runsdata)
% output: f'wmatrix_{model}_feat{n_feat}_{start}-{end}.npy'


\subsubsection{predict\_ppa.py}
\begin{itemize}
    \item for every subject, transforms the (correct) PPA contrast of the VIS
        study (and AO PPA) from bold3Tp2 into MNI space (e.g. grpbold3TP2)
    \item output: 'test/masks/in\_mni/sub-*\_VIS-PPA.nii.gz
    \item output: 'test/masks/in\_mni/sub-*\_AO-PPA.nii.gz
    \item warp all individuals' z-maps in group space into left-out subject's
    bold3Tp2 space
    \item test/sub-*/masks/in\_bold3Tp2/sub-*\_VIS-PPA.nii.gz
    \item prediction from anatomy: load the warped z-maps, mask it with left-out
        subjects PPA overlap mask * FoV, take mean of z-maps, save
    \item output: predicted-VIS-PPA\_from\_anatomy.nii.gz'; \\
        predicted-AO-PPA\_from\_anatomy.nii.gz
    \item calculate correlation between empirical z-map and z-map predicted from
        anatomy
    \item predict from CMS: use increasing number of runs of AO and AV
    \item load wmatrix for current subject (and current no. of runs/stimulus)
    \item load SRM (based on non-left-out subjects' data)
    \item for every other subject: load empirical data and mask it
    \item aligned zmaps to shared space (k feautures x t time-points; 1 time-
        point cause it's a zmap no time-series) using zmaps\_in\_cms =
        srm.transform(masked\_zmaps)
    \item for every zmap in CMS, compute dot product of left-out subject
        weight-matrix with zmap in cms(= transform zmap from CMS into subject
        space)
    \item take the mean of transformed zmaps within the left-out subjects space
    \item transform the array back into 3d image
    \item
\end{itemize}


\subsubsection{plot\_bland-altman.py}
\begin{itemize}

\item lore

\item ipsum

\end{itemize}


\subsubsection{corrstats.py}
\begin{itemize}

\item lore

\item ipsum

\end{itemize}


\subsubsection{statistics\_t-test-correlations.py}
\begin{itemize}

\item lore

\item ipsum

\end{itemize}


\subsubsection{plot\_stripplot.py}
\begin{itemize}

\item lore

\item ipsum

\end{itemize}


\subsubsection{statistics\_cronbachs.py (in test folder)}
\begin{itemize}

\item lore

\item ipsum

\end{itemize}


\subsubsection{alternative template creation}
%
lastly, we currently apply another method to get the „z-map template“ on which
the prediction is based
\begin{itemize}

\item data\_srm-vis-to-ind.py
\item data\_denoise-vis.py

\end{itemize}


\section{Results}
%
For every subject, we see the correlation of z-maps that tell us the, quote
``real'' unquote, PPA and predicted PPA
%
In green, we see the correlations between empirical values from the localizer \&
the predicted values using anatomical alignment
%
In orange, we see the correlations between empirical values \& the predicted
values using parts of the movie
%
In blue, we see the correlations between empirical values \& the predicted
values using parts of the audio-description
%
I marked subject 4, because I want to show you how results look like in a
horizontal slice of the brain of subject 4
%
We have these nice blurry EPI-images and all z-maps are threshold at a value of
bigger than 2.3.
%
always in red, we can see the z-map from the localizer experiment across the
whole brain,
%
the region of interest that we used is white and the predicted values, are blue
%
The prediction using anatomical alignment and the prediction using 15 minutes of
movie data show a correlation of about .7
%
the prediction using 15 minutes of the audio-description correlates about 0 with
the empirical z-map
%
The last one is the extreme case, but it can give you an idea of how the z-maps
look in a slice of the brain



\section{Discussion}


\subsection{Discussion of current results}
%
15 min of movie watching used for functional alignment outperform prediction
using anatomical alignment
%
30 minutes of movie watching outperform 15 minutes of movie watching
%
more than 30 minutes do not lead to a significantly improved prediction
performance.
%
assumption: visual localizer = ``ground truth''


\subsection{Future questions}
%

\subsubsection{Proof of concept}
%

\subsubsection{ROI vs. whole brain}
%
more (?) data vs. searchlight algorithm


\subsubsection{predict other t-contrasts of localizer}


\subsubsection{predict other localizers}
%
e.g. retinotopy, language areas


\subsubsection{create CMS from other study}
%
we currently have a cross-subject and cross-experiment prediction,
but we do not have a (real) cross-scanner prediction
%
create a CMS from another experiment’s data,
using another scanner and hopefully more subjects
%
In case of an alignment of time-series,
that experiment needs, at least, a part of Forrest Gump as an intersection


\subsection{the vision}
%
functional atlas
%
Imagine you scan a new, unknown subject for just 15 minutes more, and you
additionally get results from a whole variety of other paradigms mapped onto
that brain
%
Results from localizers of low-level perceptual processes, but also higher-level
cognitive processes like language, memory, emotions and so on
%
And I did not write that onto the slides because it is just a vague idea:
%
If you have different common model spaces for different subgroups, you can
investigate which alignment onto which ``subgroup common model space'' results
in less error, that lets you classify to which subgroup your new subject might
belong.


\section{Conclusion}
