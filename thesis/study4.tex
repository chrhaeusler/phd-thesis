% \noindent\vspace{0.25in}

\begin{chapterabstract}

% scientific background}
Lore ipsum...
% current state of research

% research question and objectives

% methodology

% results

% discussion

% conclusions

\end{chapterabstract}



\pagebreak

\todo[inline]{negative control: shuffle runs within or across paradigms??? imo,
across...}

\todo[inline]{finalize figures \& figure captions}

\todo[inline]{note: I could not manage to only speak of "participants";
"participant" and "subject" are used interchangably}


\section{Introduction}

% higher visual areas higher visual areas
In the domain of higher-visual perception, functionally defined
category-selective brain regions, such as the \acl{ppa} (PPA)
\citep{epstein1998ppa}, the \acl{ffa} (FFA) \citep{kanwisher1997ffa}, or the
\ac{eba} \citep{downing2001bodyarea} exhibit significantly increased \acf{bold}
activity correlated with a ``preferred'' \citep[][p.
123]{debeck2008interpreting} stimulus category.
%
While the topographies, i.e. the location, size and shape, of these
category-selective areas are similarly distributed across individuals, their
exact topographies vary interindividually \citep{rosenke2021probabilistic,
zhen2017quantifying, zhen2015quantifying, frost2012measuring}.
% definition of localizer
To identify the topography of functional areas in individual persons,
block-design \textit{functional localizer} paradigms are traditionally used that
contrast modeled hemodynamic responses correlating with the corresponding
stimulus category, such as landscapes, faces, or bodies.
% problem: one localizer for one domain
Functional localizers are designed to maximize detection power and thus limited
to mapping only one domain of brain functions, such as category-selective
regions \citep{stigliani2015temporal}, retinotopic visual areas
\citep{wang2015probabilistic}, theory of mind \citep{spunt2014validating}, or
semantic processes \citep{fedorenko2010new, fernandez2001language}.
% which gets messy
However, when mapping multiple functional domains in a limited amount of time is
desired, the approach ``one paradigm for one domain of functions'' becomes
impractical.
% localizer batteries: intro
To address this issue, researchers have attempted to create time-efficient,
multi-functional \textit{localizer batteries} \citep[e.g.,][]{barch2013function,
drobyshevsky2006rapid, pinel2007fast}.
% task based = shit
Nevertheless, the diagnostic quality of localizer paradigms heavily depends on a
participant's comprehension of the task instructions and general compliance, a
criteria that can be difficult to meet in clinical or pediatric populations
\citep{eickhoff2020towards, vanderwal2019movies}.

% ppa via audio-description Results also suggest that a naturally engaging,
% purely auditory paradigm like an audio-description could, in principle,
% substitute a visual localizer as a diagnostic procedure to assess brain
% functions in visually impaired % individuals \citep{haeusler2022processing}.
In a previous study \citep{haeusler2022processing}, we demonstrated that a
functionally defined region such as the \ac{ppa} can be localized using  a
\acf{glm} that is based on the annotated temporal structure of a two-hour long
naturalistic stimulus.
% full feature film is too long
However, conducting a two-hour long \ac{fmri} scan session may not be desirable
or feasible due to potential compliance issues or constraints on time and
resources.
% hence, predict from reference
An alternative approach that addresses the challenges of a lengthy scanning
procedure is to localize a functional area in an individual by leveraging data
collected from an independent sample of other individuals (i.e.  from a
\textit{reference group}).
% intro: estimation via common anatomical space
Previous studies have estimated the most probable location of a functional area
in an individual from a reference group by performing either a volume-based
\citep{zhen2017quantifying, zhen2015quantifying} or a surface-based
\citep{frost2012measuring, weiner2018defining, rosenke2021probabilistic,
wang2015probabilistic} \textit{anatomical alignment}.
%
First, in order to address the issue of anatomical variability across persons,
functional data of persons in the reference group are anatomically aligned with
(i.e.  projected into) a common anatomical space, such as the Montreal
Neurological Institute brain atlas \citep[MNI152 atlas;][]{fonov2011unbiased}.
% project into test subject to estimate
Then, data are projected from the common anatomical space into the individual
person's brain anatomy to provide an estimate of a functional region's location.
% volume-based alignment in one sentence
Volume-based anatomical alignment \citep[s.][for a review]{klein2009evaluation}
aligns voxels to a three-dimensional common anatomical space \citep[e.g., MNI152
atlas;][]{fonov2011unbiased}.
% surface-based alignment in one sentence
Surface-based anatomical alignment \citep{fischl1999cortical, yeo2009spherical}
aligns vertices to a two-dimensional common anatomical space \citep[e.g.,
FreeSurfer's fsaverage template;][]{fischl1999high}.
% difference in one sentence
Whereas volume-based alignment does not account for individual sulcal and gyral
folding patterns, surface-based alignment respects interindividual variability
of the cortical surface.
% surface-based estimation works better
Consequently, previous studies that compared volume-based and surface-based
alignment to estimate the location of functional regions have shown that
surface-based alignment reduces interindividual variability and improves
estimation performance \citep{rosenke2021probabilistic, frost2012measuring,
wang2015probabilistic, weiner2018defining}.
% remaining variability after surface-based alignment
However, even after surface-based alignment, the anatomical location of
functional regions varies between individuals \citep{coalson2018impact,
benson2014correction, natu2021sulcal, wang2015probabilistic, frost2012measuring,
langers2014assessment, weiner2014mid, rosenke2021probabilistic}.
% frost as an example
\citet{frost2012measuring}, for example, localized 13 functional areas of the
high-level visual cortex and ``found a large variability in the degree to which
functional areas respect macro-anatomical boundaries'' \citep[][p.
1369]{frost2012measuring}.
% functional--anatomical correspondence
The remaining variability indicates that functional areas a not necessarily
bound to anatomical landmarks, and reflects the degree of
\textit{functional--anatomical correspondence} between a brain function and its
underlying anatomical location.

% case of PPA cf. also \citet{frost2012measuring, rosenke2021probabilistic}
% \citet{weiner2018defining} showed ``that cortical folding patterns and
% probabilistic predictions reliably identify place-selective voxels in medial
% VTC across individuals and experiments''.
%
% However, ``this structural-functional coupling is not always perfect and there
% is inter-subject variability as to how much the place-selective voxels extend
% within the parahippocampal gyrus, as well as the lingual gyrus and medial
% aspects of the fusiform gyrus.
%
% Despite this inter-subject variability, place-selective voxels are always
%located within the collateral sulcus across participants.''
%\citep{weiner2018defining}.
In order to address the issue of functional-anatomical variability across
subjects, \textit{functional alignment} algorithms, such as
\textit{hyperalignment} \citep{haxby2011common, guntupalli2016model} or the
\acfi{srm} \citep{chen2015reduced, zhang2016searchlight}, have been developed.
%
Whereas anatomical alignment aligns voxels (or vertices) that share the same
anatomical location to a common anatomical space, functional alignment aligns
voxels (or vertices) that share similar functional properties to a \acfi{cfs}.
%
Functional alignment algorithms are typically used to compute both a
high-dimensional, functional brain template (i.e. the \ac{cfs}) and
subject-specific transformations based on functional data of a study's
participants.
%
A subject-specific transformation allows to project functional data from a
subject's three-dimensional voxel space into the \ac{cfs}.
%
Conversely, the inverse transformation allows to project data from the \ac{cfs}
into the subject's voxel space \citep{haxby2020hyperalignment,
kumar2020brainiak}.
%
The \ac{cfs} and transformations are computed (i.e. \textit{trained}) by either
maximizing the interindividual similarity of \ac{bold} response time series
correlating with a time-locked external stimulation \citep{haxby2011common,
chen2015reduced, sabuncu2010function}, or by maximizing the interindividual
similarity of connectivity profiles \citep{feilong2018reliable,
guntupalli2018computational, nastase2019leveraging}.
%
While connectivity-based functional alignment has been shown to be more
effective in aligning connectivity profiles, response-based functional alignment
is more effective in aligning response time-series
\citep{guntupalli2018computational}.
%
Although functional alignment algorithms can be applied to \ac{fmri} time series
data from paradigms employing simplified stimuli, data from naturalistic stimuli
provide
%
improved generalizability of the \ac{cfs}
%
and transformations
%
to novel stimuli or tasks.
%
This is presumably because naturalistic stimuli sample a broader range of brain
states \citep{haxby2011common, guntupalli2016model}.

Consequently, a more recent procedure \citep[e.g.,][]{jiahui2020predicting,
guntupalli2016model, haxby2011common} to estimate the most probable location of
a functional area in an individual from a reference performs a functional
alignment.
% solve functional-anatomical variability
First, the functional data of individuals in the reference group are
anatomically aligned with a \ac{cas}.
%
Second, to address the issue of functional-anatomical variability across
individuals, the data are functionally aligned with (i.e. projected into) a
\ac{cfs}.
%
Finally, data are projected from the \ac{cfs} into the individual's
brain anatomy, serving as an estimate of a functional region's location.
% Example: Jiahui (2020)
For instance, \citet{jiahui2020predicting} used surface-based hyperalignment to
calculate \acp{cfs} and transformations based on data from
%
the movie ``Grand Budapest Hotel'' ($\approx$\unit[50]{min};
\ac{tr}=\unit[1]{s}) and
%
the movie ``Forrest Gump'' ($\approx$\unit[120]{min}; \ac{tr}=\unit[2]{s}).
%
\citet{jiahui2020predicting} then estimated $t$-contrast maps of a visual
localizer that aimed to identify the \ac{ffa} by projecting the $t$-contrast
maps of a reference group through each \ac{cfs} into an individual's brain
anatomy.
%
Results showed that $t$-contrast maps of the visual localizer correlated more
highly with contrast maps that were estimated via hyperalignment than contrast
maps that were estimated via surface-based anatomical alignment.

% An orthonormal matrix and a matrix with orthonormal columns are closely
% related concepts, but there is a difference between the two:

% Orthonormal Matrix: An orthonormal matrix is a square matrix in which all its
% columns and rows are orthogonal to each other and have a length of 1. In other
% words, the dot product of any two distinct columns (or rows) of an orthonormal
% matrix is 0, and the length (norm) of each column (or row) is 1. Additionally,
% the transpose of an orthonormal matrix is equal to its inverse.

% Matrix with Orthonormal Columns: A matrix with orthonormal columns refers to a
% rectangular matrix in which the columns are pairwise orthogonal and have a
% length of 1. The matrix may have more columns than rows, meaning it is not
% necessarily a square matrix. The dot product of any two distinct columns is 0,
% and the length of each column is 1.

% The dot product of an orthonormal column vector with itself is always 1. This
% is because an orthonormal column vector has a length (norm) of 1, and the dot
% product of a vector with itself is equal to the square of its length.
% Mathematically, if v is an orthonormal column vector, then:
% v · v = ||v||^2 = 1^2 = 1
% Therefore, the dot product of an orthonormal column vector with itself is
% equal to 1.

% In summary, the main distinction lies in the shape of the matrix. An
% orthonormal matrix must be square, with both rows and columns being
% orthonormal, while a matrix with orthonormal columns can be rectangular, with
% only the columns satisfying the orthonormality condition.

% A matrix with orthonormal columns is not necessarily a square matrix.
% A matrix with orthonormal columns is called a column-orthonormal matrix or a
% matrix with orthonormal column vectors. This means that the columns of the
% matrix are unit vectors (vectors with a magnitude of 1) and they are
% orthogonal to each other (the dot product between any two columns is zero).
% Such a matrix can have any number of rows and columns. It can be a rectangular
% matrix, where the number of rows is not necessarily equal to the number of
% columns. The columns may still be orthonormal even if the matrix is not
% square.

% An orthonormal matrix is necessarily a square matrix. In order for a matrix to
% be orthonormal, it must have the same number of rows and columns. This is
% because the conditions for orthonormality involve the dot product between
% columns, which can only be computed when the matrices have the same number of
% elements in each column.
% So, by definition, an orthonormal matrix is always square, meaning it has an
% equal number of rows and columns.

% focus: ppa
Here again, our focus is on the \ac{ppa} \citep[e.g.,][for
reviews]{epstein2014neural, aminoff2013role}.
%
We investigated whether we can estimate the results of $t$-contrasts (i.e.
statistical $Z$-maps) created in previous studies that aimed to identify the
\ac{ppa} using response time series from three different paradigms:
%
(a) a classic visual localizer \citep{sengupta2016extension} as the assumed
``gold standard'' to localize the \ac{ppa},
%
(b) a movie \citep{haeusler2022processing}, and
%
(c) an auditory narrative \citep{haeusler2022processing}.
%
To obtain \textit{predicted $Z$-maps} of these \textit{empirical $Z$-maps}, we
employed a volume-based functional alignment approach that utilizes the \ac{srm}
\citep{chen2015reduced, richard2019fast}.
% as implemented in the open-source software package BrainIAK \citep[Brain
% Imaging Analysis Kit;
% \href{https://brainiak.org}{\url{brainiak.org}};][]{kumar2020brainiak,
% kumar2020brainiaktutorial}.  general overview of SRM
The \ac{srm} is an unsupervised probabilistic latent-factor model that
decomposes response time series of participants who have experienced the same
stimulus into a \ac{cfs} of \textit{shared features} \citep[also known as
\textit{shared feature space};][]{chen2015reduced} and subject-specific linear
transformations.
% math stuff
Specifically, the \ac{srm} algorithm uses each $n^{th}$ subject's response time
series represented as matrix $X_{n}$ ({$v$} voxels by $t$ time points) to
compute the \ac{cfs} $C$ ($k$ shared responses by $t$ time points) and
subject-specific transformation matrices $W_{n}$ ($v$ voxels by $k$ shared
responses) with orthonormal columns ($W_{n}^{T}W_{n}=I_{k}$).
% iteratively fitted
The algorithm randomly initializes and fits the transformation matrices over
iterations to minimize the error in explaining the participants' data, while
also learning the time course of the shared responses (cf.
\href{https://brainiak.org/tutorials/11-SRM/}{\url{brainiak.org/tutorials/11-SRM}}).
% number of dimensions
Unlike hyperalignment, the number of dimensions of the \ac{cfs} is not set by
the number of voxels, but rather it is determined by the researcher to a number
lower than the number of voxels, a procedure that also filters out noise and
reduces overfitting \citep{chen2015reduced}.
% phrase math in words
Each shared feature can be thought of as a weighted sum of many voxels across
subjects \citep{kumar2020brainiak}.
% result = alignment
A subject-specific transformation matrix represents the weight of each voxel in
a subject's voxel space on each shared feature, and allows to functionally align
subjects by projecting hemodynamic responses within the voxels into the
$k$-dimensional \ac{cfs}.

% multi-paradigm model
In contrast to previous studies \citep{jiahui2020predicting,
guntupalli2016model, haxby2011common} that calculated a \ac{cfs} based on data
from a single paradigm, we calculated a \textit{multi-paradigm \ac{cfs}} based
on data from three different paradigms.
% cross-validation
Following an exhaustive leave-one-subject-out cross-validation, each
\textit{training subject}'s response time series from
%
the movie ``Forrest Gump'' ($\approx$\unit[120]{min}, split into eight runs;
\ac{tr}=\unit[2]{s}),
%
the movie's audio-description that was produced for a visually impaired audience
($\approx$\unit[120]{min}, split into eight runs runs; \ac{tr}=\unit[2]{s}), and
%
the visual localizer ($\approx$\unit[20]{min} split into four runs;
\ac{tr}=\unit[2]{s})
%
were concatenated and fed into the \ac{srm} algorithm in order to calculate the
\ac{cfs} and the training subjects' transformations (s.
Fig.~\ref{fig:multi-stimulus-cfs}).
% test subject's transformation
We then aligned the \textit{test subject}'s with the \ac{cfs} to obtain the test
subject's transformation.
%
In order to investigate the prediction performance of each paradigm, a test
subject's response time series from each of the three paradigms was separately
aligned with the paradigm's corresponding \acp{tr} within the \ac{cfs} letting
us obtain transformation matrices based on each paradigm.
%
In other words, the time series of each paradigm served as a separate predictor
to estimate three different empirical $Z$-maps (i.e. one
\textit{cross-subject-within-paradigm prediction} and two
\textit{cross-subject-cross-paradigm predictions} per paradigm).
%
Further, considering that acquiring functional data from a two-hour long
naturalistic stimulus to align an individual to a \ac{cfs} may not be desirable
or feasible, we also investigated the relationship between the quantity of data
of each predictor used to obtain a test subject's transformation and the
subsequent performance of estimating each $Z$-map.
%
Our results indicate that an auditory narrative can be employed to estimate the
results of a visual localizer, although it requires a longer functional scanning
session.
%
Additionally, we find that $\approx$\unit[15]{min} of movie data sampled at
\unit{0.5}{Hz} used for volume-based functional alignment can estimate the
results of the visual localizer more accurately than an estimation procedure
based on non-linear volume-based anatomical alignment.
%
This opens up the possibility of estimating results from many localizer
paradigms using a naturalistic stimulus of similar duration of one
localizer to gain insights into individual functional brain anatomy.



\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/multi-stimulus-cfs.pdf}
\caption{
%
    \textbf{Overview of the shared response model (SRM).
}
    %
    For each fold of the leave-one-subject-out cross-validation, each training
    subject's response time series from
    %
    the movie ($\approx$\unit[120]{min}; \acf{tr}=\unit[2]{s}),
    %
    the movie's audio-description ($\approx$\unit[120]{min};
    \ac{tr}=\unit[2]{s}),
    %
    and the visual localizer ($\approx$\unit[20]{min}; \ac{tr}=\unit[2]{s})
    %
    were concatenated to serve as the input for the \ac{srm} algorithm.
    %
    From these response time series represented as matrix $X_{n}$ ({$v$} voxels
    by $t$ time points), the algorithm calculates the common functional
    space (CFS) $C$ ($k$ shared features by $t$ time points) and
    subject-specific transformation matrices $W_{n}$
    ($v$ voxels by $k$ shared features) with orthonormal columns
    ($W_{n}^{T}W_{n}=I_{k}$).
} \label{fig:multi-stimulus-cfs} \end{figure*}


\section{Methods}

% we get the data from the naturalistic PPA paper (its subdataset) datalad get
% -n inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned datalad
%  get
%  inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in\_bold3Tp2/sub-??\_task-a?movie\_run-?\_bold*.*

% reference to PPA-Paper
For the current study, we used the same subset of the studyforrest dataset as in
\citet{haeusler2022processing}.
%
The same fourteen participants
% VIS
participated in a six-category block-design visual localizer
\citep{sengupta2016extension},
% AV
watched the audio-visual movie ``Forrest Gump'' \citep{hanke2016simultaneous},
and
% AD
listened to the movie's audio-description \citep{hanke2014audiomovie}.
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications, while a summary is provided in \citet{haeusler2022processing}.



\subsection{Preprocessing}

% data sources
The analyses in this study were conducted on the same preprocessed \ac{fmri}
data (s.
\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned
}{\url{github.com/psychoinformatics-de/studyforrest-data-aligned}}) that were
used for
%
(a) the technical validation of the dataset \citep{hanke2016simultaneous},
%
(b) the localization of higher-visual areas \citep{sengupta2016extension}, and
%
(c) the investigation of responses of the \ac{ppa} correlating with naturalistic
spatial information \citep{haeusler2022processing}.
%
We reran the preprocessing and analyses steps performed in
\citet{sengupta2016extension} and \citet{haeusler2022processing} using FEAT
v6.00 \citep[FMRI Expert Analysis Tool;][]{woolrich2001autocorr} as shipped with
FSL v5.0.9 \citep[\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software
Library;}][]{smith2004fsl} to reproduce the time series that served as input for
the previous statistical analyses and their results (i.e. the empirical
$Z$-maps).
% temporal filtering
The preprocessing steps included high-pass temporal filtering (using a
Gaussian-weighted least-squares straight line) for every run of the visual
localizer (cutoff period of \unit[100]{s}) and naturalistic stimuli (cutoff
period of \unit[150]{s}).
% brain extraction & spatial smoothing
Brain extraction was performed using BET \citep{smith2002bet}, and data from all
three paradigms were spatially smoothed using a Gaussian kernel with a full
width at half maximum of \unit[4.0]{mm}.
% grand mean normalization
A grand-mean intensity normalization was applied to each run of the functional
localizer (four runs, each lasting $\approx$\unit[5]{min}; \ac{tr}=\unit[2]{s})
and naturalistic stimuli (eight runs, each lasting $\approx$\unit[15]{min};
\ac{tr}=\unit[2]{s}).
%
Further analyses on these reproduced times series were performed using Python
(v3.7) scripts that relied on
%
NiBabel v4.0.2 (\href{https://nipy.org}{\url{nipy.org}}),
%
NumPy v1.21.6 (\href{https://numpy.org}{\url{numpy.org}}),
%
Pandas v1.3.5 (\href{https://pandas.pydata.org}{\url{pandas.pydata.org}}),
%
Scipy v1.7.3 (\href{https://scipy.org}{\url{scipy.org}}),
%
scikit-learn v1.0.2 (\href{https://scikit-learn.org}{\url{scikit-learn.org}}),
%
BrainIAK v0.11
\citep[\href{https://brainiak.org}{\url{brainiak.org}}][]{kumar2020brainiak,
kumar2020brainiaktutorial},
%
Matplotlib v3.5.3 (\href{https://matplotlib.org}{\url{matplotlib.org}}),
%
seaborn v0.12.2 (\href{https://seaborn.pydata.org}{\url{seaborn.pydata.org}}),
%
and calling command line functions of FSL.

%\paragraph{Fixing FSL output}

% grand_mean_for_4d.py (formerly: data_normalize_4d.py):
% is not necessary anymore: FSL has applied grand mean scaling to
% 'filtered_func_data.nii.gz'

% input: 'sub-*/run-?.feat/filtered_func_data.nii.gz' (of VIS, AO & AV)
% output: saved to 'sub-??_task-*_run-?_bold_filtered.nii.gz'

% FSL adds back the mean value for each voxel's time course at the end of the
% preprocessing;
% hence, the script subtracts that mean again but multiplies it by 10000
% (like FSL does it, too)

% definition of grand mean scaling for 4d data:
% voxel values in every image are divided by the average global mean
% intensity of the whole session. This effectively removes any mean global
% differences in intensity between sessions.

% FSL User Guide:
% filtered_func_data will normally have been temporally high-pass filtered,
% it is not zero mean; the mean value for each voxel's time course has been
% added back in for various practical reasons.
% When FILM begins the linear modeling, it starts by removing this mean.

% masks-from-mni-to-bold3Tp2.py:
% - merges unilateral ROIs overlaps (already in MNI) to bilateral ROI
% - output: 'masks/in_mni/PPA_overlap_prob.nii.gz'
% - warps union of ROIs from MNI into each subjects space
% output: 'sub-*/masks/in_bold3Tp2/grp_PPA_bin.nii.gz' + audio_fov.nii.gz dilate
% the ROI masks by 1 voxel; output: 'grp_PPA_bin_dil.nii.gz'

% masks-from-mni-to-bold3Tp2.py:
% warp MNI masks into individual bold3Tp2 spaces

% masks-from-t1w-to-bold3Tp2.py:
% transforms 'inputs/tnt/sub-*/t1w/brain_seg*.nii.gz'
% into individual's bold3Tp2
% output: 'sub-*/masks/in_bold3Tp2/brain_seg*.nii.gz'

% mask-builder-voxel-counter.py:
% builds different individual masks by dilating, merging other masks
% creates a FoV of AO stimulus for every subject from 4d time-series of AO run
% output: sub-*/masks/in_bold3Tp2/audio_fov.nii.gz'
% counts the voxels
% long story short: we cannot used all gyri that contain PPA to some degree
% even if the mask by FoV of AO stimulus and individual gray matter mask

% data_mask_concat_runs.py:
% masks are not dilated and not masked with subject-specific gray matter mask
% outputs:
% 'sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy
% 'sub-*_task_visloc_run-1-4_bold-filtered.npy'

% reason why we do it
The \ac{srm} requires that the number of samples (i.e. \acp{tr}) exceed the
number of features (i.e. voxels).
%
In order to restrict the number of voxels, we created bilateral \acp{roi} for
each participant.
%
Specifically, we warped the union of individual \acp{ppa} \citep[s. Fig. 1
in][]{haeusler2022processing} from MNI space into each participant's voxel space
using subject-specific, non-linear transformation matrices that were previously
computed
\citep[][\href{https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms
}{\url{github.com/psychoinformatics-de/studyforrest-data-templatetransforms}}]{hanke2014audiomovie}.
% applying masks
The time series data of each participant were then masked in their native voxel
space by the union of individual \acp{ppa} and the subject-specific \ac{fov} of
the audio-description.
% voxels = [1665, 1732, 1400, 1575, 1664, 1951, 1376, 1383, 1683, 1887, 1441,
% 1729, 1369, 1437] median = 1619.5
The number of remaining voxels per participant (range 1369--1951,
$\overline{X}=1592$, $SD=188$) can be seen in Fig.~\ref{fig:plot_voxel-counts}.
% normalization
Data of each run were normalized ($z$-scored) to a mean of zero
($\overline{X}=0$) and a standard deviation of one ($SD=1$).
%
Due to an image reconstruction problem \citep[cf.][]{hanke2014audiomovie}, the
last 75 \acp{tr} of the audio-description were missing in subject 04.
%
The \ac{srm} allows for different numbers of voxels across subjects, but the
number of \acp{tr} must be the same.
%
Consequently, we removed the last 75 \acp{tr} of the audio-description from the
time series of all other participants.
% summary; AO + AV = 7123 TRs (not 7198 TRs anymore); localizer has 4 x 156 TRs
As a result, the data used to fit the \ac{srm} in the next step included 3599
\acp{tr} of the movie, 3524 \acp{tr} of the audio-description, and 624 \acp{tr}
of the visual localizer (7747 \acp{tr} in total).
%% concatenate and z-score
The time series of all three paradigms were concatenated and $z$-scored.

\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/plot_voxel-counts.pdf}
\caption{
%
\textbf{Number of voxels in the bilateral regions of interest (ROIs)
of each participant.}
%
In order to reduce the number of voxels, we warped the union of
individual \acp{ppa} \citep[cf. Fig. 1 in][]{haeusler2022processing} from
MNI space into each participant's native voxel space.
%
The remaining voxels of each participant were further constrained to those
voxels that are included in the respective participant's \ac{fov} of the
audio-description \citep[cf.][]{hanke2014audiomovie}.
}
\label{fig:plot_voxel-counts}
\end{figure*}


\begin{comment}

The number of remaining voxels per participant can be seen in Table
\ref{tab:ppamaskvoxels} (range 1369--1951, $\overline{X}=1592$, $SD=188$).


\begin{table*}[btp] \caption{
%
\textbf{Table heading.}
%
The number of remaining voxels after masking time series data of each paradigm
and participant with the union of individual \acp{ppa} warped from MNI space
into each participant's subjects-space and the subject-specific field of view
of audio-description.
    }

\label{tab:ppamaskvoxels}
\begin{tabular}{ll}
\toprule
\textbf{Subject} & \textbf{no. of voxels} \\
\midrule
sub-01 & 1665 \tabularnewline
sub-02 & 1732 \tabularnewline
sub-03 & 1400 \tabularnewline
sub-04 & 1575 \tabularnewline
sub-05 & 1664 \tabularnewline
sub-06 & 1951 \tabularnewline
sub-14 & 1376 \tabularnewline
sub-09 & 1383 \tabularnewline
sub-15 & 1683 \tabularnewline
sub-16 & 1887 \tabularnewline
sub-17 & 1441 \tabularnewline
sub-18 & 1729 \tabularnewline
sub-19 & 1369 \tabularnewline
sub-20 & 1437 \tabularnewline
\bottomrule
\end{tabular}
    % \caption*{The legend text goes here.}
\end{table*}

\end{comment}


\subsection{Estimation via functional alignment}
%
To estimate the empirical $Z$-maps (i.e. the results of the three $t$-contrast),
we followed a three-step procedure.
% create CFS and training subjects' matrices
First, for every fold of the leave-one-subject-out cross-validation (N$=$14
participants), we fit a \ac{srm} to $N-1$ training subjects' response response
time series from the movie, the audio-description, and the visual localizer.
% results in...
This step generated a \ac{cfs} for each fold of the cross-validation and a
transformation matrix with orthonormal columns for each training subject.
% align test subject
Second, we aligned the test subject's response time series data from the movie,
audio-description, and visual localizer paradigms separately to the
corresponding \acp{tr} within the \ac{cfs}.
%
This procedure yielded different transformation matrices for the test subject
based on data from different paradigms.
% quantity vs. performance
In order to examine the relationship between the estimation performance and the
amount of data used to generate a transformation matrix, we also varied the
number of runs of the paradigms.
%
This additional procedure during step two produced transformation matrices based
on an increasing number of runs per paradigm.
%
In the third step, we estimated a test subject's empirical $Z$-map by first
projecting the training subjects' empirical $Z$-maps from their voxel space into
the \ac{cfs} using their transformation matrices.
% project from CFS into test subject
Then, we projected the training subjects' $Z$-maps from the \ac{cfs} into the
test subject's voxel space using the transpose of the test subject's
transformation matrix (that is equivalent to the inverse of the transformation
matrix due to its orthonormal columns).
% actual prediction
Finally, we obtained the test subject's predicted $Z$-maps by calculating the
arithmetic mean of the respective paradigm's projected empirical $Z$-maps.



\subsubsection{Fitting the SRM}
%
In order to obtain the \ac{cfs} and the training subjects' transformation
matrices, we used the probabilistic \ac{srm} algorithm that is implemented in
BrainIAK v.11 \citep[Brain Imaging Analysis Kit;][]{kumar2020brainiak,
kumar2020brainiaktutorial}, and approximates the \ac{srm} based on the
Expectation Maximization (EM) algorithm as proposed by \citet{chen2015reduced}
and optimized by \citet{anderson2016enabling}.
% number of dimensions / features ``The effect of number of PCs on BSC was
% similar for models that were based only on Princeton (n = 10) or Dartmouth (n
% = 11) data, suggesting that this estimate of dimensionality is robust across
% differences in scanning hardware and scanning parameters''
% \citep{haxby2011common}.
%
% ``These dimensionality estimates are a function of the spatial and temporal
% resolution of fMRI and the number and variety of response vectors used to
% derive the common space'' \citep{guntupalli2016model}.
%
% ``The true dimensionality of representation in human cortex surely involves
% vastly more distinct tuning functions. Estimates of the dimensionality of
% cortical representation, therefore, will almost certainly be much higher as
% data with higher spatial and temporal resolution for larger and more varied
% samples of response vectors are used to build new common models''
% \citep{guntupalli2016model}.
We chose a value of $k=10$ for the number of shared features (i.e. the number of
dimensions in the \ac{cfs}) based on the temporal and spatial resolution of our
data (\ac{tr} = \unit[2]{s}; \unit[2.5 $\times$ 2.5 $\times$ 2.5]{mm}), the
average number of voxels per \ac{roi}, and findings from
\citet{haxby2011common}.
%
\citet{haxby2011common} used hyperalignment to create a \ac{cfs} of 1,000
dimensions based on functional data (\ac{tr} = \unit[3]{s}) of voxels (\unit[3
$\times$ 3 $\times$ 3]{mm}) located in the ventral temporal cortex.
%
They then reduced the dimensionality of the \ac{cfs} by applying a \ac{pca} in
order to determine the subspace that is sufficient to capture the full range of
response-pattern distinctions.
%
Results revealed that approximately 35 principal components (i.e. dimensions)
were sufficient to represent the information content of a movie from which the
\ac{cfs} was derived.
%
Results also showed that the cortical topographies of category-selective brain
regions was preserved in the 35-dimensional \ac{cfs}.
% ...as judged by visual inspection
In the present study, we also computed \acp{cfs} of $k=5, 20, 30, 40, 50$ but
the prediction performance based on these \acp{cfs} barely varied from a
10-dimensional \ac{cfs}.
% iterations:
The algorithm was set to iterate 30 times to minimize the error.

% correlations of regressors
In order to visualize characteristics of the \ac{cfs}, we calculated the Pearson
correlation coefficients between the shared responses and the regressors that we
previously created to model hemodynamic responses during the three paradigms
\citep[cf.][]{sengupta2016extension, haeusler2022processing}.
%
As an example, we chose the \ac{cfs} that was created in the first fold of the
cross-validation from $N-1$ subjects to estimate the $Z$-maps of subject 01.
%
The time series of the shared features were trimmed to match the corresponding
\acp{tr} of the respective paradigms.
%
Fig.~\ref{fig:corr-vis-reg-srm} shows the correlations between regressors
created to model hemodynamic responses during the visual localizer and shared
responses (trimmed to the \acp{tr} that match the visual localizer).
Fig.~\ref{fig:corr-av-reg-srm} shows the correlations between regressors created
to model hemodynamic responses during the movie \citep[cf. Table 3
in][]{haeusler2022processing} and shared responses, while
Fig.~\ref{fig:corr-ao-reg-srm} shows the correlations between regressors created
to model hemodynamic responses during the audio-description \citep[cf. Table 3
in][]{haeusler2022processing} and shared responses.


\todo[inline]{make colors of non-used TRs more transparent (alpha=.15?)}

\todo[inline]{shuffle runs across paradigms, not just within
paradigms???}

% shuffle runs
As a negative control, we created 1000 models based on randomly shuffled time
series.
%
We expected that the \ac{srm} algorithm would yield ``shared'' responses that
are not correlated with the regressors.
%
For each of the 1000 models, the order of runs of the visual localizer and
naturalistic stimuli were shuffled separately for each paradigm and training
subject.
%
Then, we concatenated the time series, fit the \ac{srm}, calculated the Person
correlation coefficients per model, and finally their means across the 1000
models.
%
As hypothesized, the shared features within \acp{cfs} based on shuffled time
series show no or just minor mean correlations with the regressors, as shown in
%
Fig.~\ref{fig:corr-vis-reg-srm-shuffled},
%
Fig.~\ref{fig:corr-av-reg-srm-shuffled}, and
%
Fig.~\ref{fig:corr-ao-reg-srm-shuffled}.


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/corr_vis-regressors-vs-cfs_sub-01_srm-ao-av-vis_feat10-iter30_7123-7747.pdf}
\caption{
%
\textbf{Pearson correlation coefficients between regressors of the visual
localizer and shared features.}
%
The time series of the shared features within the multi-paradigm \ac{cfs}
%
(as calculated for subject 01 in the first fold of the cross-validation)
%
were trimmed to match the corresponding \acp{tr} of the visual localizer
paradigm \citep{sengupta2016extension}.
%
The six regressors of the visual localizer model hemodynamic responses to
the six categories of pictures that were presented in blocks.
}
\label{fig:corr-vis-reg-srm}
\end{figure*}


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/corr_av-regressors-vs-cfs_sub-01_srm-ao-av-vis_feat10-iter30_3524-7123.pdf}
\caption{
    %
    \textbf{Pearson correlation coefficients between regressors of the movie
    and shared features.}
    %
    The time series of the shared features within the multi-paradigm \ac{cfs}
    %
    (as calculated for subject 01 in the first fold of the cross-validation)
    %
    were trimmed to match the corresponding \acp{tr} of the movie
    \citep{hanke2016simultaneous}.
    %
    The regressors \texttt{vse\_new} to \texttt{vno\_cut} are based on
    annotations of movie frames, whereas the regressors
    \texttt{fg\_av\_ger\_lr} to \texttt{fg\_av\_ger\_ud} represent low-level
    visual or auditory confounds
    \citep[cf. Table 3 in][]{haeusler2022processing}.
    %
    \texttt{vse\_new}: change of the camera position to a setting not depicted
    before;
    \texttt{vse\_old}: change of the camera position to a recurring setting;
    %
    \texttt{vlo\_ch}: change of the camera position to another locale within
    the same setting;
    %
    \texttt{vpe\_new}: change of the camera position within a locale not
    depicted before;
    %
    \texttt{vpe\_old}: change of the camera position within a recurring locale;
    %
    \texttt{vno\_cut}: a pseudorandomly selected frames within a continuous
    movie shot;
    %
    \texttt{fg\_av\_ger\_lr}: left-right luminance difference;
    %
    \texttt{fg\_av\_ger\_lrdiff}: left-right volume difference;
    %
    \texttt{fg\_av\_ger\_ml}: mean luminance;
    %
    \texttt{fg\_av\_ger\_pd}: perceptual difference;
    %
    \texttt{fg\_av\_ger\_rms}: root mean square volume;
    %
    \texttt{fg\_av\_ger\_ud}: upper-lower luminance difference.
    }
\label{fig:corr-av-reg-srm}
\end{figure*}


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/corr_ao-regressors-vs-cfs_sub-01_srm-ao-av-vis_feat10-iter30_0-3524.pdf}
\caption{
    %
    \textbf{Pearson correlation coefficients between regressors of the
    audio-description and shared features.}
    %
    The time series of the shared features within the multi-paradigm \ac{cfs}
    %
    (as calculated for subject 01 in the first fold of the cross-validation)
    %
    were trimmed to match the corresponding \acp{tr} of the
    audio-description \citep{hanke2014audiomovie}.
    %
    The regressors \texttt{body} to \texttt{sex\_m} are based on
    annotated categories of nouns spoken by the audio-description's narrator,
    whereas the regressors \texttt{fg\_ad\_ger\_lrdiff} and
    \texttt{fg\_ad\_ger\_rms} represent low-level auditory confounds
    \citep[cf. Table 3 in][]{haeusler2022processing}.
    %
    \texttt{body}: trunk of the body; overlaid clothes;
    %
    \texttt{bpart}: limbs and trousers;
    %
    \texttt{fahead}: (parts) of the face or head;
    %
    \texttt{furn}: moveable furniture (insides \& outsides);
    %
    \texttt{geo}: immobile landmarks;
    %
    \texttt{groom}: rooms \& locales or geometry-defining elements;
    %
    \texttt{object}: moveable and countable entities with firm boundaries;
    %
    \texttt{se\_new}: a setting occurring for the first time;
    %
    \texttt{se\_old}: a recurring setting;
    %
    \texttt{sex\_f}: female name, female person(s);
    %
    \texttt{sex\_m}: male name, male person(s);
    %
    \texttt{fg\_ad\_lrdiff}: left-right volume difference;
    %
    \texttt{fg\_ad\_rms}: root mean square volume.
    %
    \texttt{geo\&groom} is a combination of regressors as used on the positive
    side of the primary contrasts aimed to localize the \ac{ppa}
    \citep[cf. Table 5 in][]{haeusler2022processing}.
    }
\label{fig:corr-ao-reg-srm}
\end{figure*}


\subsubsection{Alignment of a test subject}

% AO: 0-451, 0-892, 0-1330, 0-1818, 0-2280, 0-2719, 0-3261, 0-3524
% AV: 3524-3975, 3524-4416, 3524-4854, 3524-5342, 3524-5804, 3524-6243,
%     3524-6785, 3524-7123
% AO+AV: 0-7123

% in-code documentation of
% https://github.com/brainiak/brainiak/blob/master/brainiak/funcalign/srm.py
% says: # Solve the Procrustes problem; A =
% subjectFMRIdata.dot(SharedResponses.T) U, \_, V = np.linalg.svd(A,
% full\_matrices = False) return U.dot(V)

%
We aligned the test subject's response time series from the visual localizer,
the movie, or the audio-description to the corresponding \acp{tr} within the
\ac{cfs} by factorizing the response time series data via singular value
decomposition.
%
This step produced transformation matrices with orthonormal columns that allow a
linear transformation of data from a test subject's voxel space into the
\ac{cfs}.
%
To investigate how the amount of data used to acquire a transformation matrix
affects the estimation performance, we also varied the number of runs per
paradigm.
%
Specifically, we used
%
one up to four runs (each lasting $\approx$\unit[5]{min}) of the visual
localizer, and
%
one up to eight runs (each lasting $\approx$\unit[15]{min}) of the naturalistic
stimuli
%
to align the test subject's time series with the corresponding \acp{tr} within
the \ac{cfs}.
%
Therefore, for each test subject, we obtained four matrices based on data from
the visual localizer and eight different matrices per naturalistic stimulus,
each transformation matrix having a size of $v$ voxels by $k$ shared responses
but being based on an increasing amount of data used to calculate the linear
transformation.


\subsubsection{Estimation of a test subject's $Z$-maps}

% overview
We estimated the empirical $Z$-maps of the test subject by projecting the
empirical $Z$-maps of all training subjects trough the \ac{cfs} into the test
subject's voxel space.
% functional alignment; into CFS (calling srm.transform(masked\_zmaps))
First, we masked the empirical $Z$-maps of the training subjects with the same
subject-specific masks that we used to mask the time series data.
%
Then, we used the transformation matrices derived during the training of the
\ac{cfs} to map the masked empirical $Z$-maps from each training subject's voxel
space into the \ac{cfs}.
%
Next, we used the transpose of a transformation matrix obtained from the
alignment of the test subject to project the $Z$-maps from the \ac{cfs} into the
test subject's voxel space.
% take the mean
For each of the three $t$-contrasts, we obtained the test subject's predicted
$Z$-map by calculating the arithmetic mean of the respective paradigm's
projected empirical $Z$-maps.


\subsection{Estimation via anatomical alignment}
%
As a baseline, we used an non-linear anatomical alignment procedure to estimate
a test subject's empirical $Z$-maps.
%
First, we projected the masked empirical $Z$-maps of each paradigm and each
training subject from their native voxel space into the MNI space via previously
computed subject-specific transformation matrices
\citep[][\href{https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms}{\url{github.com/psychoinformatics-de/studyforrest-data-templatetransforms}}]{hanke2014audiomovie}.
% from MNI into subject
We then used the test subject's pseudoinverse transformation matrix to project
the data from MNI space into the test subject's voxel space.
% take the mean
Similar to our functional alignment procedure, we obtained an estimation of the
test subject's empirical $Z$-maps by calculating the arithmetic mean of the
respective paradigm's projected $Z$-maps.



\subsection{Reliability of the empirical $Z$-maps}
%
We calculated Cronbach's $\alpha$ as a measure of reliability and the amount of
measurement error \citep{cronbach1951coefficient, cortina1993coefficient}
present in the empirical $Z$-maps of each paradigm and subject.
%
Cronbach's $\alpha$ expresses the expected correlation between the currently
used empirical $Z$-maps and an additional set of empirical $Z$-maps calculated
based on data of a hypothetical independent dataset collected from the same
paradigm and subjects \citep{jiahui2020predicting, jiahui2022cross}.
%
These expected correlations, represented by Cronbach's $\alpha$, were calculated
based on the first-level \ac{glm} $Z$-maps (four in case of the visual
localizer; eight in case of the naturalistic stimuli) that were averaged in the
second-level \ac{glm} analyses of \citet{sengupta2016extension} and
\citet{haeusler2022processing} respectively.
%
Cronbach's $\alpha$ of the empirical (i.e. second-level) $Z$-maps for each
subject and paradigm can be seen in Fig.~\ref{fig:cronbachs}, descriptive
statistics across subjects for each paradigm can be seen in
Table~\ref{fig:cronbachs}.

% The Cronbach's $\alpha$ values for each paradigm are as follows:
% For the visual localizer, the mean is 0.899990 with a standard deviation of
% 0.087051. The minimum value is 0.658523, the 25th percentile is 0.906643, the
% median is 0.934019, the 75th percentile is 0.947906, and the maximum is
% 0.963065.

% For the movie paradigm, the mean is 0.611332 with a standard deviation of
% 0.137878. The minimum value is 0.284440, the 25th percentile is 0.555529, the
% median is 0.627240, the 75th percentile is 0.676353, and the maximum is
% 0.800254.
% movie's outlier: sub-06 (0.28); but when movie PPA is predicted he / she is
% not the outlier

% For the audio-description paradigm, the mean is 0.476194 with a
% standard deviation of 0.358019. The minimum value is -0.526626, the 25th
% percentile is 0.428975, the median is 0.627799, the 75th percentile is
% 0.679987, and the maximum is 0.823584.
% audio-description's outlier: sub-05 (-0.5!), sub-02 (0.0), sub-20 (0.27);
% when audio PPA is predicted, these three subjects are the outliers


\begin{figure*}[tbp] \centering
    \includegraphics[width=\linewidth]{figures/plot_cronbachs.pdf}
    \caption{\textbf{Cronbach's $\alpha$ of the empirical $Z$-maps for each
    paradigm and subject.}
    %
    Cronbach's $\alpha$ was calculated based on the $Z$-maps yielded by the
    first-level \ac{glm} analyses of the visual localizer
    \citep[four runs;][]{sengupta2016extension} and naturalistic stimuli
    \citep[eight runs;][]{haeusler2022processing}.
    %
    The second-level \ac{glm} analyses across runs yielded the empirical
    $Z$-maps that were estimated in the present study.
    }
\label{fig:cronbachs}
\end{figure*}


\begin{table*}[btp]
\centering
    \caption{
    %
    \textbf{Descriptive statistics of Cronbach's $\alpha$ across subjects.}
    %
    Cronbach's $\alpha$ of the empirical $Z$-maps that are the result of the
    second-level \ac{glm} analyses performed in
    \citet{sengupta2016extension} and \citet{haeusler2022processing}. Values of
    Cronbach's $\alpha$ were calculated based on the first-level $Z$-maps (four
    in case of the visual localizer; eight in case of the naturalistic stimuli)}
\label{tab:cronbachs}
\begin{tabular}{llll}
    \toprule
    \textbf{statistic} & \textbf{localizer} & \textbf{movie} & \textbf{audio-description} \\
    \midrule
    mean & 0.90 & 0.61 & 0.48 \tabularnewline
    std & 0.09 & 0.14 & 0.36 \tabularnewline
    min & 0.66 & 0.28 & -0.53 \tabularnewline
    25\% & 0.91 & 0.56 & 0.43 \tabularnewline
    50\% & 0.93 & 0.63 & 0.63 \tabularnewline
    75\% & 0.95 & 0.68 & 0.68 \tabularnewline
    max & 0.96 & 0.80 & 0.82 \tabularnewline
    \bottomrule
\end{tabular}
     % \caption*{The legend text goes here.}
\end{table*}



\section{Results}


\begin{comment}

``Because the localizer task comprises several scanning runs, we calculated the
reliability of the localizer across runs with Cronbach's $\alpha$ to provide an
estimate of the noise ceiling for these correlations'' \citep{jiahui2022cross}.

\end{comment}


The unthresholded $Z$-maps in their respective subject's voxel space can be
accessed at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}.
\todo{empty}
%
In order to assess the performance of the alignment procedures, we calculated
the Pearson correlation coefficients between each individual's empirical
$Z$-maps obtained from the previous analyses \citep{haeusler2022processing,
sengupta2016extension} and their respective predicted $Z$-maps (s.
Fig.~\ref{fig:stripplot}).
%
In general, the mean Pearson correlation coefficients vary depending on paradigm
being estimated (i.e. $Z$-maps of the visual localizer, movie, or
audio-description), as well as the alignment procedure (anatomical vs.
functional alignment).
%
In the case of functional alignment, the quantity of the paradigm's data used as
a predictor and to align a test subject to the \ac{cfs} also affects the
correlation coefficients.
%
However, the functional alignment procedure consistently shows an increasing
estimation performance as more data of a predictor is used to align the test
subjects.
%
In order to investigate potential differences between some conditions, we
conducted 15 pairwise comparisons using Fisher z-transformed correlation values.
%
These comparisons were not pre-planned, but rather were selected later as
examples for further exploration.
%
We used a Bonferroni correction for multiple comparisons to adjust the alpha
level to an $\alpha$ of $0.05 / 15 = 0.00\overline{3}$.


\begin{figure*}[tbp] \centering
    \includegraphics[width=\linewidth]{figures/plot_corr-emp-vs-estimation.pdf}
    \caption{
    %
    \textbf{Correlations between empirical and predicted
    \textit{\textbf{Z}}-maps for each subject and paradigm.}
    %
    Functional alignment was performed based on an increasing amount of
    functional data used to align a test subject to the common functional space
    (CFS):
    each of the four runs of the visual localizer paradigm lasted
    $\approx$\unit[5]{min} (\ac{tr}=\unit[2]{s});
    each of the eight runs of the naturalistic stimuli lasted
    $\approx$\unit[15]{min} (\ac{tr}=\unit[2]{s}).
    %
    Solid horizontal lines:
    %
    median of Cronbach's $\alpha$ across subjects the for empirical $Z$-maps of
    the respectively estimated paradigm (cf. Fig.~\ref{fig:cronbachs}).
    %
    Dotted horizontal lines:
    %
    mean of Cronbach's $\alpha$ across subjects for the empirical $Z$-maps of
    the respectively estimated paradigm (cf. Fig.~\ref{fig:cronbachs}).
    %
    Grey dots:
    %
    correlations between empirical $Z$-maps and an estimation using anatomical
    alignment.
    %
    Green dots:
    %
    correlations between empirical $Z$-map and an estimation using functional
    alignment based on transformations calculated from one up to four
    runs of the visual localizer.
    %
    Red dots:
    %
    correlations between empirical $Z$-map and an estimation using functional
    alignment based on transformations calculated from one up to eight runs
    of the movie.
    %
    Blue dots:
    %
    correlations between empirical $Z$-map and an estimation using functional
    alignment based on transformations calculated from one up to eight runs
    of the audio-description.
    }
\label{fig:stripplot}
\end{figure*}


% localizer: localizer data
When estimating the $Z$-maps of the visual localizer, the mean correlation
between empirical $Z$-maps and $Z$-maps predicted using $\approx$\unit[15]{min}
of the visual localizer (within-paradigm prediction) was significantly higher
than the mean correlation between empirical $Z$-maps and $Z$-maps predicted via
an anatomical alignment ($t(14)={13.99}$, $p<.0001$).
% localizer: movie data
Similarly, the mean correlation between empirical $Z$-maps and $Z$-maps
predicted using $\approx$\unit[15]{min} of the movie (cross-paradigm prediction)
was significantly higher than the correlations between empirical $Z$-maps and
$Z$-maps predicted via anatomical alignment ($t(14)=6.35$, $p<.0001$).
%
A comparison between the prediction based on $\approx$\unit[15]{min} of the
movie and $\approx$\unit[15]{min} of localizer data revealed a significantly
lower performance of functional alignment using the movie ($t(14)=-11.64$,
$p<.0001$).
%
The prediction performance based on $\approx$\unit[30]{min} of the movie was
significantly higher than the prediction performance based on
$\approx$\unit[15]{min} of the movie ($t(14)=5.49$, $p=.0001$), with no
significant difference between $\approx$\unit[45]{min} and
$\approx$\unit[30]{min} of the movie ($t(14)=0.13$, $p=.8990$).
% localizer: audio data
Visual inspection indicated that the prediction performance based on
$\approx$\unit[15]{min} of the audio-description was lower than the prediction
performance based on anatomical alignment, functional alignment using
$\approx$\unit[15]{min} of the localizer, or $\approx$\unit[15]{min} of the
movie.
%
However, the prediction performance of the audio-description increased
monotonically the more data were used to align the test subjects.
%
A $t$-test comparing the prediction performance based on
$\approx$\unit[120]{min} of the audio-description to anatomical alignment
yielded no significant difference ($t(14)=-1.17$, $p=.2640$).

%
When estimating the $Z$-maps of the movie, the mean correlation between
empirical $Z$-maps and predicted $Z$-maps using $\approx$\unit[15]{min} of the
movie (within-paradigm prediction) were significantly higher than the
correlations between empirical $Z$-maps and predicted $Z$-maps via an anatomical
alignment ($t(14)= 5.78$, $p<.0001$).
%
Comparing the within-paradigm prediction using $\approx$\unit[15]{min} of the
movie to the cross-paradigm prediction using $\approx$\unit[15]{min} of the
localizer revealed a higher prediction performance when using the movie data for
alignment ($t(14)=5.53$, $p<.0001$).
%
There was no significant difference between the prediction using
$\approx$\unit[15]{min} of the localizer and the prediction via anatomical
alignment ($t(14)=1.15$, $p=.2726$).
%
The prediction performance based on $\approx$\unit[30]{min} of movie data was
significantly higher than the prediction performance based on
$\approx$\unit[15]{min} of the movie ($t(14)= 3.75$, $p=.0024$), whereas there
was no significant difference between the performance based on
$\approx$\unit[45]{min} and $\approx$\unit[30]{min} of the movie ($t(14)=2.58$,
$p=.0230$).
%
As is evident by visual inspection, the prediction performance based on
$\approx$\unit[15]{min} of the audio-description was lower than a prediction
based on anatomical alignment, functional alignment using
$\approx$\unit[15]{min} of the localizer, or functional alignment using
$\approx$\unit[15]{min} of the movie.
%
Here again, the prediction performance of the audio-description monotonically
increased the more data were used to align the test subjects.
%
A $t$-test comparing the prediction performance based on
$\approx$\unit[120]{min} of the audio-description to a prediction via anatomical
alignment yielded no significant difference ($t(14)=-2.40$, $p=.0318$).

%
When estimating the $Z$-maps of the audio-description, the mean correlation
between empirical $Z$-maps and predicted $Z$-maps via $\approx$\unit[15]{min} of
the audio-description (within-paradigm prediction) was not significantly
different to the mean correlation between empirical $Z$-maps and predicted
$Z$-maps via anatomical alignment ($t(14)=-1.82$, $p=.0925$).
%
Comparing the prediction based on $\approx$\unit[120]{min} of the
audio-description to the anatomical alignment procedure yielded a significantly
higher performance of the estimation via functional alignment ($t(14)=6.56$,
$p<=.0001$).
%
Comparing the prediction based on functional alignment via
$\approx$\unit[120]{min} of the movie to the anatomical alignment procedure
yielded no significant difference ($t(14)=-0.76$, $p=.4625$).



\section{Discussion}

\todo[inline]{introduction of discussion is long and method-heavy but, imo, it
makes sense to repeat stuff 'cause the stuff done is pretty extensive}

%
Block-design functional localizer paradigms are traditionally used to identify
functional areas in individual participants.
%
However, these paradigms are typically limited to mapping a single domain of
brain functions.
%
Additionally, the diagnostic quality of functional localizers heavily relies on
the participants' comprehension of task instructions and their compliance.
%
In our study, we focused on \ac{ppa} as an example of a higher-visual area.
%
We aimed to estimate the results of $t$-contrasts (i.e. the empirical $Z$-maps),
created in previous studies \citep{sengupta2016extension,
haeusler2022processing} in order to localize the \ac{ppa}, in an individual by
leveraging data collected from a reference group.
%
To address the challenge of functional-anatomical variability across individuals
and, we employed a functional alignment approach based on the \acf{srm}
\citep{chen2015reduced}.
%
Following an exhaustive leave-one-subject-out cross-validation, we computed a
multi-paradigm \acf{cfs} based on the training subjects' concatenated response
time series from a visual localizer, a movie, and the movie's audio-description.
%
Each test subject's response time series from one of the paradigms were
separately used to functionally aligning the test subject with the corresponding
paradigm's \acp{tr} within the \ac{cfs}.
%
Finally, we projected the empirical $Z$-maps of the reference group through the
\ac{cfs} into the test subject's brain anatomy to generate the predicted
$Z$-maps for each test subject.
%
Considering the challenges of acquiring functional data from a two-hour paradigm
to functionally an individual with a \ac{cfs}, we also explored the relationship
between the amount of data used for alignment and the subsequent estimation
performance.
%
As a baseline comparison, we employed an anatomical alignment approach, where
the training subjects' $Z$-maps were projected via nonlinear, volume-based
transformation through the MNI space into the test subject's anatomical voxel
space.
%
Our findings demonstrate that an auditory narrative can be utilized to estimate
the results of a visual localizer, although it requires a longer functional
scanning session.
%
Furthermore, we observed that employing $\approx$\unit[15]{min} of movie data
sampled at \unit{0.5}{Hz} for volume-based functional alignment leads to more
accurate estimations of the visual localizer results compared to a non-linear
volume-based anatomical alignment approach.
%
By leveraging data from a reference group, our procedure opens up the
possibility of estimating results from many localizer paradigms using a
naturalistic stimulus of similar duration of one traditional localizer paradigm.





\subsection{Estimating the results of the visual localizer}

We estimated the results of the visual localizer, which is the established
method for identifying the \ac{ppa}.
%
Our findings indicate that $\approx$\unit[15]{min} of localizer data or
$\approx$\unit[15]{min} movie data used for functional alignment with a \ac{cfs}
are sufficient to estimate the results of the visual localizer with higher
fidelity compared to an estimation procedure based on anatomical alignment.
%
When comparing the within-paradigm prediction based on the localizer data to the
cross-paradigm prediction based on the movie data, the within-paradigm
prediction showed a superior estimation performance.
%
These results are in line with \citep{haxby2011common}, who created two
\acp{cfs}:
%
one \ac{cfs} based on a controlled paradigm that employed still images of
stimulus categories and another \ac{cfs} based on a movie.
%
They found that a cross-subject classification of the controlled paradigm's
categories via the \ac{cfs} and transformations calculated from the same
paradigm's data outperformed the classification based on the movie data.
%
\citet{haxby2011common} interpreted these results gained from two paradigms
providing roughly the same amount of \acp{tr} to calculate the \ac{cfs} and
align a test subject as the controlled paradigm sampling the investigated brain
states more extensively than the movie.
%
In our study, the prediction performance based on movie data increases
significantly when $\approx$\unit[30]{min} of the movie are used for alignment
with the \ac{cfs}.
%
However, comparing $\approx$\unit[30]{min} to $\approx$\unit[45]{min} for
aligning a test subject did not yield a significant difference.
%
Overall, the results suggest that the estimation based on functional alignment
using movie data approaches a performance limit, with diminishing benefits of
longer scanning time (cf. Fig.~\ref{fig:stripplot}).

%
We also explored a cross-paradigm prediction of the visual localizer's results
using time series from the audio-description that lacks a visual stimulation.
%
The prediction based on the audio-description showed the lowest performance
among all alignment procedures, including anatomical alignment.
%
However, as more data were used, the prediction performance based on the
audio-description improved while narrowing the gap to the prediction based on
the same amount of movie data.
%
When comparing the prediction based on $\approx$\unit[120]{min} of the
audio-description to the prediction based on anatomical alignment, we found no
statistical difference.
%
These findings suggest that an auditory paradigm could potentially substitute a
visual paradigm for functional alignment and a subsequent estimation of a visual
paradigm's results.
%
However, achieving a comparable estimation performance requires a lengthy
functional scanning session.
%
Further investigations might explore alternative auditory paradigms that more
extensively sample the targeted responses to achieve higher estimation
performances.


\subsection{Estimating the results of the movie \& audio-description}
%
We also estimated the results of \citet{haeusler2022processing}, where we
created $t$-contrast based on modeled hemodynamic activity during two
naturalistic stimuli to localize the \ac{ppa} under more ecologically
conditions.
%
Once again, the within-paradigm predictions achieved higher correlations between
empirical and predicted $Z$-maps compared to the cross-paradigm predictions.
%
Similar to the estimation of the visual localizer results, the estimation of the
movie results based on the audio-description showed the lowest performance among
all alignment procedures, including anatomical alignment.
%
However, as more data from the audio-description were incorporated for
alignment, the prediction performance improved.
%
Moreover, similar to the estimation of the visual localizer, the gap between the
estimation performance using the audio-description and the same amount of movie
data narrowed.
%
Notably, the within-paradigm prediction based on $\approx$\unit[15]{min} of the
audio-description did not outperform the prediction via the anatomical
alignment.
%
Given the exploratory nature of this study, identifying the exact reasons for
the relatively poor performance of the audio-description is challenging.
%
However, three possible interpretations seem plausible.
%
\todo[inline]{just some wild speculations:}

\textbf{First}, it is worth considering that the audio-description revealed
shared responses of lower peak amplitudes compared to the movie and visual
localizer despite normalizing all time series to a mean of zero and a standard
deviation of one separately before concatenating the time series and calculating
the shared responses.
%
The audio-description may have a lower signal-to-noise ratio\todo{is this the
case???}, or evoke responses that are less congruent across participants
resulting in shared responses of lower peak amplitude and therefore lower
estimation performance.
%
Further investigations are needed to better understand the specific factors
contributing to the observed lower amplitudes and their impact on the prediction
performance.

\todo[inline]{if that makes sense at all (higher amplitude is only obvious to
see in shared response 1), add plot of TRs of top 3 shared responses in the
\ac{cfs} to method section}

%
\textbf{A second} possible explanation, which is not mutually exclusive with the
previous explanation, could be that the auditory paradigm does not sample the
targeted neural responses as comprehensively due to a sparser event structure.
%
This limitation might have become particularly pronounced when estimating the
results of visual paradigms.
%
\textbf{A third} explanation, also not mutually exclusive, could be that the
neural responses in the \ac{ppa} to an auditory paradigm differ from responses
to a visual paradigm, and therefore contributing to the lower estimation
performance.
%
If this explanation holds true, it could support the findings of
\citet{haeusler2022processing}, where we observed hemodynamic activity during
auditory stimulation that is restricted to the anterior part of the \ac{ppa} as
identified with the visual localizer.
%
Nevertheless, further studies using controlled paradigms are needed to
investigate responses of the \ac{ppa} to auditory spatial information, as
confounding variables such as a participant's alertness and attentional focus
are difficult to experimentally and statistically control during naturalistic
auditory stimulation.

%
Calculating Cronbach's $\alpha$ of the audio-description's $Z$-maps revealed
mediocre to poor reliability in three participants.
%
The three participants with poor Cronbach's $\alpha$ also exhibited the lowest
correlations between the audio-description's predicted and empirical $Z$-maps.
%
Results suggest that these participants are not outliers showing a consistently
deviant response from the norm but rather exhibited more variable responses to
auditory spatial information during the two-hour stimulation.
%
When estimating the $Z$-maps of the visual paradigms based on the
audio-description, we did not identify the same participants as outliers,
suggesting that an auditory narrative sufficiently samples responses that can be
used to align a test subject to a \ac{cfs} and acquire transformations that
enable the estimation of a visual paradigm's results.
%
These findings are in line the work of \citet{haxby2011common}, who demonstrated
that the general validity of their functional alignment procedure ``based on the
responses to the movie is not dependent on responses to stimuli that are in both
the movie and the category perception experiments but, rather, appears to rest
on stimulus properties that are more abstract and of more general utility''
\citep[][p. 409]{haxby2011common}.
%
In light of our current findings, we hypothesize that a stimulus used for
functional alignment does not necessarily need to sample hemodynamic responses
identical to those evoked by the target paradigm, however with the disadvantage
of a longer scanning time.
%
Therefore, it would be interesting to explore whether functional alignment can
be employed to estimate $Z$-maps from paradigms designed to elicit brain
processes that are typically not sampled during passive movie watching, such as
planning or decision-making.

%
It is challenging to untangle the individual contributions of the exclusively
auditory stimulus, the multi-paradigm \ac{cfs}, the transformation matrices of
test subjects, and the mean reliability of the audio-description's $Z$-maps on
the estimation performance.
%
However, despite the presence of outliers, the within-paradigm prediction
performs well for the majority of participants when a sufficient amount of data
is available for alignment.
%
This indicates that outliers do not have a significant impact on the prediction
performance.
%
Moreover, the observation that the average correlation between empirical and
predicted $Z$-maps exceeds mean Cronbach's $\alpha$ as more data from the
audio-description are used can be interpreted as the \ac{srm} denoising a test
subject's data \citep[cf.][]{chen2015reduced}.
%
Therefore, it would be interesting to estimate the results of a controlled
auditory experiment, such as a speech localizer, that yields more reliable
results in all participants.
%
To test the hypothesis that the \ac{srm} filters out noise, one could calculate
more reliable second-level $Z$-maps from actual time-series, and time series
with artificially added noise at different intensities.
%
If the \ac{srm} filters out noise, the prediction based on an alignment using a
test subject's time series with added noise should resemble the actual $Z$-maps
more closely than the $Z$-maps calculated from noisy data.

\todo[inline]{does that makes sense to you???}

%
Lastly, it is important to acknowledge that our analysis was restricted to
voxels located in the union of voxels that exhibited significantly increased
activity in at least one participant of the visual localizer's $Z$-maps.
%
However, the findings of \citet{haeusler2022processing} suggest that hemodynamic
activity during auditory semantic stimulation is confined to the anterior part
of the \ac{ppa}.
%
Consequently, the \ac{roi} introduced a bias against the reliability of the
$Z$-maps derived from the audio-description.
%
This is because the \ac{roi} includes voxels that did not show significantly
increased activity in response to auditory spatial information in at least one
participant, but rather exhibit random variation (or potentially significantly
decreased activation, which would not affect the reliability measure).


\subsection{Summary, shortcomings \& future studies}

%
In summary, our study revealed that using $\approx$\unit[15]{min} of the
audio-description for aligning a test subject with the \ac{cfs} resulted in
relatively low performance in cross- and within-paradigm prediction.
%
Nevertheless, results suggest that it is in principle feasible to substitute a
visual paradigm with an auditory paradigm.
%
An interesting avenue for further investigation would be to estimate the
topography of language-related areas using a narrative for functional alignment
and compare the results with an estimation based on a movie.
%
Furthermore, our findings indicate that $\approx$\unit[15]{min} of movie data
used for functional alignment are sufficient to estimate the results of the
visual localizer with higher fidelity than an estimation based on an anatomical
alignment.
%
The prediction performance based on the movie significantly improves when
$\approx$\unit[30]{min} of the movie are used.
%
However, we found no significant difference in performance between aligning a
test subject using $\approx$\unit[45]{min} compared to $\approx$\unit[30]{min}.
%
Overall, our results suggest that the functional alignment based on movie data
is approaching a performance limit, and longer scanning sessions than
\unit[30]{min} during audio-visual stimulation may not yield substantial
benefits.

% Results of \citet{frost2012measuring}: ``We localized 13 widely studied
% functional areas and found a large variability in the degree to which
% functional areas respect macro-anatomical boundaries across the cortex. The
% percent gain in overlap [after surface-based alignment] differed greatly
% across the different functional regions throughout the cortex. There is a
% strong structural-functional correspondence in some areas whilst in others the
% spatial location of the functional area is not tightly bound to anatomical
% landmarks and varies greatly across subjects within a cortical area. Language
% areas were found to vary greatly across subjects whilst a high degree of
% overlap was observed in sensory and motor areas. PPA gained 17.6\% in the left
% and 27.7\% in the right hemisphere. The FFA gained 44.1\% in the left and 12\%
% in the right hemisphere. The FFA did not exhibit the same strong
% structural-functional correspondence and saw more modest increases in overlap
% after macro-anatomical alignment. FFA varies in its location along the length
% of the fusiform gyrus even though the gyri themselves are well aligned across
% subjects. LOC gained 62.7\% in the left and 38.4\% in the right hemisphere''
% \citep{frost2012measuring}.

% studyforrest dataset: other t-contrasts of localizer: The visual localizer of
% the studyforrest dataset also contains contrast that aimed to localize the
% \ac{ffa} and \ac{ofa} that are associated with face perception
% \citep{kanwisher1997ffa, pitcher2011occipitalfacearea}, the \ac{eba} that is
% associated with the perception of human bodies \citep{downing2001bodyarea},
% and the \ac{loc} that is associated with the perception of (small) objects
% (like tools or toys) \citep{malach1995loc}, % The subject-specific \acp{roi}
% masks for these areas that where created by \citep{sengupta2016extension} and
% our analysis pipeline we used in \citet{haeusler2022processing} provide an
% opportunity for future studies (e.g., a master's thesis or part of a PhD
% project) to explore the hemodynamic responses that correlate with auditory
% information related to faces, body parts, or small objects.

%
When estimating the results of the visual localizer, the cross-paradigm
prediction based on the movie showed lower performance than the
within-prediction based on the localizer.
%
However, movies offer a more complex stimulation compared to controlled
paradigms.
%
While we focused on the \ac{ppa} as a higher-visual area, naturalistic stimuli
have been successfully employed to investigate various domains of brain
function, including vision, audition, language, emotions, or social cognition
\citep[s.][for a review]{jaaskelainen2021movies}.
%
Given the ability of naturalistic stimuli to elicit responses across diverse
domains, they potentially provide transformations that better generalize across
a wider range of paradigms compared to transformations obtained from dedicated
experiments such as a visual localizer.
%
Previous studies that used an anatomical alignment procedure to estimate the
most probable location of functional areas have ``found a large variability in
the degree to which functional areas respect macro-anatomical boundaries''
\citep[][p. 1369]{frost2012measuring}.
%
For instance, retinotopically defined regions of the early visual cortex exhibit
low interindividual variability \citep{rosenke2021probabilistic}, while the
spatial location of other functional areas, such as language areas, varies
greatly across individuals \citep{frost2012measuring}.
%
Even within the domain of category-selective areas, interindividual variability
varies across functional areas, with scene-selective regions showing larger
variability in spatial topography compared to face-selective regions
\citep{zhen2015quantifying, zhen2017quantifying, frost2012measuring}.
%
Future studies should explore the performance of functional alignment in other
domains than higher-visual perception.
%
In particular, we showed that an auditory paradigm can be used to estimate the
results of a visual paradigm.
%
Therefore, it would be interesting to explore whether functional alignment can
be employed to estimate $Z$-maps from paradigms that were designed to elicit
brain processes that are not sampled during passive movie watching such as
planning or decision-making.

%
Lastly, we emphasize that our study is exploratory, based on a sample of 14
participants, and therefore provides preliminary results that warrant further
in-depth investigations into aspects that are beyond of scope here.
%
For example, we restricted our analysis to voxels in a \ac{roi} with an average
of $\approx$\unit[1600]{voxel} per participant.
%
The use of searchlight functional alignment
\citep[e.g.,][]{zhang2016searchlight, guntupalli2016model} could cover the
entire cortex but restricts the simultaneously aligned voxels to those within
the searchlight's sphere.
%
Future studies are needed to develop functional alignment algorithms that can
align voxels across larger distances.
%
Such algorithms would be especially beneficial for functional areas that show
large interindividual variability in anatomical location, such as language
areas, or for cases of atypical topography, e.g., resulting from cortical
reorganization after brain injuries.

\todo[inline]{imo, connectivity functional alignment (again) fucks up that
argument}

% \citep{bazeille2021empirical}: Searchlight functional alignment
% \citep{zhang2016searchlight, guntupalli2016model} learns local transformations
% and aggregates them into a single large-scale alignment. The searchlight
% scheme [Kriegeskorte, 2006, Information-based...], popular in brain imaging
% \citep{guntupalli2018computational, guntupalli2016model} has been used as a
% way to divide the cortex into small overlapping spheres of a field radius.
% This method allows researchers to remain agnostic as to the location of
% functional or anatomical boundaries, such as those suggested by parcellation-
% based approaches. A local transform can then be learned in each sphere and %
% the full alignment is obtained by aggregating (e.g. summing as in %
% \citep{guntupalli2016model} or averaging) across overlapping transforms.
% The aggregated transformation produced is no longer guaranteed to bear the
% type of regularity (e.g orthogonality, isometry, or diffeomorphicity enforced
% during the local neighborhood fit). In the case of searchlight Procrustes,
% we selected searchlight parameters to match those used in Guntupalli et al.
% (2016): each searchlight had 5 voxel radius, with a 3 voxel distance between
% searchlight centers'' \citep{bazeille2021empirical}.

% examples of probabilistic atlasses: \citet{rosenke2021probabilistic}:
% Cortical atlases have been developed, which allow localization of visual areas
% ``in new subjects by leveraging ROI data from an independent set of typical
% participants: Frost and Goebel 2012; ventral temporal cortex (VTC) category
% selectivity: Julian et al. 2012, Zhen et al. 2017, Weiner et al. 2018;
% visual field maps: Benson et al. 2012, Benson and Winawer 2018; Wang et al.
% 2015''.

% A shared calibration scan across datasets could be used to transfer data
% between datasets, a procedure that is easier to accomplish than shared
% subjects across datasets \citep[cf.][an extension of the \ac{srm} for
% shared subjects across datasets]{zhang2018transfer}}

%
Our results suggest that $\approx$\unit[15]{min} \ac{fmri} scanning during a
naturalistic stimulus that captures a wide range of brain states could provide
sufficient data for a functional \textit{calibration scan} lasting
\unit[15]{min} to \unit[30]{min}.
%
This calibration scan could be employed to align a new subject to a \ac{cfs}
derived from extensive scans of a reference group.
%
The reference group's scans would encompass data collected from both
naturalistic paradigms and controlled paradigms.
%
The controlled paradigms would include functional localizers specifically
designed to reliably map perceptual or cognitive processes.
%
Compared to a diagnostic run based on a controlled paradigm, a naturalistic
stimulus would offer the additional benefits of higher engagement and better
compliance \citep{vanderwal2015inscapes, eickhoff2020towards}, especially in
children or patients.
%
Once a new subject is aligned with the \ac{cfs}, functional data collected from
the reference group could be mapped through the \ac{cfs} into the new subject's
voxel space.
%
This enables the estimation of patterns in the new subject that are common in
the normative reference group when obtaining additional functional scans is not
feasible due to scanner availability, time constraints, financial limitations,
or compliance issues.
%
Furthermore, this approach allows for quantifying the similarity (or
dissimilarity) between a new subject's actual pattern and the common pattern
estimated from the normative reference.
%
For instance, \citet{yates2021emergence} calculated a \ac{srm} to investigate
the presence and localization of adult brain functions in children.
%
The authors mapped hemodynamic responses of adults watching a movie through a
\ac{cfs} into the anatomical brain space of children that watched the same
movie.
%
They found reliable correlations between the predicted and actual \ac{fmri}
activity of children, and the strength of these correlations in the precuneus,
inferior frontal gyrus, and lateral occipital cortex could predict the
children's age.



\section{Conclusion}

\todo[inline]{just a template}

\todo[inline]{revise with fix, printed and marked discussion}

\footnotesize

%
In conclusion, our study demonstrates the potential of using naturalistic
stimuli for functional alignment to estimate brain activity patterns
in individuals based on a normative reference group.
%
This study explores the feasibility of using naturalistic stimuli and functional
alignment techniques to estimate brain activity patterns based on a normative
reference group.
%
Results indicate that approximately 15 minutes of functional scanning using an
engaging naturalistic stimulus can generate sufficient data for alignment to a
common functional space and estimate brain patterns with higher fidelity than an
procedure based on anatomical alignment.
%
We found that approximately 15 minutes of data from an engaging
naturalistic stimulus can provide sufficient data for a functional calibration
scan, allowing alignment with a \ac{cfs} derived from a reference group's scans.
%
Specifically, our findings suggest that an auditory paradigm can be used to
estimate the results of a visual paradigm, raising the possibility of employing
functional alignment for estimating brain processes not directly sampled during
passive movie watching, such as planning or decision-making.
%
By mapping functional data from the reference group through the CFS into a new
subject's voxel space, we can estimate typical patterns and quantify the
similarity or difference between a subject's actual brain activity and patterns
derived from the reference group.
%
By mapping functional data through the common space, typical patterns can be
estimated and similarities to a reference group quantified.
%
Our findings suggest that this approach holds promise for applications where
additional functional scans are not feasible due to various constraints.
%
This approach offers several advantages over traditional controlled paradigms,
including higher engagement and better compliance, particularly in children or
patients.
%
Naturalistic stimuli provide transformations that potentially generalize better
across a wider range of paradigms compared to dedicated experiments using
controlled paradigms.
%
While our study focused on higher-visual perception as an example, future
research should explore functional alignment performance in other domains and
brain functions.
%
It is essential to note that our study is exploratory and based on a sample size
of 14 subjects, providing preliminary results that warrant further
investigation.
%
This broader content coverage is particularly valuable for investigating higher
cognitive processes beyond basic perception.
%
Alternative approaches that enhance alignment across larger distances should be
explored.

%
By leveraging the data from this reference group, it becomes possible to
identify and characterize the functional area of interest without the need for
extensive scanning sessions for each individual.
%
This opens up the possibility of estimating results from many localizer
paradigms using a naturalistic stimulus of similar duration of one traditional
localizer paradigm.
%
This method offers a more efficient and cost-effective solution while still
providing valuable insights into individual brain functional anatomy.

\normalsize



\section*{Data Availability}

\todo[inline]{GIN link to an empty directory}

% \href{https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-analysis}}

% new; PPA analysis
All fMRI data and results are available as Datalad \citep{halchenko2021datalad}
datasets, published to or linked from the \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).
% original
Raw data of the audio-description, movie and visual localizer were originally
published on the \emph{OpenfMRI} portal
\citep[\url{https://legacy.openfmri.org/dataset/ds000113},][]
{Hanke2014ds000113}, \space
\citep[\url{https://legacy.openfmri.org/dataset/ds000113d},][]
{hanke2016ds000113d}.
% visual localizer
Results from the localization of higher visual areas are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-visualrois}{\url{github.com/psychoinformatics-de/studyforrest-data-visualrois}}).
% raw data
The realigned participant-specific time series that were used in the current
analyses were derived from the raw data releases and are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned}{\url{github.com/psychoinformatics-de/studyforrest-data-aligned}}).
% OpenNeuro
The same data are available in a modified and merged form on OpenNeuro at
\url{https://openneuro.org/datasets/ds000113}.
% NeuroVault for z-maps of SRM
Unthresholded $Z$-maps of all contrasts can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}.


\section*{Code Availability}

Scripts to generate the results as Datalad \citep{halchenko2021datalad} datasets
are available in a \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).


\begin{comment}


\pagebreak

\section{Backup of texts}

\subsection{Localizer generalizes amazingly well}

\todo[inline]{in general, I think this does not need to be discussed; still,
here are some ideas}

\todo[inline]{Haxby claims: dedicated paradigms do not generalize well}

\todo[inline]{But their BSC of movie segments based on localizer \ac{cfs} is
"more difficult" than our case}

\todo[inline]{Raider movie: 1,700 TRs}

\todo[inline]{Haxby's localizer: 1,536 TRs; 1.7 TRs per pic (896 pics shown)}

\todo[inline]{Haxby's animals: 1,640 TRs; 1.4 TRs per pic (1140 pics shown)}

\todo[inline]{our localizer: 624 TRs; 1.6 TRs per pic (384 pics shown)}

\todo[inline]{is our multi-paradigm model so good? or is it the matrices? or is
our test (estimating $Z$-maps instead of BSC of movie segments) easier?}

\paragraph{Haxby templates}

%
``Our previous results showed that the sampling of response vectors from
simpler, controlled experiments is impoverished and produces a model
representational space that does not generalize well to new stimuli in other
experiments \citep{haxby2020hyperalignment}'' \citep{guntupalli2016model}.

%
``We base the derivation of the transformation matrices and the common space on
responses to the movie---a complex, naturalistic, dynamic stimulus.
%
Although the algorithm also can be applied to fMRI data from more controlled
experiments, we found that a common model based on such data has greatly
diminished general validity \citep{haxby2011common}, presumably because,
relative to a rich and dynamic naturalistic stimulus, such experiments sample an
impoverished range of brain states \citep{guntupalli2016model}''.

%
``Common models based on responses to smaller, more controlled stimulus
sets---still images of a limited number of categories---were valid only for
restricted stimulus domains, indicating that these models captured only a
subspace of the substantially larger representational space in VT cortex''
\citep{haxby2011common}.

``We asked whether hyperalignment based on these simpler stimulus sets was
sufficient to derive a common space with general validity across a wider array
of complex stimuli.
%
We applied the hyperalignment parameters derived from the face and object data
to the movie data in the Princeton subjects and the hyperalignment parameters
derived from the animal species data to the movie data in the Dartmouth
subjects.
%
BSC of 18s movie time segments after hyperalignment based on category perception
experiment data was markedly worse than BSC after hyperalignment based on movie
data (17.6\% ± 1.3\% versus 65.8\% ± 2.7\% for Princeton subjects; 28.3\% ±
2.8\% versus 74.9\% ± 4.1\% for Dartmouth subjects; p < 0.001 in both cases;
Figure 4).
%
Thus, hyperalignment of data using a set of stimuli that is less diverse than
the movie is effective, but the resultant common space has validity that is
limited to a small subspace of the representational space in VT cortex''
\citep{haxby2011common}.

%
``We also tested whether the general validity of the model space reflects
responses to stimuli that are in both the movie and the category perception
experiments or reflects stimulus properties that are not specific to these
stimuli.
%
We recomputed the common model after removing all movie time points in which a
monkey, a dog, an insect, or a bird appeared. We also removed time points for
the 30 s that followed such episodes to factor out effects of delayed
hemodynamic responses.
%
BSC of the face and object and animal species categories, including distinctions
among monkeys, dogs, insects, and birds, was not affected by removing these time
points from the movie data [65.0\% ± 1.9\% versus 64.8\% ± 2.3\% for faces and
objects; 67.1\% ± 3.0\% versus 67.6\% ± 3.1\% for animal species; Figure S4B].
%
This result suggests that the movie-based hyperalignment parameters that afford
generalization to these stimuli are not stimulus specific but, rather, reflect
stimulus properties that are more abstract and of more general utility for
object representations'' \citep{haxby2011common}.




\subsection{Other functional alignment algorithms}

\subsubsection{Volume- vs surface-based}

We compare volume-based anatomical alignment to volume-based functional
alignment.
%
Opportunity costs: $Z$-maps of localizer were calculated in voxel space; we are
nearer on the raw data (less error accumulation) because we work with voxels
(surface vertices need additional mapping of $Z$-maps calculated from smoothed
data); probably "difficult" to adapt to subcortical structures.
%
Future work could compare surface-based alignment that respects cortical folding
structure -- that out-performs predictions based on [affine] volume-based
anatomical alignment \citep{weiner2018defining} -- to surface-based functional
alignment.



\subsubsection{Time series vs connectivity-based}

\citet{jiahui2022cross} do connectivity-based 1-step hyperalignment across
different movie datasets which is as good as response hyperalignment; however,
it's a shitty procedure to scale because it needs a transformation matrix for
every pair of subjects (i.e no \ac{cfs})

%
``Derivation of this common space can be based on either neural response
profiles (e.g. data collected during tasks, such as movie viewing (Haxby et al.,
2011)) or functional connectivity profiles files
\citep{guntupalli2018computational}.
%
Response-based hyperalignment maps data from the anatomical space to a common
information space based on time-point response patterns across cortical
vertices.
%
Connectivity-based hyperalignment maps data from the anatomical space to a
common information space based on functional connectivity patterns''
\citep{busch2021hybrid}.

%
``The number of voxels that can be considered simultaneously for functional BOLD
response time series alignment is limited by the number of timepoints in the
calibration scan (about 300-400 voxels for a 15min scan with a 2s TR,
corresponding to a local cortical neighborhood of about 1cm in diameter for a
standard resolution).
%
This limitation does not exist in this form for a functional alignment that is
based on connectivity vectors.
%
The length of these connectivity vectors is determined by the number of
reference (or seed) regions in the brain'' [project proposal].


% Kumar on Nastase's ugly mofo paper
``Estimating the SRM from functional connectivity data rather than response time
series circumvents the need for a single shared stimulus across subjects.
%
Connectivity SRM allows us to derive a single shared response space across
different stimuli with a shared connectivity profile
\citep{nastase2019leveraging}'' \citep{kumar2020brainiak}.
%
``The sampling of connectivity vector space is defined by the selection of
connectivity targets, but the richness and reliability of connectivity estimates
depends on the variety of brain states over which connectivity is estimated''
\citep{haxby2020hyperalignment}.


%
``Response-based hyperalignment was shown to align response-based data better
than connectivity-based hyperalignment, whereas connectivity-based
hyperalignment was shown to better align connectivity-based data than
response-based hyperalignment
%
Response-based common spaces better align response data, whereas
connectivity-based common spaces better align connectivity data
\citep{guntupalli2018computational}'' \citep{busch2021hybrid}.




\subsection{Calculate $Z$-maps mean in the common space already}
%
I also tested averaging $Z$-maps in the \ac{cfs} (i.e.: not in the test
subject's voxel space): similar results
%
In case of anatomical alignment, I did not test averaging data in MNI152 space.


\subsection{Calculate $Z$-map from training subjects' TRs in FEAT}

iirc, I projected all subjects' localizer time series through
model space into test subject voxel space; then, calculated the contrast
with these data (s. scripts 'test/data\_denoise-vis.py' \&
'test/data\_srm-vis-to-ind.py').
%

The problem was: if one wants to test the different transformation matrices (I
only did it with one; imo, based on alignment using the whole audio-description)
it gets totally messy \& computational intensive.
%
Results were similar to the original procedure if not slightly worse.

\end{comment}
