\todo[inline]{I use "prediction" and "estimation" pretty much interchangeably}

\section{Introduction}

% higher visual areas higher visual areas
In the domain of higher-visual perception, functionally defined
category-selective brain regions, such as the \ac{ppa} \citep{epstein1998ppa},
the \ac{ffa} \citep{kanwisher1997ffa}, or \ac{eba} \citep{downing2001bodyarea}
exhibit significantly increased \ac{bold} activity correlated with a
``preferred'' \citep{debeck2008interpreting} stimulus class.
%
While the topographies (i.e. the location, size and shape) of these
category-selective areas are similarly distributed across individuals, the exact
topographies vary interindividually \citep{rosenke2021probabilistic,
zhen2017quantifying, zhen2015quantifying, frost2012measuring}.
% definition of localizer
To identify the topography of functional areas in individual persons,
block-design \textit{functional localizer} paradigms are traditionally used that
contrast modeled hemodynamic responses correlating with the corresponding
stimulus classes, such as landscapes, faces, or bodies.
% problem: one localizer for one domain
Functional localizers are designed to maximize detection power and thus limited
to mapping only one domain of brain functions, such as retinotopic visual areas
\citep{wang2015probabilistic}, category-selective regions
\citep{stigliani2015temporal}, theory of mind \citep{spunt2014validating}, or
semantic processes \citep{fedorenko2010new, fernandez2001language}.
% which gets messy
However, when mapping multiple functional domains in a limited amount of time is
desired, the "one paradigm for one domain of functions" approach becomes
impractical.
% localizer batteries: intro
To address this issue, researchers have attempted to create time-efficient,
multi-functional localizer batteries \textit{localizer batteries}
\citep[e.g.,][]{barch2013function, drobyshevsky2006rapid, pinel2007fast}.
% task based = shit
Nevertheless, the diagnostic quality of localizer paradigms heavily depends on a
participant's comprehension of the task instructions and general compliance, a
criterion that can be difficult to meet in clinical or pediatric populations
\citep{eickhoff2020towards, vanderwal2019movies}.

% ppa via audio-description Results also suggest that a naturally engaging,
% purely auditory paradigm like an audio-description could, in principle,
% substitute a visual localizer as a diagnostic procedure to assess brain
% functions in visually impaired % individuals \citep{haeusler2022processing}.
In \citet{haeusler2022processing}, we demonstrated that a functionally defined
region such as the \ac{ppa} can be localized using a model-driven \ac{glm}
analysis that is based on the annotated temporal structure of a two-hour long
naturalistic stimulus.
% full feature film is too long
However, for practical and monetary reasons, a two-hour long paradigm is
unsuitable for clinical applications.
% hence, predict from reference
One approach to reduce the time and costs is to identify a functional area in an
individual person's brain anatomy based on data collected from an independent
sample of different individuals (i.e. data from a \textit{reference group}).
% intro: estimation via common anatomical space
Previous studies have estimated the most probable location of a functional area
in an individual from a reference group by performing either a volume-based
\citep[e.g.,][]{zhen2017quantifying, zhen2015quantifying} or a surface-based
\citep[e.g.,][]{frost2012measuring, weiner2018defining,
rosenke2021probabilistic, wang2015probabilistic} \textit{anatomical alignment}.
%
First, in order to address the issue of anatomical variability across persons,
functional data of persons in the reference group are anatomically aligned to
(i.e.  projected into) a \textit{common anatomical space} (e.g., Montreal
Neurological Institute brain atlas; \citep[MNI152,][]{fonov2011unbiased}).
% project into test subject to estimate
Then, data are projected from the common anatomical space into the individual
person's brain anatomy to provide an estimate a functional region's location.
% volume-based alignment in one sentence
Volume-based anatomical alignment \citep[s.][for a review]{klein2009evaluation}
aligns voxels to a three-dimensional common anatomical space \citep[e.g., MNI152
atlas;][]{fonov2011unbiased}.
% surface-based alignment in one sentence
Surface-based anatomical alignment \citep{fischl1999cortical, yeo2009spherical}
aligns vertices to a two-dimensional common anatomical space \citep[e.g.,
FreeSurfer's fsaverage template;][]{fischl1999high}.
% difference in one sentence
Whereas volume-based alignment does not account for individual sulcal and gyral
folding patterns, surface-based alignment respects interindividual variability
of the cortical surface.
% surface-based estimation works better
Consequently, previous studies that compared volume-based and surface-based
alignment to estimate the location of functional regions have shown that
surface-based alignment reduces inter-subject variability and improves
estimation performance \citep{rosenke2021probabilistic, frost2012measuring,
wang2015probabilistic, weiner2018defining}.
% remaining variability after surface-based alignment
However, even after surface-based alignment, the anatomical location of
functional regions varies between individuals \citep[e.g.,][]{coalson2018impact,
benson2014correction, natu2021sulcal, wang2015probabilistic, frost2012measuring,
langers2014assessment, weiner2014mid, rosenke2021probabilistic}.
% frost as an example
\citet{frost2012measuring}, for example, localized 13 functional areas of the
high-level visual cortex and ``found a large variability in the degree to which
functional areas respect macro-anatomical boundaries'' \citep[][p.
1369]{frost2012measuring}.
% functional--anatomical correspondence
The remaining variability indicates that functional areas a not necessarily
bound to anatomical landmarks, and reflects the degree of
\textit{functional--anatomical correspondence} between a brain function and its
underlying anatomical location.

% case of PPA cf. also \citet{frost2012measuring, rosenke2021probabilistic}
% \citet{weiner2018defining} showed ``that cortical folding patterns and
% probabilistic predictions reliably identify place-selective voxels in medial
% VTC across individuals and experiments''.
%
% However, ``this structural-functional coupling is not always perfect and there
% is inter-subject variability as to how much the place-selective voxels extend
% within the parahippocampal gyrus, as well as the lingual gyrus and medial
% aspects of the fusiform gyrus.
%
% Despite this inter-subject variability, place-selective voxels are always
%located within the collateral sulcus across participants.''
%\citep{weiner2018defining}.
In order to address the issue of functional-anatomical variability across
subjects, \textit{functional alignment} algorithms, such as
\textit{hyperalignment} \citep{haxby2011common, guntupalli2016model} or the
\textit{shared response model} \citep{chen2015reduced, zhang2016searchlight},
have been developed.
%
Whereas anatomical alignment aligns voxels (or vertices) that share the same
anatomical location to a common anatomical space, functional alignment aligns
voxels (or vertices) that share similar functional properties to a
\textit{common functional space} (CFS).
%
Functional alignment algorithms are typically used to compute both a
high-dimensional, functional brain template (i.e. the \ac{cfs}) subject-specific
transformations from the functional data of a study's participants.
%
A subject-specific transformation allows to project functional data from a
subject's three-dimensional voxel space into the \ac{cfs} or vice versa
\citep{haxby2020hyperalignment, kumar2020brainiak}.
%
The \ac{cfs} and transformations are calculated (i.e. \textit{trained}) by
maximizing the inter-subject similarity of \ac{bold} response time series
correlating with a time-locked external stimulation \citep{haxby2011common,
chen2015reduced, sabuncu2010function}, or by maximizing the inter-subject
similarity of connectivity profiles \citep{feilong2018reliable,
guntupalli2018computational, nastase2019leveraging}.
%
Whereas connectivity-based functional alignment better aligns connectivity
profiles, response-based functional alignment better aligns response time-series
\citep{guntupalli2018computational}.
%
Although functional alignment algorithms can be applied to \ac{fmri} time series
data from paradigms employing simplified stimuli, data from naturalistic stimuli
provide
%
improved generalizability of the \ac{cfs}
%
and transformation matrices
%
to novel stimuli or tasks.
%
This is presumably because naturalistic stimuli sample a broader range of brain
states \citep{haxby2011common, guntupalli2016model}.

\todo[inline]{imo, validity (\& generalizability) of CFS and matrices are
non-separable claims!}

A more recent procedure \citep[e.g., ][]{jiahui2020predicting,
guntupalli2016model, haxby2011common} to estimate the most probable location of
a functional area in an individual from a reference performs an functional
alignment.
% solve functional-anatomical variability
First, the functional data of individuals in the reference group are
anatomically aligned to a common anatomical space.
%
Second, to address the issue of functional-anatomical variability across
individuals, the functional data are functionally aligned (i.e. projected into)
a \ac{cfs}.
%
Finally, data are projected from the \ac{cfs} into the individual's brain
anatomy, serving as an estimate of a functional region's location.
% Example: Jiahui (2020)
For instance, \citet{jiahui2020predicting} used surface-based hyperalignment to
calculate \acp{cfs} and transformations based on data from
%
the movie ``Grand Budapest Hotel'' ($\approx$\unit[50]{m}; \ac{tr}=\unit[1]{s}),
and
%
the movie ``Forrest Gump'' ($\approx$\unit[120]{m}; \ac{tr}=\unit[2]{s}).
%
\citet{jiahui2020predicting} then estimated $t$-contrast maps from a visual
localizer that aimed at identifying the \ac{ffa} by projecting the $t$-contrast
maps from a reference group through each \acp{cfs} into an individual's brain
anatomy.
%
Results showed that $t$-contrast maps from the visual localizer correlated more
highly with contrast maps that were estimated via hyperalignment than contrast
maps that were estimated via surface-based anatomical alignment.


\todo[inline]{\textit{criterion} and \textit{predictors} are defined here to
make the discussion easier; however, it's not predictor / criterion in a
strict sense as typically used (as referring to single variables)}

% focus: ppa
Here again, we focus on the \ac{ppa} \citep[e.g.,][for
reviews]{epstein2014neural, aminoff2013role}, and investigate whether we can
estimate the results of $t$-contrasts (i.e. $Z$-maps) that were created to
identify the \ac{ppa} using functional data from three different paradigms as
the to predicted \textit{criteria}:
%
(a) a classic visual localizer \citep{sengupta2016extension} as the assumed
``gold standard'',
%
(b) a movie \citep{haeusler2022processing}, and
%
(c) an auditory narrative \citep{haeusler2022processing}.
% math stuff from citep{vodrahalli2018mapping}
% ``SRM learns $N$ maps $W_{i}$ with orthogonal columns such that
% $||X_{i}-W_{i}S||_{F}$ is minimized over $\left\{ W_{i}\right\} _{i=1}^{N},S$,
% where $X_{i}\in\mathbb{R}^{v\times{T}}$ is the $i^{th}$ subject's fMRI
% response ($v$ voxels by $T$ repetition times) and
% $S\in\mathbb{R}^{k\times{T}}$ is a feature time-series in a $k$-dimensional
% shared space'' \citep{vodrahalli2018mapping}.
% Inverse vs. transpose of a matrix:
% for orthogonal transformations (like we should have here, i.e. only rotation,
% expansion) these two are one and the same thing:
% https://www.quora.com/When-is-the-inverse-of-a-matrix-equal-to-its-transpose
% why SRM
Our volume-based functional alignment approach utilizes the \ac{srm} algorithm
\citep{chen2015reduced, richard2019fast} as implemented in the open-source
software package BrainIAK \citep[Brain Imaging Analysis Kit;
\href{https://brainiak.org}{\url{brainiak.org}};][]{kumar2020brainiak,
kumar2020brainiaktutorial}.
% general overview of SRM
The \ac{srm} is an unsupervised probabilistic latent-factor model that
decomposes \ac{bold} \ac{fmri} response time series of participants who have
experienced the same stimulus into a \ac{cfs} of \textit{shared features}
\citep[also known as ``\textit{shared feature space}'';][]{chen2015reduced} and
subject-specific linear transformations matrices.
% math stuff
More specifically, the \ac{srm} algorithm uses each $n^{th}$ subject's response
time series represented as matrix $X_{n}$ ({$v$} voxels by $t$ time points) to
compute the \ac{cfs} $C$ ($k$ shared responses by $t$ time points) and
subject-specific transformation matrices $W_{n}$ ($v$ voxels by $k$ shared
responses) with orthonormal columns ($W_{n}^{T}W_{n}=I_{k}$).
% iteratively fitted
The algorithm randomly initializes and fits the transformation matrices over
iterations to minimize the error in explaining the participants' data, while
also learning the time course of the shared responses (s.
\href{https://brainiak.org/tutorials/11-SRM/}{\url{brainiak.org/tutorials/11-SRM}}).
% number of dimensions
In contrast to hyperalignment, the number of dimensions of the \ac{cfs} is not
set by the number of voxels, but rather it is determined by the researcher to a
number lower than the number of voxels, a procedure that also filters out noise
and reduces overfitting \citep{chen2015reduced}.
% phrase math in words
As a result, each shared feature can be thought of as a weighted sum of many
voxels across subjects \citep{kumar2020brainiak}.
% result = alignment
A subject-specific transformation matrix can thought of as the weight of each
voxel in a subject's voxel space on each shared feature, and allows a subject's
functionally data to be aligned to the \ac{cfs} by projecting responses within
the voxels into the $k$-dimensional \ac{cfs}.


\todo[inline]{check if $W_{n}^{T}W_{n}=I_{k}$ is True}

\todo[inline]{biggest issue (for discussion, too): how to separate validity /
generalizability of CFS from validity / generalizability of transformation
matrices?}

% multi-paradigm model
In contrast to previous studies \citep[e.g.][]{jiahui2020predicting,
guntupalli2016model, haxby2011common} that calculated a \ac{cfs} based on data
from a single paradigm, we calculated a \textit{multi-paradigm \ac{cfs}} based
on data from three paradigms.
% cross-validation
We followed an exhaustive leave-one-subject-out cross-validation (N$=$14
subjects) to train a shared feature space (i.e. the \ac{cfs}) based on
concatenated response time series of
%
the movie ``Forrest Gump'' ($\approx$\unit[120]{m}; \ac{tr}=\unit[2]{s}),
%
the movie's audio-description  that was produced for a visually impaired
audience ($\approx$\unit[120]{m}; \ac{tr}=\unit[2]{s}), and
%
a visual localizer ($\approx$\unit[21]{m}; \ac{tr}=\unit[2]{s})
%
from $N-1$ \textit{training subjects} as seen in
Fig.~\ref{fig:multi-stimulus-cfs}.
% four aspects to explore
The purpose of this study was to investigate four aspects.
%
First, we explored the validity and generalizability of our multi-paradigm
\ac{cfs} by predicting a left-out \textit{test subject}'s results from the
analysis of
%
(a) the localizer \citep{sengupta2016extension} as the assumed ``gold
standard'',
%
(b) the movie \citep{haeusler2022processing}, and
%
(c) the auditory narrative \citep{haeusler2022processing}
%
serving as the criteria.
% three predictors
Second, we use a test subject's response time series from each of the three
paradigms separately in order to align the test subject with the corresponding
\acp{tr} within the \ac{cfs}.
%
This \textit{partial alignment} lets us assess the validity and generalizability
of the three paradigms serving as \textit{predictors} (i.e. one
\textit{cross-subject-within-paradigm prediction}, and two
\textit{cross-subject-cross-paradigm predictions}).
% partial alignment
Third, considering the impracticality of using a complete naturalistic stimulus
in a clinical setting to align a test subject, we also explored the relationship
between the estimation performance of the results from each of the three
paradigms and the quantity of data from each of the three paradigms used to
functionally align the subject with the multi-paradigm \ac{cfs}.
% benchmark: anatomical alignment the criteria
Fourth, we compared the performance of our volume-based, functional alignment
procedures to the performance of a volume-based, anatomical alignment approach
that serves as a benchmark.

\todo[inline]{add 2-3 sentences stating the results}
%
% Our results provide evidence that transformation matrices calculated based
% on data from naturalistic stimuli promise an increased validity of derived
% transformation for functional alignment over transformation matrices based on
% data (of the same!) paradigm based on simplified stimuli.

\todo[inline]{add 2-3 sentences stating a conclusion \& vision}
% Our results suggest that it is possible to ``scan once, estimate many

\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/multi-stimulus-cfs.pdf}
\caption{
%
\textbf{Overview of the shared response model.
}
    %
    For each fold of the leave-one-subject-out cross-validation, each training
    subject's response time series of
    %
    a movie ($\approx$\unit[120]{m}; \ac{tr}=\unit[2]{s}),
    %
    the movie's audio-description ($\approx$\unit[120]{m}; \ac{tr}=\unit[2]{s}),
    %
    and the visual localizer ($\approx$\unit[21]{m}; \ac{tr}=\unit[2]{s})
    %
    were concatenated to serve as input for the \ac{srm} algorithm.
    %
    From these response time series represented as matrix $X_{n}$ ({$v$} voxels
    by $t$ time points), the algorithm calculates the common functional
    space (CFS) $C$ ($k$ shared features by $t$ time points) and
    subject-specific, transformation matrices $W_{n}$
    ($v$ voxels by $k$ shared features) with orthonormal columns
    ($W_{n}^{T}W_{n}=I_{k}$).
} \label{fig:multi-stimulus-cfs} \end{figure*}





\section{Methods}

% we get the data from the naturalistic PPA paper (its subdataset) datalad get
% -n inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned datalad
%  get
%  inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in\_bold3Tp2/sub-??\_task-a?movie\_run-?\_bold*.*

% reference to PPA-Paper
For the current study, we used the same subset of the studyforrest dataset that
we previously used in \citet{haeusler2022processing}:
%
The sample included the same fourteen participants who
% VIS
(a) participated in a dedicated six-category block-design visual localizer
\citep{sengupta2016extension},
% AV
(b) watched the audio-visual movie ``Forrest Gump''
\citep{hanke2016simultaneous}, and
% AD
(c) listened to the movie's audio-description \citep{hanke2014audiomovie}.
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications, while a summary is provided in \citet{haeusler2022processing}.



\subsection{Preprocessing}

% data sources
The analyses in this study were conducted on the same preprocessed fMRI data he
current analyses were carried out on the same preprocessed fMRI data (s.
\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned
}{\url{github.com/psychoinformatics-de/studyforrest-data-aligned}}) that were
used for
%
(a) the technical validation of the dataset \citep{hanke2016simultaneous},
%
(b) the localization of higher-visual areas \citep{sengupta2016extension}, and
%
(c) the investigation of responses of the \ac{ppa} correlating with naturalistic
spatial information in \citep{haeusler2022processing}.
%
We reran the preprocessing and analyses steps performed in
\citet{sengupta2016extension} and \citet{haeusler2022processing} using FEAT
v6.00 \citep[FMRI Expert Analysis Tool;][]{woolrich2001autocorr} as shipped with
FSL v5.0.9 \citep[\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software
Library;}][]{smith2004fsl} to reproduce the time series that served as input for
the previous statistical analyses and their results (i.e. the statistical
$Z$-maps).
% temporal filtering
The preprocessing steps included high-pass temporal filtering (using a
Gaussian-weighted least-squares straight line) for every run of the visual
localizer (cutoff period of \unit[100]{s}), and every segment of the movie and
audio-description (cutoff period of \unit[150]{s}).
% brain extraction & spatial smoothing
Brain extraction was performed using BET \citep{smith2002bet}., and data from
all three paradigms were spatially smoothed using a Gaussian kernel with a full
width at half maximum of \unit[4.0]{mm}.
% grand mean normalization
A grand-mean intensity normalization was applied to each run of the functional
localizer and each segment of the naturalistic stimuli.
%
Further analyses on these reproduced times series were performed using Python
scripts that relied on
%
NiBabel v3.2.1 (\href{https://nipy.org}{\url{nipy.org}}),
%
NumPy v1.20.2 (\href{https://numpy.org}{\url{numpy.org}}),
%
Pandas v1.2.3 (\href{https://pandas.pydata.org}{\url{pandas.pydata.org}}),
%
Scipy v1.6.2 (\href{https://scipy.org}{\url{scipy.org}}),
%
scikit-learn v1.0 (\href{https://scikit-learn.org}{\url{scikit-learn.org}}),
%
BrainIAK v0.11
\citep[\href{https://brainiak.org}{\url{brainiak.org}}][]{kumar2020brainiak,
kumar2020brainiaktutorial},
%
Matplotlib v3.4.0 (\href{https://matplotlib.org}{\url{matplotlib.org}}),
%
seaborn v0.11.2 (\href{https://seaborn.pydata.org}{\url{seaborn.pydata.org}}),
%
and calling command line functions of FSL.

%\paragraph{Fixing FSL output}

% grand_mean_for_4d.py (formerly: data_normalize_4d.py):
% is not necessary anymore: FSL has applied grand mean scaling to
% 'filtered_func_data.nii.gz'

% input: 'sub-*/run-?.feat/filtered_func_data.nii.gz' (of VIS, AO & AV)
% output: saved to 'sub-??_task-*_run-?_bold_filtered.nii.gz'

% FSL adds back the mean value for each voxel's time course at the end of the
% preprocessing;
% hence, the script substracts that mean again but multiplies it by 10000
% (like FSL does it, too)

% definition of grand mean scaling for 4d data:
% voxel values in every image are divided by the average global mean
% intensity of the whole session. This effectively removes any mean global
% differences in intensity between sessions.

% FSL User Guide:
% filtered_func_data will normally have been temporally high-pass filtered,
% it is not zero mean; the mean value for each voxel's time course has been
% added back in for various practical reasons.
% When FILM begins the linear modeling, it starts by removing this mean.

\todo[inline]{restriction to ROIs is not mentioned in the intro. Is that okay?}

% masks-from-mni-to-bold3Tp2.py:
% - merges unilateral ROIs overlaps (already in MNI) to bilateral ROI
% - output: 'masks/in_mni/PPA_overlap_prob.nii.gz'
% - warps union of ROIs from MNI into each subjects space
% output: 'sub-*/masks/in_bold3Tp2/grp_PPA_bin.nii.gz' + audio_fov.nii.gz dilate
% the ROI masks by 1 voxel; output: 'grp_PPA_bin_dil.nii.gz'

% masks-from-mni-to-bold3Tp2.py:
% warp MNI masks into individual bold3Tp2 spaces

% masks-from-t1w-to-bold3Tp2.py:
% transforms 'inputs/tnt/sub-*/t1w/brain_seg*.nii.gz'
% into individual's bold3Tp2
% output: 'sub-*/masks/in_bold3Tp2/brain_seg*.nii.gz'

% mask-builder-voxel-counter.py:
% builds different individual masks by dilating, merging other masks
% creates a FoV of AO stimulus for every subject from 4d time-series of AO run
% output: sub-*/masks/in_bold3Tp2/audio_fov.nii.gz'
% counts the voxels
% long story short: we cannot used all gyri that contain PPA to some degree
% even if the mask by FoV of AO stimulus and individual gray matter mask

% data_mask_concat_runs.py:
% masks are not dilated and not masked with subject-specific gray matter mask
% outputs:
% 'sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy
% 'sub-*_task_visloc_run-1-4_bold-filtered.npy'

% reason why we do it
The \ac{srm} requires that the number of samples (i.e. the number of \acp{tr})
exceed the number of features (the number of voxels).
%
In order to restrict the number of voxels, we created bilateral \acp{roi} for
each subject.
%
Specifically, we warped the union of individual \acp{ppa}
citep[s.][]{haeusler2022processing} from MNI space into each subject's voxel
space using subject-specific, non-linear transformation matrices that were
previously computed
\citep[][\href{https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms
}{\url{github.com/psychoinformatics-de/studyforrest-data-templatetransforms}}]{hanke2014audiomovie}.
% applying masks
The time series data of each subject were then masked in their native voxel
space by the union of individual \acp{ppa} and the subject-specific \ac{fov} of
the audio-description.
% voxels = [1665, 1732, 1400, 1575, 1664, 1951, 1376, 1383, 1683, 1887, 1441,
% 1729, 1369, 1437] median = 1619.5
The number of remaining voxels per subject (range 1369--1951,
$\overline{X}=1592$, $SD=188$) can be seen in Fig.~\ref{fig:plot_voxel-counts}.
% normalization
Data of each run were normalized ($z$-scored) to a mean of zero
($\overline{X}=0$) and a standard deviation of one ($SD=1$).
%
Due to an image reconstruction problem \citep[s.][]{hanke2014audiomovie}, the
last 75 \acp{tr} of the audio-description were missing in subject 04.
%
The \ac{srm} allows for different numbers of voxels across subjects, but the
number of \acp{tr} must be the same.
%
Consequently, we removed the last 75 \acp{tr} of the audio-description from the
time series of all other subjects.
% summary; AO + AV = 7123 TRs (not 7198 TRs anymore); localizer has 4 x 156 TRs
As a result, the data used to fit the \ac{srm} in the next step included 3599
\acp{tr} from the movie, 3524 \acp{tr} from the audio-description, and 624
\acp{tr} from the visual localizer experiment (7747 \acp{tr} in total).
%% concatenate and z-score
The time series of all three paradigms were concatenated and $z$-scored.

\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/plot_voxel-counts.pdf}
\caption{
%
\textbf{Number of voxels in the bilateral regions of interest (ROIs)
of each subject.}
%
In order to reduce the number of voxels, we warped the union of
individual \acp{ppa} \citep[cf. Fig. 1 in][]{haeusler2022processing} from
MNI152 space into each subject's native voxel space.
%
The remaining voxels of each subject were further constrained to those
voxels that are included in the respective subject's \ac{fov} of the
audio-description \citep[cf.][]{hanke2014audiomovie}.
}
\label{fig:plot_voxel-counts}
\end{figure*}


\begin{comment}

The number of remaining voxels per subject can be seen in Table
\ref{tab:ppamaskvoxels} (range 1369--1951, $\overline{X}=1592$, $SD=188$).


\begin{table*}[btp] \caption{
%
\textbf{Table heading.}
%
The number of remaining voxels after masking time series data of each paradigm
and subject with the union of individual \acp{ppa} warped from MNI space
into each individual's subjects-space and the subject-specific field of view
of audio-description.
    }

\label{tab:ppamaskvoxels}
\begin{tabular}{ll}
\toprule
\textbf{Subject} & \textbf{no. of voxels} \\
\midrule
sub-01 & 1665 \tabularnewline
sub-02 & 1732 \tabularnewline
sub-03 & 1400 \tabularnewline
sub-04 & 1575 \tabularnewline
sub-05 & 1664 \tabularnewline
sub-06 & 1951 \tabularnewline
sub-14 & 1376 \tabularnewline
sub-09 & 1383 \tabularnewline
sub-15 & 1683 \tabularnewline
sub-16 & 1887 \tabularnewline
sub-17 & 1441 \tabularnewline
sub-18 & 1729 \tabularnewline
sub-19 & 1369 \tabularnewline
sub-20 & 1437 \tabularnewline
\bottomrule
\end{tabular}
\caption*{The legend text goes here.}
\end{table*}

\end{comment}




\subsection{Estimation via functional alignment}


\subsubsection{Overview}
%
To estimate the empirical $Z$-maps for $t$-contrasts using functional alignment,
we followed a four-step procedure.
% create CFS and training subjects' matrices
First, for every fold of a leave-one-out cross-validation (N$=$14 subjects), we
trained a \ac{srm} on $N-1$ training subjects' response time series of the
movie, the audio-description, and the visual localizer.
% results in...
This step generated a \ac{cfs} for each fold of the cross-validation and an
orthonormal transformation matrix for each training subject.
% align test subject
Second, we aligned the test subject to the corresponding \acp{tr} within the
\ac{cfs} using time series data from the visual localizer, the movie, or the
audio-description.
%
Second, we aligned the test subject's time series data from the movie,
audio-description, and visual localizer paradigms separately to the
corresponding TRs within the \ac{cfs}.
%
This step produced different transformation matrices for the test subject based
on data from different paradigms.
% quantity vs. performance
In order to examine the relationship between estimation performance and the
amount of data used to generate a transformation matrix, we also varied the
number of runs of the paradigms.
%
This step produced transformation matrices based on an increasing number of runs
per paradigm.
%
In the third step, we mapped the training subjects' empirical $Z$-maps from
their voxel space into the \ac{cfs} using their transformation matrices.
% project from CFS into test subject
Finally, we projected the training subjects' $Z$-maps from the \ac{cfs} into the
test subject's voxel space using the transpose of the test subject's
transformation matrix
% actual prediction
We obtained the test subject's predicted $Z$-maps by calculating the arithmetic
mean of the projected Z-maps



\subsubsection{Fitting the SRM}
%
In order to obtain the \ac{cfs} and the training subjects' transformation
matrices, we used the probabilistic \ac{srm} algorithm that is implemented in
BrainIAK v.11 \citep[Brain Imaging Analysis Kit;][]{kumar2020brainiak,
kumar2020brainiaktutorial}, and approximates the \ac{srm} based on the
Expectation Maximization (EM) algorithm as proposed by \citet{chen2015reduced}
and optimized by \citet{anderson2016enabling}.
% number of dimensions / features ``The effect of number of PCs on BSC was
% similar for models that were based only on Princeton (n = 10) or Dartmouth (n
% = 11) data, suggesting that this estimate of dimensionality is robust across
% differences in scanning hardware and scanning parameters''
% \citep{haxby2011common}.
%
% ``These dimensionality estimates are a function of the spatial and temporal
% resolution of fMRI and the number and variety of response vectors used to
% derive the common space'' \citep{guntupalli2016model}.
%
% ``The true dimensionality of representation in human cortex surely involves
% vastly more distinct tuning functions. Estimates of the dimensionality of
% cortical representation, therefore, will almost certainly be much higher as
% data with higher spatial and temporal resolution for larger and more varied
% samples of response vectors are used to build new common models''
% \citep{guntupalli2016model}.
We chose a value of $k=10$ for the number of shared features (i.e. the number of
dimensions in the \ac{cfs}) based on the temporal and spatial resolution of our
data (\ac{tr} = \unit[2]{s}; \unit[2.5 $\times$ 2.5 $\times$ 2.5]{mm}), the
average number of voxels per \ac{roi}, and findings from
\citet{haxby2011common}.
%
\citet{haxby2011common} used hyperalignment to create a \ac{cfs} of 1,000
dimensions based of functional data (\ac{tr} = \unit[3]{s}) of voxels (\unit[3
$\times$ 3 $\times$ 3]{mm}) located in the ventral temporal cortex.
%
Then, \citet{haxby2011common} reduced the dimensionality of the \ac{cfs} by
applying a \ac{pca} in order to determine the subspace that is sufficient to
capture the full range of response-pattern distinctions.

They then applied principal component analysis \ac{pca} to reduce the
dimensionality of the \ac{cfs} to determine the subspace that sufficiently
captured the range of response-pattern distinctions.
%
Furthermore, the cortical topographies of category-selective brain regions were
preserved in the 35-dimensional \ac{cfs}.
% ...as judged by visual inspection
In the present study, we also computed \acp{cfs} of $k=5, 20, 30, 40, 50$ but
prediction performance based on these \acp{cfs} barely varied from a
10-dimensional \ac{cfs}.
% iterations:
The algorithm was set to iterate 30 times to minimize the error.

% correlations of regressors
In order to visualize characteristics of the \ac{cfs}, we calculated the Pearson
correlation coefficients between the shared responses and the regressors that
were previously modeled \citep[cf.][]{sengupta2016extension,
haeusler2022processing} to investigate hemodynamic responses during the three
paradigms.
%
As an example, we chose the \ac{cfs} that was created in the first fold of the
cross-validation from $N-1$ subjects to estimate $Z$-maps of subject 01.
%
The time series of the shared features were trimmed to match the corresponding
\acp{tr} of the respective paradigms.
%
Fig.~\ref{fig:corr-vis-reg-srm} shows the correlations between regressors
created to model hemodynamic responses during the visual localizer and shared
responses (trimmed to \acp{tr} that match the visual localizer).
Fig.~\ref{fig:corr-av-reg-srm} shows the correlations between regressors created
to model hemodynamic responses during the movie \citep[cf. Table 3
in][]{haeusler2022processing} and shared responses, while
Fig.~\ref{fig:corr-ao-reg-srm} shows the correlations between regressors created
to model hemodynamic responses during the audio-description \citep[cf. Table 3
in][]{haeusler2022processing} and shared responses.


\todo[inline]{What do the plots suggest?}

\todo[inline]{make colors of non-used TRs more transparent (alpha=15)}

% shuffle runs
As a negative control, we randomly shuffled the order of runs of the visual
localizer and the segments of the naturalistic stimuli separately for each
paradigm and training subject. We then concatenated the time series, fit the
\ac{srm}, and calculated the Pearson correlation coefficients.
%
We expected that the \ac{srm} algorithm would fail to fit ``meaningful''
\todo{??} shared responses to randomly shuffled training data.
%
As hypothesized, the results based on shuffled time series revealed no or only
minor correlations between the shared responses and regressors, as shown in
Fig.~\ref{fig:corr-vis-reg-srm-shuffled},
Fig.~\ref{fig:corr-av-reg-srm-shuffled}, and
Fig.~\ref{fig:corr-ao-reg-srm-shuffled}.


\todo[inline]{Plots are located in the appendix; add representation of the model
to the plots}


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/corr_vis-regressors-vs-cfs_sub-01_srm-ao-av-vis_feat10-iter30_7123-7747.pdf}
\caption{
%
\textbf{Pearson correlation coefficients between regressors of the visual
localizer and shared features.}
%
The time series of the shared features within the multi-paradigm \ac{cfs}
%
(as calculated for subject 01 in the first fold of the cross-validation)
%
were trimmed to match the corresponding \acp{tr} of the visual localizer
paradigm \citep{sengupta2016extension}.
%
The six regressors of the visual localizer model hemodynamic responses to
the six categories of pictures that were presented in blocks.
}
\label{fig:corr-vis-reg-srm}
\end{figure*}


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/corr_av-regressors-vs-cfs_sub-01_srm-ao-av-vis_feat10-iter30_3524-7123.pdf}
\caption{
%
\textbf{Pearson correlation coefficients between regressors of the movie
and shared features}
%
The time series of the shared features within the multi-paradigm \ac{cfs}
%
(as calculated for subject 01 in the first fold of the cross-validation)
%
were trimmed to match the corresponding \acp{tr} of the movie
\citep{hanke2016simultaneous}.
%
The regressors \texttt{vse\_new} to \texttt{vno\_cut} are based on
annotations movie frames, whereas the regressors
\texttt{fg\_av\_ger\_lr} to \texttt{fg\_av\_ger\_ud} represent low-level
visual or auditory confounds
\citep[cf. Table 3 in][]{haeusler2022processing}.
}
\label{fig:corr-av-reg-srm}
\end{figure*}


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/corr_ao-regressors-vs-cfs_sub-01_srm-ao-av-vis_feat10-iter30_0-3524.pdf}
\caption{
%
\textbf{Pearson correlation coefficients between regressors of the
audio-description and shared features.}
%
The time series of the shared features within the multi-paradigm \ac{cfs}
%
(as calculated for subject 01 in the first fold of the cross-validation)
%
were trimmed to match the corresponding \acp{tr} of the
audio-description \citep{hanke2014audiomovie}.
%
The regressors \texttt{body} to \texttt{sex\_m} are based on
annotations of nouns spoken by the audio-description's narrator,
whereas the regressors \texttt{fg\_ad\_ger\_lrdiff} and
\texttt{fg\_ad\_ger\_rms} represent low-level auditory confounds
\citep[cf. Table 3 in][]{haeusler2022processing}.
%
\texttt{geo\&groom} is a combination of
regressors as used on the positive side of the primary contrasts aimed to
localize the \ac{ppa} \citep[cf. Table 5 in][]{haeusler2022processing}.
}
\label{fig:corr-ao-reg-srm}
\end{figure*}


\subsubsection{Alignment of the test subject}

% AO: 0-451, 0-892, 0-1330, 0-1818, 0-2280, 0-2719, 0-3261, 0-3524
% AV: 3524-3975, 3524-4416, 3524-4854, 3524-5342, 3524-5804, 3524-6243,
%     3524-6785, 3524-7123
% AO+AV: 0-7123

% in-code documentation of
% https://github.com/brainiak/brainiak/blob/master/brainiak/funcalign/srm.py
% says: # Solve the Procrustes problem; A =
% subjectFMRIdata.dot(SharedResponses.T) U, \_, V = np.linalg.svd(A,
% full\_matrices = False) return U.dot(V)

%
We aligned the test subject's response time series from the visual localizer,
the movie, or the audio-description to the corresponding \acp{tr} within the
\ac{cfs} by factorizing the response time series data via singular value
decomposition.
%
This step produced an orthonormal transformation matrix $W_{n}$ each paradigm
that allow a mapping of data from a test subject's voxel space into the
\ac{cfs}.
%
To investigate how the amount of data used to acquire a transformation matrix
affects estimation performance, we also varied the number of runs per paradigm.
%
Specifically, we we used one up to four runs (each lasting \unit[5.2]{m}) of the
visual localizer, and one up to eight segments (each lasting
$\approx$\unit[15]{m}) to align the test subject to the corresponding \acp{tr}
within the \ac{cfs}.
%
Therefore, for each test subject, we obtained four matrices based on data from
the visual localizer and eight different matrices per naturalistic stimulus,
each transformation matrix having a size of $v$ voxels by $k$ shared responses
but being based on an increasing amount of data used to calculate the mapping.


\subsubsection{Estimation of $t$-contrasts' results}

% overview
to estimate the results of
the three $t$-contrast (i.e. the \textit{empirical $Z$-maps}).


We estimated the results of the three $t$-contrast (i.e. the \textit{empirical
$Z$-maps}) of the test subject by projecting the empirical $Z$-maps of all
training subjects trough the \ac{cfs} into the test subject's voxel space:
% functional alignment; into CFS (calling srm.transform(masked\_zmaps))
First, we masked the empirical $Z$-maps of the training subjects' empirical
$Z$-maps with the same subject-specific masks used to generate the \ac{cfs} from
the time series data.
%
Then,  we used the transformation matrices derived during the training of the
\ac{cfs} to map the masked empirical $Z$-maps from each training subject's voxel
space into the \ac{cfs}.
%
Next, we used the transpose of a transformation matrix obtained from the
alignment of the test subject to project the $Z$-maps from the \ac{cfs} into the
test subject's voxel space.
% take the mean
For each of the three $t$-contrasts, we computed the arithmetic mean of the
respective projected $Z$-maps, which served as an estimation of the test
subject's empirical $Z$-maps (hence, a \textit{predicted $Z$-maps}).

\subsection{Estimation via anatomical alignment}

\todo[inline]{strictly speaking data were not projected through the MNI152 atlas
but through a study-specific brain template co-registered to the MNI152 atlas,
wasn't it?}

\todo[inline]{was the matrix the transpose or a "totally" different one?}

%
As a baseline, we used an anatomical alignment procedure to estimate the results
of $t$-contrast of each paradigm.
%
We predicted a test subject's empirical $Z$-maps from the analysis of
%
the visual localizer \citep{sengupta2016extension},
%
the movie \citep{haeusler2022processing}, and
%
the audio-description \citep{haeusler2022processing}
%
using the training subjects' results of the same paradigm (hence,
cross-subject-within-paradigm predictions).
%
To do this, we projected the masked empirical $Z$-maps of each paradigm and each
subject were projected from their native voxel space into the MNI space via
previously computed subject-specific transformation matrices
\citep[][\href{https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms}{\url{github.com/psychoinformatics-de/studyforrest-data-templatetransforms}}]{hanke2014audiomovie}
% from MNI into subject
We then used the transpose of the transformation matrix to project the data from
the MNI space into the test subject's voxel space.
% take the mean
Similar to our functional alignment procedure, we obtained an estimation of the
test subject's empirical $Z$-maps by taking the arithmetic mean of the projected
$Z$-maps.



\subsection{Cronbach's alpha}

\todo[inline]{I have no clue where \citet{jiahui2020predicting, jiahui2022cross}
have the statement about what Cronbach's expresses from...}

\todo[inline]{Shift results stated here to actual result section? In particular,
in case a t-test involving Cronbach's is calculated (which is not the case so
far)?}



%
We calculated Cronbach's $\alpha$ as a measure of reliability and the amount of
measurement error \citep{cronbach1951coefficient, cortina1993coefficient}
present in the empirical $Z$-maps of each paradigm and subject.
%
Cronbach's $\alpha$ expresses the expected correlation between the currently
used empirical $Z$-maps and an additional set of empirical $Z$-maps calculated
based on data of a hypothetical independent dataset collected from the same
paradigm and subjects \citep{jiahui2020predicting, jiahui2022cross}.
%
These expected correlations, represented by Cronbach's $\alpha$, were calculated
based on the first-level \ac{glm} $Z$-maps (four in case of the visual
localizer; eight in case of the naturalistic stimuli) that were averaged in the
second-level \ac{glm} analyses of \citet{sengupta2016extension} and
\citet{haeusler2022processing}.
%
Cronbach's $\alpha$ of empirical (i.e. second-level) $Z$-maps for each subject
and paradigm can be seen in Fig.~\ref{fig:cronbachs}, descriptive statistics
across subjects for each paradigm can be seen in Table~\ref{fig:cronbachs}.

\todo[inline]{imo, descriptive statistics given in a text are not necessary;
i.e. plot and/or table are better}

%Visual localizer:  mean=0.899990, std=0.087051, min=0.658523,
%25\%=0.906643, 50\%=0.934019, 75\%=0.947906, max=0.963065.
%
%Movie: mean=0.611332, std=0.137878, min=0.284440,
%25\%=0.555529, 50\%=0.627240, 75\%=0.676353, max=0.800254.
%
%Audio-description: mean=0.476194, std=0.358019, min=-0.526626,
%25\%=0.428975, 50\%=0.627799, 75\%=0.679987, max=0.823584.

\todo[inline]{movie's outlier: sub-06 (0.28); but when movie PPA is predicted he
/ she is not the outlier}

\todo[inline]{audio-description's outlier: sub-05 (\textbf{-0.5!!}), sub-02
(0.0), sub-20 (0.27); when audio PPA is predicted (based on max of functional
data per paradigm), these three subjects are the outliers}

The Cronbach's $\alpha$ values for each paradigm are as follows:
%
for the visual localizer,
%
the mean is 0.899990 with a standard deviation of 0.087051.
%
The minimum value is 0.658523, the 25th percentile is 0.906643, the median is
0.934019, the 75th percentile is 0.947906, and the maximum is 0.963065.
%
For the movie paradigm,
%
the mean is 0.611332 with a standard deviation of 0.137878.
%
The minimum value is 0.284440, the 25th percentile is 0.555529, the median is
0.627240, the 75th percentile is 0.676353, and the maximum is 0.800254.
%
Finally, for the audio-description paradigm, the mean is 0.476194 with a
standard deviation of 0.358019.
%
The minimum value is -0.526626, the 25th percentile is 0.428975, the median is
0.627799, the 75th percentile is 0.679987, and the maximum is 0.823584.




\begin{table*}[btp]
\centering
    \caption{
    %
    \textbf{Descriptive statistics of Cronbach's $\alpha$ across subjects.}
    %
    Imo, not super necessary to provide these numbers. Stripplot + boxplots
    could be sufficient. If table is supposed to be kept, round the numbers,
    write a good description for the table.}
\label{tab:cronbachs}
\begin{tabular}{llll}
    \toprule
    \textbf{statistic} & \textbf{localizer} & \textbf{movie} & \textbf{audio-description} \\
    \midrule
    mean & 0.89999 & 0.611332 & 0.476194 \tabularnewline
    std & 0.087051 & 0.137878 & 0.358019 \tabularnewline
    min & 0.658523 & 0.28444 & -0.526626 \tabularnewline
    25\% & 0.906643 & 0.555529 & 0.428975 \tabularnewline
    50\% & 0.934019 & 0.62724 & 0.627799 \tabularnewline
    75\% & 0.947906 & 0.676353 & 0.679987 \tabularnewline
    max & 0.963065 & 0.800254 & 0.823584 \tabularnewline
    \bottomrule
\end{tabular}
\caption*{The legend text goes here.}
\end{table*}


\begin{figure*}[tbp] \centering
    \includegraphics[width=\linewidth]{figures/plot_cronbachs.pdf}
    \caption{\textbf{Cronbach's $\alpha$ of the empirical $Z$-maps for each
    paradigm and subject.}
    %
    Cronbach's $\alpha$ was calculated based on the $Z$-maps yielded by the
    first-level \ac{glm} analyses of the visual localizer
    \citep{sengupta2016extension} (four runs) and naturalistic stimuli
    \citep{haeusler2022processing} (eight segments each) respectively.
    %
    The second-level \ac{glm} analyses across runs / segments yielded the
    empirical $Z$-maps that were estimated in the present study.
    }
    \label{fig:cronbachs}
\end{figure*}



\pagebreak



\section{Results}


\begin{comment}

``Because the localizer task comprises several scanning runs, we calculated the
reliability of the localizer across runs with Cronbach's alpha to provide an
estimate of the noise ceiling for these correlations'' \citep{jiahui2022cross}.

\end{comment}


%
In order to quantify the estimation performance of the alignment procedures, we
correlated each individual's empirical $Z$-maps gained from previous analyses
\citep{sengupta2016extension, haeusler2022processing} with their respective
predicted $Z$-map (cf. Fig.~\ref{fig:stripplot}).
%
Unthresholded empirical and predicted $Z$-maps in their respective subject's
voxel space can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}\todo{still
empty}.


\begin{figure*}[tbp] \centering
    \includegraphics[width=\linewidth]{figures/plot_corr-emp-vs-estimation.pdf}
    \caption{
    %
    \textbf{Correlations between empirical and predicted
    \textit{\textbf{Z}}-maps for each paradigm and subject.}
    %
    Functional alignment was performed based on an increasing amount of
    functional data used to align a test subject to the common functional space
    (CFS): runs of the visual localizer paradigm lasted \unit[5.2]{m}; segments
    of the naturalistic stimuli lasted $\approx$\unit[15]{m}.
    %
    Solid horizontal lines:
    %
    median of Cronbach's $\alpha$ across subjects for empirical $Z$-maps of the
    respectively estimated paradigm (cf. Fig.~\ref{fig:cronbachs}).
    %
    Dotted horizontal lines:
    %
    mean of Cronbach's $\alpha$ across subjects for empirical $Z$-maps of the
    respectively estimated paradigm (cf. Fig.~\ref{fig:cronbachs});
    %
    Grey dots:
    %
    correlations between empirical $Z$-maps and an estimation using anatomical
    alignment.
    %
    A left-out subject's $Z$-map was estimated by projecting the training
    subjects' $Z$-maps ($N=13$) from their respective voxel space through the
    MNI152 space into the test subject's voxel space, then averaging the values
    across projected $Z$-maps;
    %
    Green dots:
    %
    correlations between empirical $Z$-map and an estimation using functional
    alignment based an transformation matrices calculated from one up to four
    runs (each lasting \unit[5.2]{m}) of the visual localizer.
    %
    Red dots:
    %
    correlations between empirical $Z$-map and an estimation using functional
    alignment based an transformation matrices calculated from one up to eight
    segments (each lasting $\approx$\unit[15]{m}) of the movie.
    %
    Blue dots:
    %
    correlations between empirical $Z$-map and an estimation using functional
    alignment based an transformation matrices calculated from one up to eight
    segments (each lasting $\approx$\unit[15]{m}) of the audio-description.
    %
    \textbf{to do: keep both, median and mean of Cronbach's? Improve legend
    somehow without crowding figure / subplots?}
   }
    \label{fig:stripplot}
\end{figure*}


\subsection{General trends in the results}

\todo[inline]{imo, results should start with describing general trends according
to the "four aspects under investigation" stated in the intro}

%
The [mean] Pearson correlation coefficients vary depending on the criterion to
be estimated (i.e. $Z$-maps of the visual localizer, movie, or
audio-description), the functional alignment procedure (anatomical vs.
functional alignment), and---in case of functional alignment---on the quantity
of a paradigm's data used to align a test subject to the \ac{cfs} as can be seen
in Fig.~\ref{fig:stripplot}.
%
However, the functional alignment procedure across criteria and predictors
reveals a monotonically increasing estimation performance the more functional
data are used to align the test subjects.


\subsection{The exemplary tests}

\todo[inline]{report descriptive statistics of the (un)transformed
correlations?}
%
%The mean Pearson correlation values [not yet Fisher transformed] between
%empirical $Z$-maps and predicted $Z$-maps via anatomical alignment were 0.xx
%($N=14$, $\overline{X}=0.xx$, $SD=0.xx$, range [?], median
% [9], 25\%, 50\%, 75\%).

\todo[inline]{I tested all samples of Fisher transformed correlations for
normality via Shapiro-Wilk test (imo, the most appropriate test here)}

\todo[inline]{of all samples that were part of one of the eleven t-test, just
one sample (4 runs of localizer in order to estimate localizer) accepted H1
(i.e. the sample is NOT drawn from normal distribution); compute Wilcoxon
signed-rank test (which is for dependent samples)?}

\todo[inline]{t-test assumes equal variances which is probably often not
fulfilled; alternative: two sample permutation test?}

\todo[inline]{correct the alpha-level (as it is done now) or the p-values?}

\todo[inline]{report descriptive statistics of the correlations?}

\todo[inline]{round the values or just write $p$<.0001}

\todo[inline]{give one-sentence interpretations / summary à la "Results
suggest..."? imo, that's not good}

\todo[inline]{so far, I did not test any difference(s) to Cronbach's; s.
comments below where it would makes sense to test difference to Cronbach's}

%
In order to investigate the differences between some conditions [well, if you
know what I mean] more closely, we post-hoc\todo{?? it's not post-hoc an ANOVA}
performed eleven [final number?] paired t-tests on Fisher z-transformed
correlation values, and set the $\alpha$-level to a Bonferroni corrected
$\alpha$ of $0.05 / 11 = 0.00\overline{45}$.

%
\textbf{For example, in case of estimating the $Z$-maps of the visual
localizer},
%
the correlations between empirical $Z$-maps and $Z$-maps predicted
via the first movie segment were significantly higher than the correlations
between empirical $Z$-maps and $Z$-maps predicted via anatomical alignment
(Fisher z-transformed, t(14)= 6.3525802, $p$=0.0000253).
%
However, functional alignment based on data of the visual localizer
(within-paradigm prediction) and audio-description was lower than anatomical
alignment independent of the number of runs / segments used to align the test
subjects to the \ac{cfs} [not tested; clear from eyeballing].
%
Whereas the functional alignment via the first movie segment (451 \acp{tr})
yielded significantly higher correlations than a functional alignment via four
localizer runs lasting 624 \acp{tr} (Fisher z-transformed, t(14)=5.8905545,
$p$<0.0000532),
%
the functional alignment via the first segment of the audio-description yielded
significantly lower correlations than the functional alignment via four
localizer runs (Fisher z-transformed, t(14)=-2.3000009, $p$<0.0386588, will not
be significant when Bonferroni corrected).
%
The mean correlation between empirical $Z$-maps and $Z$-maps predicted via the
first movie segment were significantly lower than the correlations between
empirical $Z$-maps and $Z$-maps predicted via the first two movie segments
(Fisher z-transformed, t(14)= -5.4946197, $p$=0.0001031, Bonferroni corrected).
%
The difference between mean correlations based on two movie segments in
comparison the mean correlation based on three movie segments was not
significant (Fisher z-transformed, t(14)= -0.1293547, $p$=0.8990569).

\todo[inline]{possibly run test that compares 8 movie segments to Cronbach's
alpha? a.k.a. 8 runs of movie for functional alignment are "not as good" as
Cronbach's alpha}



\textbf{In case of estimating the $Z$-maps of movie,...}

\todo[inline]{following phrasing is just copied from above; values are adjusted
accordingly}

...the correlations between empirical $Z$-maps and predicted $Z$-maps via the
first movie segment were significantly higher than the correlations between
empirical $Z$-maps and predicted $Z$-maps via anatomical alignment (Fisher
z-transformed, t(14)= 5.7754451, $p$=0.0000643).
%
Whereas the functional alignment via the first movie segment (451 \acp{tr})
yielded significantly higher correlation than a functional alignment via four
localizer runs lasting 624 \acp{tr} (Fisher z-transformed, t(14)=6.8532349,
$p$<0.0000116),
%
the functional alignment via the first segment of the audio-description yielded
no significantly different correlations than the functional alignment via four
localizer runs (Fisher z-transformed, t(14)=-1.8674144, $p$<0.084551).
%
The mean correlation between empirical $Z$-maps and predicted $Z$-maps via the
first segment alone of the movie were significantly lower than the correlations
between empirical $Z$-maps and predicted $Z$-maps predicted $Z$-maps via the
first two movie segments (Fisher z-transformed, t(14)= -3.7454592,
$p$=0.0024485).
%
The prediction performance based on data of the audio-description increases with
more data. However, the "relevant" test compared "similar" amount of \acp{tr}:
%
The difference between mean correlations based on two movie segments in
comparison the mean correlation based on three movie segments was [not]
different [when adjusted alpha-level used] (Fisher z-transformed, t(14)=
-2.5759899, $p$=0.02303).

\todo[inline]{possibly run test that compares x movie segments for alignment to
Cronbach's alpha of movie's empirical $Z$-map? a.k.a functional alignment will
"sooner or later be better" than Cronbach.}


\textbf{In case of estimating the $Z$-maps of the audio-description,...}

\todo[inline]{This part is the ugly one; all samples will probably reject H0
assuming normality}

%
...correlations between empirical $Z$-maps and predicted $Z$-maps via the first
movie segment were significantly lower than the correlations between empirical
$Z$-maps and predicted $Z$-maps via anatomical alignment (Fisher z-transformed,
t(14)= -4.2004329, $p$=0.0010387, Bonferroni corrected).

\todo[inline]{test a differences compared to Cronbach's a? e.g.,
audio-description for alignment will sooner or later be significantly higher
than Cronbach's; might be a cue that SRM does a denoising of the e.g., map (and
does not estimate "reliable patterns deviant from the norm" wrongly)}

\todo[inline]{the three outliers are always sub-02, sub-05, sub-20 (always=
anatomical alignment, 4 runs of localizer, 8 runs of movie, 8 runs of
audio-description)}





\pagebreak


\section{Discussion}

\todo[inline]{generally, I think it's best to abstract A LOT from the specific
results (other studies discuss general stuff only, too; fucking joke, anyway)}

\todo[inline]{Therefor, two possibilities: a) write general shit \& give some
concrete examples (a.k.a. statistical tests) to make the point, or b) discuss
concrete examples (a.k.a. the tests) \& draw the general inference (how it is
done now)}

\todo[inline]{Problem: \citet{feilong2022individualized} assess data quantity,
too; results suggest 30m are "good" in case of their model.}

\todo[inline]{point for the discussion: what makes the prediction of the
localizer based on localizer runs for alignment so "bad"? Is it our model or the
transformation matrices? In any case, our results are different from
\citep{haxby2011common}: CFS \& transformation matrices based on controlled
paradigms were as good as / better than CFS \& transformation matrices based on
movie when estimating results of controlled paradigm; if it is discussed
(lengthy), then add findings from \citet{haxby2011common} that compare (within-
and across-paradigm) prediction based on naturalistic \ac{cfs} (more TRs) to
non-naturalistic \ac{cfs} (less TRs) here; if this is mentioned here, then also
define  within- and across-paradigm prediction here already and not below in
"Here, we..."}

\todo[inline]{point for the discussion: the highest correlations between
individual regressors \& shared responses can be seen in the visual localizer
(the model cannot be that bad for the localizer TRs?) => but prediction via
localizer runs suck when localizer results are predicted: interpretation?}

\todo[inline]{point for the discussion:  when audio PPA is estimated, the lowest
correlation between predicted and empirical $Z$-maps are in those participants
that have a poor Cronbach's (and no significant cluster for the audio PPA in
\citet{haeusler2022processing}); imo, they not "reliable outliers" being
different from the norm but probably just noisy asses; hence, SRM should
probably do more of a denoising as opposed to modeling (reliable) outliers
incorrectly}

\todo[inline]{even if the \ac{glm} model is freaking awesome, does a
naturalistic stimulus provide enough events sufficiently balanced across
segments when $t$-contrasts for whatever functions are supposed to be modeled?
Naturalistic stimuli are not the panacea...!}


\subsection{Short summary of aims \& hypotheses, method, results}

\todo[inline]{if necessary, write similar text at the beginning of results}

\todo[inline]{Try to start with aims \& goals before reviewing methods}

\todo[inline]{in general, it's probably (still) too long}


\paragraph{Methods}
%
In the present study, we estimated the results of $t$-contrasts that were
created in previous studies \citep{sengupta2016extension,
haeusler2022processing} in order to localize the \ac{ppa}.
%
Following a leave-one-subject-out cross-validation, the $t$-contrasts' empirical
$Z$-maps serving as the to be predicted criteria were estimated using an
anatomical alignment approach \citep[cf.][]{zhen2015quantifying,
zhen2017quantifying} and a functional alignment approach.
%
In case of the functional alignment approach, we fit a shared response model
\citep{chen2015reduced} to the training subjects' concatenated response time
series of three different paradigms in order to acquire a \ac{cfs} and the
training subjects' subject-specific transformation matrices.
%
In order to acquire the test subject's transformation matrix, we used an
increasing amount of the test subject's response time series of the three
paradigms separately as predictors by functionally aligning the test subject
to the corresponding \acp{tr} of the shared feature space (i.e. \ac{cfs}).
%
The empirical $Z$-maps of each training subject were projected from their
respective voxel space through the \ac{cfs} into the test subject's voxel space
to yield the test subject's predicted $Z$-maps.
%
In case of the anatomical alignment approach, the training subjects' $Z$-maps
were projected via nonlinear, volume-based transformation through the MNI152
brain atlas \todo{kind of incorrect} into the test subject's voxel space.

%
The estimation performance of the alignment approaches was quantified by
correlating the empirical $Z$-maps with their respective predicted $Z$-map
(which can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}).


\paragraph{Three aspects under investigation}

\todo[inline]{strictly speaking, with comparison to anatomical alignment it's
four aspects}

\todo[inline]{biggest issue for discussion: how to separate validity /
generalizability of \ac{cfs} from validity / generalizability of transformation
matrices? If I do not find a solution for that the intro should be adjusted (to,
maybe, just two points?)}

\todo[inline]{the highest correlations between individual regressors and shared
responses can be seen in the visual localizer => but the matrices based on
localizer runs suck the most for estimation; interpretation?}

\todo[inline]{in any case: within-paradigm prediction is more about validity of
model (and matrices?); cross-paradigm prediction is more generalizability of
(model and) matrices}

%
Zero, we compare anatomical to functional alignment.
%
First, we assess the \textbf{validity of the \ac{cfs}} by estimating $Z$-maps
from the analysis (i.e. $t$-contrast) of same paradigms used to align the test
subject (cross-subject-\textbf{within-paradigm} prediction).
%
Second, we assess the \textbf{generalizability of the transformation matrices}
by estimating $Z$-maps from the analysis (i.e. $t$-contrast) of paradigms that
were not used to align the test subject (cross-subject-\textbf{cross-paradigm}
prediction).
%
Third, we explore the relationship between the estimation performance of the
t-contrast's results and the quantity of data from each of the three paradigms
used to functionally align the subject with the \ac{cfs}.


\paragraph{Results}

\todo[inline]{if a high-level summary of results is supposed to be written here
(at the moment I think it does not make a lot of sense), the summary heavily
depends on the "final" phrasing of the discussion}

\todo[inline]{Maybe, it's simply better to write a stupid high-level discussion
and ---to make the point--- cherry-pick some statistical tests as concrete
examples}

Results suggest that "partial alignment" works for multi-paradigm \ac{cfs}
derived from concatenated time series of multiple paradigms.
%
For all criteria, the prediction performance improves continuously with more
data of the paradigms used as predictors.


\subsection{Functional alignment vs. anatomical alignment}
%
When the visual paradigms (i.e. localizer and movie PPA) are estimated, the
movie beats anatomical alignment with just one movie segment (which is
statistically tested).
%
Movie gets (sooner or later) as good as anatomical alignment when audio PPA is
estimated (but is worse when just one movie segment is used).

%
Localizer runs suck compared to anatomical alignment when used to estimate
localizer $Z$-map (within-paradigm prediction) and movie PPA (cross-paradigm
prediction).
%
Surprisingly, when audio PPA is estimated (cross-paradigm), localizer for
alignment gets close to anatomical alignment; i.e. audio PPA is the criterion
the visual localizer can estimate the best.\todo{but why???}

%
Audio-description sucks---compared to anatomical alignment---for cross-paradigm
estimation.
%
Though, it gets better when more / all segments are used for alignment:
%
For within-paradigm prediction, audio-description eventually outperforms
anatomical alignment (when more than 4-6 segments are used)
%
Generally, the more data of the audio-description is used, the narrower the gap
between movie and audio-description gets.

%
For every alignment procedure (functional as well as anatomical), the outliers
of Cronbach's of audio PPA are difficult to estimate.


\paragraph{Summary \& conclusion on anatomical vs. functional alignment}

\todo[inline]{Results suggest...? Interpretation}

Anatomical alignment is actually not that bad.
%
It's pretty consistent across paradigms, too (always a median correlation of
about 0.6).



\subsection{Functional alignment across predictors \& criteria}

\todo[inline]{at the moment, I think it's best to order by paradigm used as
predictor; for every predictor, discuss each criterion and the quantity of
data}

\todo[inline]{problem: \citet{feilong2022individualized} already assessed data
quantity; results suggest 30m are "good" in case of their model; plus, they
model individual component.}

The multi-paradigm model is kind of valid, I guess...






\subsubsection{Validity \& generalizability of movie}

\todo[inline]{a.k.a. movie is best (which is in line with previous research)}

\paragraph{criterion: localizer}

\paragraph{criterion: movie PPA}

%
It's not a dynamic localizer employing video snippets of landscapes and faces
but a "real naturalistic localizer".

\paragraph{criterion: audio PPA}

\paragraph{general stuff about quantity of data}

%
Results indicate that $\approx$15 minutes (\ac{tr}=2s) of movie watching used
for functional alignment outperform prediction using anatomical alignment.
%
Prediction performance further increases when $\approx$30 minutes of movie data
submitted to the algorithm to calculate the subject-specific transformation
matrices.
%
However, three segments ($\approx$45 minutes) do not lead to an significantly
increased estimation performance suggesting a decreasing benefit of longer
scanning time than 30 minutes during audio-visual naturalistic stimulation.



\subsubsection{Validity \& generalizability of visual localizer}

\todo[inline]{a.k.a. why is it so "bad"???}

\todo[inline]{We have way more \acp{tr} in case of naturalistic stimuli; however
not necessarily equal number of events; and events are not "clean" events but
heavily confounded by other stuff}

\paragraph{criterion: localizer}

%
Surprisingly, prediction of localizer via localizer runs (i.e. within-experiment
prediction) is worse than anatomical alignment.
%
How does that come? At least the correlations of regressors with shared
responses show the "clearest" correlations of a regressor with a shared response
(which is not the case in the \acp{tr} of the movie or audio-description).
%
Maybe, our naturalistic stimuli (about 7000 TRs) fucked up our shared responses
in the 450 \acp{tr} within the SRM (imo, Fig.~\ref{fig:corr-vis-reg-srm}
suggests otherwise)


\paragraph{criterion: movie PPA}

\paragraph{criterion: audio PPA}

\paragraph{general stuff about quantity of data}




\subsubsection{Validity \& generalizability of audio-description}

\todo[inline]{a.k.a. it's totally different from different previous studies!}

\todo[inline]{here it is about audio-description as predictor!}

\todo[inline]{below is a paragraph about "audio PPA as criterion"; separate
points more clearly or somehow merge if the two topics have too many
intersecting points}


\paragraph{criterion: localizer}
%
A daring cross-modal prediction.
%
Kind of works, but you need a shit ton of data, i.e. it gets better, the more
data are available


\paragraph{criterion: movie PPA}
%
Another daring cross-modal prediction (similar pattern, eventually outperforms
visual localizer for alignment and gap between audio-description and movie for
alignment gets narrower the more data are used).

\paragraph{criterion: audio PPA}
%
Even with just one segment it outperforms (slightly) the functional alignment
using localizer runs or movie segments (not statistically tested though).
%
Eventually, audio-description based functional alignment outperforms anatomical
alignment.


\paragraph{general stuff about quantity of data}
%
It does not look like "garbage in, garbage out".

\paragraph{Inference}

\todo[inline]{yeah, what does that mean?}

%
Audio-description is lacking visual stimulation, audio-description is
"not-as-rich" as the movie.
%
Additionally, maybe, the auditory response in PPA is (too) different from visual
response?
%
Still, results show that data collected during listening to an audio-description
[which is richer than a mere narrative] could, in principle, be used to estimate
a visual category-selective area [but impractical amounts of data with current
approach].

%
Interesting would be estimation of results from a controlled speech
paradigm, i.e. another same-modality criterion.


\subsection{Interim summary: Validity \& generalizability of multi-stimulus
model and validity \& generalizability of matrices}

\todo[inline]{how to separate validity \& generalizability of \ac{cfs} from
validity \& generalizability of matrices?}

%
Our results provide evidence that transformation matrices calculated based on
data from naturalistic stimuli promise an increased validity and [or?]
generalizability for functional alignment over transformation matrices based on
data of a controlled paradigm based on simplified stimuli.



%
This is the case for both within-paradigm prediction (e.g., audio-description
for alignment to estimate $Z$-maps from the audio-description's $t$-contrast)
and cross-paradigm prediction (e.g. audio-description for alignment to estimate
$Z$-maps from the visual localizer's $t$-contrast).

%
One segment of audio-description is not statistically different from four
runs of localizer to estimate the localizer (after Bonferroni correction;
p=0.03).

\todo[inline]{probably discuss \citet{haxby2011common} here; cf. templates at
the very end of this chapter}

\todo[inline]{if \citet{haxby2011common} is discussed (a lot), it should
probably be primed before "Here, we..." in the introduction}

%
Our results are different results of \citet{haxby2011common} who found that the
prediction of $Z$-maps from a controlled paradigm via matrices \& \ac{cfs} based
on the same controlled paradigm was as good or better than prediction (of the
same) $Z$-maps via matrices \& \ac{cfs} based on movie data.

%
In summary, the multi-stimulus \ac{cfs} generalizes over paradigms
to be estimated but performance depends on the paradigm used to align the test
subject to the \ac{cfs}.



\subsection{More specific stuff}

\subsubsection{Localizer as criterion: "ground truth"?}

\todo[inline]{imo, this is more a topic for the general discussion}

\todo[inline]{cf. general discussion: pros \& cons of naturalistic stimuli}


\subsubsection{audio PPA as criterion: the issue of reliability}

\todo[inline]{also true (but less severe) in case of movie PPA}

\todo[inline]{topic for general discussion, too: pros \& cons of naturalistic}

\todo[inline]{the "deviant" participants in the audio-description are the ones
that have a poor Cronbach's; imo, they're not "reliable outliers" being
different from the norm but noisy asses}

\todo[inline]{but potential point to make: results could be interpreted as the
SRM doing some denoising (as opposed to modeling reliable outliers incorrectly)}

Results of \citet{haeusler2022processing} could be influenced by paradigm and
methodological choices.


\paragraph{Issues of methodological decisions}

\todo[inline]{a.k.a. operationalization \& construct validity}

%
\citet{haeusler2022processing} might have modeled the time course of "spatial
responses" "insufficiently".
%
However, the primary audio-description contrast in
\citet{haeusler2022processing} ``yielded bilateral clusters in nine participants
that are within or overlapping with the block-design localizer results.
%
For another participant (sub-09) the analysis yielded one cluster in the
left-hemispheric PPA'' \citep{haeusler2022processing}.
%
Hence, we should have modeled the construct "correctly" for >50\% of subjects in
\citet{haeusler2022processing}.


Is the number of events across segments too unbalanced?
%
Here again, the dead-end argument is that it worked for most subjects.


\paragraph{Issues of the paradigm}
%
Results of \citet{haeusler2022processing} suggest that the audio-description
samples the responses to (at least auditory) spatial information.
%
Moreover, the plots of shared responses \& AO regressors (cf.
Fig.~\ref{fig:corr-ao-reg-srm}) suggest that the shared responses during
\acp{tr} of the audio-description are not total garbage.
%
Shared responses are correlated with specific regressors of the visual
localizer, whereas shared features during \acp{tr} of the movie /
audio-description seem to be more abstract (or the regressors simply suck).
%
Finally, the within-paradigm prediction of audio PPA works for most subjects;
%
which also suggests that the Cronbach's-a-outliers do not degrade the model too
much.


\todo[inline]{No, I am not eager to look at the outliers' first-level
$Z$-maps in detail...}

%
So, maybe, the audio-description gave too much room for variations across the
time of the paradigm in some subjects?
%
It might be an issue auditory domain in general (whatever that is supposed to
mean)?
%
It might be the case that a 2h-long auditory stimulation is not as immersive /
engaging as the flashy multi-modal audio-visual movie?
%
Problem to control: the attentional focus is harder to judge / control (compared
to eye-tracking during movie);
%
alertness (via EEG) should be possible though.



\paragraph{Inference}

\todo[inline]{add text about SRM as denoising technique?}
%
But still, current results suggest that the audio-description engages the
process of auditory spatial information reliable across subjects in such a way
that the ``SRM will improve sensitivity for detecting a cognitive process of
interest in the test data'' \citep{cohen2017computational}.


\todo[inline]{Following are pretty bold statements}
%
Present results support evidence that results in \citet{haeusler2022processing}
that are restricted to the anterior part of the localizer PPA are not based on
the methodological decisions.
%
Further, present results add evidence that that the responses to auditory
spatial information lead to different activity patterns than visual stimulation
[cf. interpretation in \citet{haeusler2022processing}].
%
We need---of course!---further studies that use controlled paradigms to
investigate auditory spatial information, and studies that use auditory
narratives and employ more sophisticated models of event structure in order to
assess the suitability of naturalistic stimuli as "true naturalistic localizer".



\subsubsection{audio PPA as criterion: the issue of individual differences}

\todo[inline]{imo, this topic can be dropped because the differences are not
reliable}



\subsection{Vision: calibration scan + database}

% A shared calibration scan across datasets could be used to transfer data
% between datasets, a procedure that is easier to accomplish than shared
% subjects across datasets \citep[cf.][an extension of the \ac{srm} for shared
% subjects across datasets]{zhang2018transfer}.

\todo[inline]{text needs to match with general discussion's "Vision"}

\todo[inline]{focus on what we did in the present study}

\todo[inline]{especially, "clinical context" belongs to general discussion}



\subsubsection{Intro}

\todo[inline]{repeat phrasing from introduction (cf. localizer paradigms)}

\todo[inline]{do not open pandora's box of "functional atlas"!!}

%
Our results suggest that additional 15 minutes functional scanning using an
engaging naturalistic stimuli could provide sufficient data for a
\textit{calibration scan}.
%
A standardized calibration scan could be used to align a new subject to a
\ac{cfs} that was derived from extensive scans of a reference group.
%
Extensive scans of the reference group would comprise data collected from
naturalistic paradigms but also controlled paradigms considered to be the ``gold
standard'' to probe low-level auditory or visual perception, or higher cognition
like theory of mind \citep{spunt2014validating} or semantic processes
\citep{fedorenko2010new, fernandez2001language}.
%
The diagnostic run would be based on a multifaceted naturalistic stimulus that
samples a broad range of brain states in order to allow a valid alignment to the
reference \ac{cfs}.
%
Compared to a diagnostic run based on a controlled paradigm, a naturalistic
stimulus would have the additional benefit of higher engagement and better
compliance \citep{vanderwal2015inscapes, eickhoff2020towards} in, e.g., children
or a patients.


\paragraph{application: estimate \& quantify regular vs. deviant pattern}

%
Once a new subjects is aligned to the \ac{cfs}, functional patterns collected in
a reference group could be mapped through the \ac{cfs} into the new subject's
voxel space.
%
On the one hand, this would allow to estimate regular patterns in a new subject
when additional functional scans are not possible to scanner availability,
time-limitations or monetary constrains, or compliance issues.
%
On the other hand, this would allow to quantify the similarity (or difference)
of a new subject's actual pattern (i.e. a empirical $Z$-map) to a pattern
estimated from a healthy or clinical reference group.


\paragraph{Examples for clinical populations}

%
Patient populations, such as patients are blind
%
[Mahon et al. 2009; He et al., 2013; Striem-Amit et al. 2012b; van den Hurk et
al. 2017, Wolbers et al., 2011],
%

\paragraph{Example: \citet{yates2021emergence} quantify difference}

\todo[inline]{This is essentially the abstract of \citet{yates2021emergence}}

For example, \citet{yates2021emergence} tested ``the presence and localization
of adult functions in children using shared response modeling.
%
The feature space was learned from fMRI activity of adults watched a movie.
%
The shared features were then translated into the anatomical brain space of
children 3--12 years old.
%
The found reliable correlations between reconstructed activity and children's
actual fMRI activity as they watched the same movie.
%
The strength of the correlation in the precuneus, inferior frontal gyrus, and
lateral occipital cortex predicted chronological age''
\citep{yates2021emergence}.



\subsection{Shortcomings \& future questions}

\subsubsection{Bigger sample size}
%
The SRM is computationally less demanding [= "computationally more efficient"?]
than hyperalignment, an advantage for scientists who want to replicate our
results but do not have access to a high-performance computer cluster [or
"high-throughput computer cluster" or simply "specialized hardware"?].
%
Moreover, it should scale pretty well (compared that to
\citep{jiahui2020predicting, jiahui2022cross} 1-step alignment that needs
pair-wise matrices for subjects 'cause no \ac{cfs}; similarly,
\citep{busch2021hybrid})\todo{check Busch}.


\paragraph{...because our sample size was small with following effect}

\todo[inline]{topic might be dropped}

% what is the case
The correlations of shared responses within the \acp{cfs} created from $N-1$
training subjects varied across the folds of the cross-validation.
% interpretation
That means, a change of 1/13 of the data for every subject's analysis is causing
the estimates to vary [how much?].
% conclusion
Future studies, should create a \ac{cfs} based on data from more subjects and
investigate the relationship between number of participants, variability of
parameters, and estimation performance.



\subsubsection{Other ROIs than category-selective areas}

\todo[inline]{a.k.a. explore estimation performance in case of other functions}

\todo[inline]{don't write too much here}

\todo[inline]{more a topic for general discussion: naturalistic stimuli
might be limited (e.g., might not sample executive functions sufficiently)}

%
In the present study, we focused on the \ac{ppa} as an classic example of a
higher-visual area.

%
However, studies have shown a varying degree of \textit{functional--anatomical
correspondence} between a brain function and its underlying anatomical location.
%
Previous studies that used either volume-based \citep{zhen2017quantifying,
zhen2015quantifying} or surface-based alignment \citep{rosenke2021probabilistic,
frost2012measuring} in order to estimate the most probable location of
functional area in a test subject have ``found a large variability in the degree
to which functional areas respect macro-anatomical boundaries across the
cortex'' \citep{frost2012measuring}.
%
``There is a strong structural-functional correspondence in some areas whilst in
others the spatial location of the functional area varies greatly across
subjects within a cortical area'' \citep{frost2012measuring}.

%
For example, in domain of category-selective areas, the interindividual
variability varies across functional areas \citep{zhen2017quantifying,
zhen2015quantifying, frost2012measuring}:
%
``Scene-selective regions showed larger interindividual variability [after
nonlinear volume-based alignment] than the face-selective regions in spatial
topography'' \citep{zhen2017quantifying}.




\paragraph{Studyforrest dataset's other localizer t-contrasts}

\todo[inline]{does it make sense to discuss that? it's such a low-hanging fruit}

\todo[inline]{if it makes sense to be discussed, discuss it here and not in the
general discussion}

%
The studyforrest dataset's visual localizer \citep{sengupta2016extension} offers
other contrasts (and thus \acp{roi} masks) aimed at localizing the \ac{ffa} and
\ac{ofa} that are associated with face perception \citep{kanwisher1997ffa,
pitcher2011occipitalfacearea}, the \ac{eba} that is associated with the
perception of human bodies \citep{downing2001bodyarea}, and the \ac{loc} that is
associated with the perception of (small) objects (like tools or toys)
\citep{malach1995loc}.

Quickly shifted from general discussion to here:
%
The visual localizer performed by \citet{sengupta2016extension} employed images
from six categories (houses, landscapes, faces, bodies without heads, small
objects, and scrambled images).
%
As a results, the corresponding dataset provides subject-specific \acp{roi}
masks for higher visual areas besides the \ac{ppa}:
%
the fusiform face area (FFA) \citep{kanwisher1997ffa} and the occipital face
area (OFA) \citep{pitcher2011occipitalfacearea},
%
the extrastriate body area (EBA) \citep{downing2001bodyarea},
%
and the lateral occipital complex (LOC) \citep{malach1995loc}.
%
Future studies (e.g., a master's thesis or part of a PhD project) could adjust
our extension of the annotation of speech created in study 2 and the
corresponding analysis pipeline in order to explore hemodynamic responses
correlating with auditory information related to faces, body parts or small
objects.


\paragraph{Examples}

\todo[inline]{shorten this part}

% Zhen 2017
``Scene-selective regions showed larger interindividual variability [after
nonlinear volume-based alignment] than the face-selective regions in both
spatial topography and functional selectivity'' \citet{zhen2017quantifying}.


% category-specific areas
Similar to nonlinear volume-based alignment, similarity across person is higher
after surface-based alignment ``for retinotopically defined regions, with
character-selective regions showing the lowest consistency for both alignments,
closely followed by mFus- and IOG-faces'' \citep{rosenke2021probabilistic}.

%
``We localized 13 widely studied functional areas and found a large variability
in the degree to which functional areas respect macro-anatomical boundaries
across the cortex'' \citep{frost2012measuring}.
%
``The percent gain in overlap [after surface-based alignment] differed greatly
across the different functional regions throughout the cortex''
\citep{frost2012measuring}.
%
``There is a strong structural-functional correspondence in some areas whilst in
others the spatial location of the functional area is not tightly bound to
anatomical landmarks and varies greatly across subjects within a cortical area''
\citep{frost2012measuring}.
%
``Language areas were found to vary greatly across subjects whilst a high degree
of overlap was observed in sensory and motor areas'' \citep{frost2012measuring}.

%
``Area LOC also showed increased overlap after CBA with a 62.7\% gain in the
left hemisphere and 38.4\% on the right'' \citep{frost2012measuring}.
%
Finally PPA exhibit more gain in the right hemisphere with 27.7\% gain, than on
the left with 17.6\%'' \citep{frost2012measuring}.
%
``FFA varies in its location along the length of the fusiform gyrus even though
the gyri themselves are well aligned across subjects''
%
The FFA did not exhibit the same strong structural-functional correspondence and
saw more modest increases in overlap after macro-anatomical alignment with
44.1\% and 12\% gain for the left and right hemispheres''
\citep{frost2012measuring}.



\paragraph{More severe: language areas}

\todo[inline]{If this is mentioned at all, write half a sentence about speech
lateralization; problem: even in case whole-cortex alignment algo is used,
speech areas probably vary too much (search lights a probably too small in case
of atypical speech topographies)}

%
More severe: ``Language areas were found to vary greatly across subjects whilst
a high degree of overlap was observed in sensory and motor areas''
\citep{frost2012measuring}.


\paragraph{Inference}

\todo[inline]{Mention whole-cortex (searchlight) alignment here already?
...instead of writing a dedicated treatise about "Other functional alignment
algorithms" below?}

\todo[inline]{maybe, cite studies that did research on other brain functions
using naturalistic stimuli; or just cite a good review}

%
Hence, a new dataset should not just have more subjects but also more
localizers.
%
Then, future studies can investigate other functional domains.
%
Naturalistic stimuli have been used to investigate a variety of domains like XYZ
[check for, e.g., visual or auditory perception, spatial cognition; emotion;
music, speech or social perception], and possibly allow valid mapping of
functional data in all of these domains.


\subsubsection{\ac{srm} is problematic in case of atypical organization /
outliers}

\todo[inline]{Check \citet{feilong2022individualized, jiahui2022cross,
turek2018capturing}; at least two of them have modeled an individual component.}

%
A drawback of the SRM algorithm we employed here is that it models responses
that are common across persons without an individual component.
%
A person's idiosyncratic responses are excluded from the shared response model
but ``are not necessarily noise and may in fact be highly reliable within
participants'' \citep{cohen2017computational}.
%
In order to predict a reliably atypical pattern (and not just quantify the
deviation from norm), you need matching \ac{cfs} or---probably better---model
has to have a shared response + individual component + noise.
%
Assess performance of whatever functional alignment algo to [directly] estimate
[reliable] outliers in a sample.
%
Do you estimate the regular pattern (which allows you do quantify the difference
of a deviant actual pattern to a norm) or do you directly estimate the deviant
pattern from regular reference? Hm...

%
Imo, present results suggest estimation of regular pattern (a.k.a. denoising).


\paragraph{Templates that can probably be dropped}
%
``SRM can be used to isolate participant-unique responses by examining the
residuals after removing shared group responses, or it can be applied
hierarchically to the residuals to identify subgroups [\citet{chen2017shared}]
'' \citep{cohen2017computational}.
%
``In cases where each subject's unique response is of more interest than the
shared signal, SRM can be used to factor out the shared component thereby
isolating the idiosyncratic response for each subject
[\citep{chen2015reduced}]'' \citep{kumar2020brainiak}.
%
``Recognizing that signal exists beyond the average or shared response of a
group, such studies exploit idiosyncratic but stable responses to account for
previously unexplained variance in brain function, behavioral performance and
clinical measures [e.g., Finn (2015). Functional fingerprinting (based on
connectivity)]'' \citep{cohen2017computational}.




\subsubsection{Other functional alignment algorithms}

\todo[inline]{In general, a big treatise of pro \& cons of different algorithms
needs to be avoided}

\todo[inline]{Evtl. kann man alle Punkte dieser Subsubsection bereits oben
verwursten; hier kommen die Punkte eh so ein wenig zusammenhangslos als
Selbstzweck und ziehen allgemeine ganz am Ende die Ergebnisse runter (``a.k.a.
"warum hast du nicht gleich Algo XY benutzt?'')}


\subsubsection{Volume- vs surface-based}

\todo[inline]{An "issue" for reviewers but, imo, it's not a real issues}

\todo[inline]{$Z$-maps of localizer were calculated in voxel space; cf. general
discussion (opportunity costs)}

\todo[inline]{we are nearer on the raw data (less error accumulation?) 'cause we
work with voxels (surface vertices need additional mapping of $Z$-maps
calculated from smoothed data), and both our anatomical and functional alignment
use voxels as input (i.e. it is not mixed)}

We compare volume-based [nonlinear] anatomical alignment to volume-based
functional alignment.
%
Future work could compare surface-based alignment that respects cortical folding
structure -- that out-performs predictions based on [affine] volume-based
anatomical alignment \citep{weiner2018defining} -- to surface-based functional
alignment.


\subsubsection{ROI vs. whole-brain (i.e. searchlight)}

\todo[inline]{Maybe, shift that to "future studies on other ROIs"}

\todo[inline]{searchlight SRM \citep{zhang2016searchlight}}

\todo[inline]{negative: more parameters to vary and to assess}

``Searchlight functional alignment [\citep{zhang2016searchlight,
guntupalli2016model}] learns local transformations and aggregates them into a
single large-scale alignment.
%
The searchlight scheme [Kriegeskorte, 2006, Information-based functional brain
mapping], popular in brain imaging [Guntupalli et al., 2018; 2016], has been
used as a way to divide the cortex into small overlapping spheres of a field
radius.
%
This method allows researchers to remain agnostic as to the location of
functional or anatomical boundaries, such as those suggested by
parcellation-based approaches.
%
A local transform can then be learned in each sphere and the full alignment is
obtained by aggregating (e.g. summing as in \citep{guntupalli2016model} or
averaging) across overlapping transforms.
%
The aggregated transformation produced is no longer guaranteed to bear the type
of regularity (e.g orthogonality, isometry, or diffeomorphicity enforced during
the local neighborhood fit)'' \citep{bazeille2021empirical}.
%
``In the case of searchlight Procrustes, we selected searchlight parameters to
match those used in Guntupalli et al. (2016):
%
each searchlight had 5 voxel radius, with a 3 voxel distance between searchlight
centers'' \citep{bazeille2021empirical}.


\subsubsection{Time series vs connectivity-based}

\todo[inline]{kind of a killer cause you do not need intersection of
time series}

\todo[inline]{avoid a treatise by stating in the intro that response based
aligns responses better, connectivity-based aligns connectivity better}

\todo[inline]{what did \citep{nastase2019leveraging} do? As of now, I do not
care anymore tbh}

\todo[inline]{\citet{jiahui2022cross} do connectivity-based 1-step
hyperalignment across different movie datasets which is as good as response
hyperalignment; however, it's a shitty procedure to scale because the need a
transformation matrix for every pair of subjects (i.e no \ac{cfs})}


%
``Response-based hyperalignment (RHA) mapped data from the anatomical space to a
common information space based on time-point response patterns across cortical
vertices.
%
Connectivity-based hyperalignment (CHA) mapped data from the
anatomical space to a separate common information space based on functional
connectivity patterns derived from the movie response data''
\citep{busch2021hybrid}.

``Response-based hyperalignment was shown to align response-based data better
than connectivity-based hyperalignment, whereas connectivity-based
hyperalignment was shown to better align connectivity-based data than
response-based hyperalignment [Guntupalli et al., 2018]''
\citep{busch2021hybrid}.
%
``Response-based common spaces better align held-out response data, whereas
connectivity-based common spaces better align held-out connectivity data.
[Guntupalli et al., 2018] \citep{busch2021hybrid}.

%
``Both connectivity hyperalignment and response hyperalignment increased ISCs
and bsMVPC classification accuracies significantly over anatomy-based alignment,
but each algorithm achieves better alignment for the information that it uses to
derive a common model, namely connectivity profiles and patterns of response,
respectively'' \citep{guntupalli2018computational}.


%
In other words, RHA outperforms CHA on response-based metrics of alignment,
whereas CHA outperforms RHA on connectivity-based metrics'' ``Hyperalignment
projects cortical pattern vectors into a common, high-dimensional information
space \citep{haxby2020hyperalignment}.

%
Derivation of this common space can be based on either neural response profiles
(e.g. data collected during tasks, such as movie viewing (Haxby et al., 2011))
or functional connectivity profiles files \citep{guntupalli2018computational}''
\citep{busch2021hybrid}.

%
``The number of voxels that can be considered simultaneously for functional BOLD
response time series alignment is limited by the number of timepoints in the
calibration scan (about 300-400 voxels for a 15min scan with a 2s TR,
corresponding to a local cortical neighborhood of about 1cm in diameter for a
standard resolution).
%
This limitation does not exist in this form for a functional alignment that is
based on connectivity vectors.
%
The length of these connectivity vectors is determined by the number of
reference (or seed) regions in the brain'' [project proposal].

% Kumar on Nastase's ugly mofo paper
``Estimating the SRM from functional connectivity data rather than response time
series circumvents the need for a single shared stimulus across subjects.
%
Connectivity SRM allows us to derive a single shared response space across
different stimuli with a shared connectivity profile
\citep{nastase2019leveraging}'' \citep{kumar2020brainiak}.
%
``The sampling of connectivity vector space is defined by the selection of
connectivity targets, but the richness and reliability of connectivity estimates
depends on the variety of brain states over which connectivity is estimated''
\citep{haxby2020hyperalignment}.




\subsection{Conclusion}

God bless America!


\section{Data Availability}

\todo[inline]{all from PPA-Paper but with new GIN link leading to an empty repo}

% \href{https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-analysis}}

% new; PPA analysis
All fMRI data and results are available as Datalad \citep{halchenko2021datalad}
datasets, published to or linked from the \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).
% original
Raw data of the audio-description, movie and visual localizer were originally
published on the \emph{OpenfMRI} portal
(\url{https://legacy.openfmri.org/dataset/ds000113}; \citep{Hanke2014ds000113},
\space \url{https://legacy.openfmri.org/dataset/ds000113d};
\citep{hanke2016ds000113d}).
% visual localizer
Results from the localization of higher visual areas are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-visualrois}{\url{github.com/psychoinformatics-de/studyforrest-data-visualrois}}).
% raw data
The realigned participant-specific time series that were used in the current
analyses were derived from the raw data releases and are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned}{\url{github.com/psychoinformatics-de/studyforrest-data-aligned}}).
% OpenNeuro
The same data are available in a modified and merged form on OpenNeuro at
\url{https://openneuro.org/datasets/ds000113}.
% NeuroVault for z-maps of SRM
Unthresholded $Z$-maps of all contrasts can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}.


\section*{Code Availability}

Scripts to generate the results as Datalad \citep{halchenko2021datalad} datasets
are available in a \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).






\pagebreak

\section{Backup of texts regarding "done but not mentioned"}


\subsection{Calculate $Z$-maps mean in the common space already}
%
I also tested averaging $Z$-maps in the \ac{cfs} (i.e.: not in the test
subject's voxel space): similar results
%
In case of anatomical alignment, I did
not test averaging data in MNI152 space.


\subsection{Calculate $Z$-map from training subjects' TRs in FEAT}

iirc, I projected all subjects' localizer time series through
model space into test subject voxel space; then, calculated the contrast
with these data (s. scripts 'test/data\_denoise-vis.py' \&
'test/data\_srm-vis-to-ind.py').
%

The problem was: if one wants to test the different transformation matrices (I
only did it with one; imo, based on alignment using the whole audio-description)
it gets totally messy \& computational intensive.
%
Results were similar to the original procedure if not slightly worse.


\subsection{Leakage of test data in union of individual \acp{ppa}}

%
We used the union of individual \acp{ppa} as spatial constrain for $Z$-maps.
%
But we have a leakage of test data (test subject is in data for the mask).
%
We might miss some voxels (of some participants) at the borders of the \ac{roi},
because the subject-specific, binary masks are based on a ("titrated")
threshold.  \citep{sengupta2016extension}
%
In the future, an independent probabilistic atlas should be used, the \ac{roi}
dilated, [and a separate model calculated for each hemisphere].
