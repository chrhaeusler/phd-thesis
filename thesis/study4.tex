\todo[inline]{add stuff about "auditory PPA" (yes, I am in trouble)}

\todo[inline]{speaking of ``topography'' when it is just about one functional
area is a little overstatement?}

\todo[inline]{why did we use SRM? cf. evaluation of algos in
\citet{chen2015reduced, bazeille2021empirical}; well, it's ``nicely''
implemented in BrainIAK (cf. general intro on reproducibility) and, despite not
using the fastSRM algo, it's probably still faster than hyperalignment?}

\todo[inline]{probably faster, computationally less demanding than
hyperalignment; ``computational efficiency, as the latter is an important
consideration for scientists who may not have access to specialized hardware''
\citep{bazeille2021empirical}}

\todo[inline]{mih: Dieses Kapitel sollte konzeptuell und inhaltlich deutlich als
"und jetzt alles zusammen" erscheinen}


\section{Introduction}

\todo[inline]{intro is pretty similar to general introduction}

% brain mapping
``Mapping the functional neuroanatomy of the human brain is one of the
fundamental goals of neuroscience research. Lesion studies following stroke
provided critical early information that specific functions such as motor
function or language were consistently localized to certain parts of the
cerebral cortex'' \citep{satterthwaite2015towards}.
%
Neuroimaging studies have parcellated the cerebral cortex into distinctive
\textit{functional areas} by topographically mapping brain functions, perceptual
or cognitive processes, onto the brain's anatomy.
% higher visual areas higher visual areas
In the domain of higher-visual perception, replicated findings suggest that
functionally defined, category-selective brain regions like the parahippocampal
place area (\ac{ppa}, \citep{epstein1998ppa}), the fusiform face area (\ac{ffa},
\citep{kanwisher1997ffa}), or the extrastriate body are (\ac{eba}
\citep{downing2001bodyarea}) exhibit significantly increased \ac{bold} activity
correlated with a ``preferred'' stimulus class.

\todo[inline]{cf. general intro \& our papers; add info about involved gyri}
% ppa
The \ac{ppa}, for example, exhibits increased \ac{bold} activation when subjects
passively view photos of landscapes and compared to other stimuli like tools or
faces \citep{aguirre1998area, epstein1998ppa}.


\subsection{Functional localizer}

\todo[inline]{cf. general intro; longer text here or in general intro?}

% example: higher functional areas
The ``topographies of category-selective areas are mostly distributed similarly
across individuals, but great individual variability exists in the locus, the
size, and the shape of the category-selective areas \citep{zhen2015quantifying,
zhen2017quantifying} '' \citep{jiahui2020predicting}.

\todo[inline]{give example of study regarding variability of PPA here}
\todo[inline]{cite paper(s) that does not estimate individual location from
reference 'cause these papers will be cited in ``The solution: estimation from
reference data''}

For example, \citep{frost2012measuring, weiner2018defining,
rosenke2021probabilistic, wang2015probabilistic} have shown that...


% localizer
Traditionally, location, size, and shape of functional areas are determined by
letting study participants perform a task during a \textit{functional localizer}
paradigm.
%
``To deal with the idiosyncratic topography of functional areas,
category-selective areas are identified separately in each individual using a
``functional localizer'' fMRI scan'' \citep{jiahui2020predicting}.
%
``Functional localizers use a simple contrast between responses to different
categories of stimuli, such as responses to faces versus responses to objects,
to identify category-selective areas or to map a category-selectivity topography
[\citep{saxe2006divide}]'' \citep{jiahui2020predicting}.


\subsubsection{The problem (with functional localizers)}

% problem: one localizer for one domain
Functional localizers are designed to maximize detection power and thus
dedicated to map just one domain of brain functions.
% which gets messy
Consequently, if one wants to map a variety of domains, the approach ``one
paradigm for one domain of functions'' gets time-consuming and inefficient.
% localizer batteries: intro
Researchers have tried to tackle that issue by creating time-efficient
multi-functional \textit{localizer batteries} (e.g., \citep{barch2013function,
drobyshevsky2006rapid, pinel2007fast}).
% task based = shit
Nevertheless, the diagnostic quality of localizer batteries relies heavily on
the participant's comprehension of the task instruction and general compliance,
a criterion that can be difficult to meet in clinical or pediatric populations.


\subsection{The solution: estimation from reference data}

% prediction: general idea
Another approach to reduce time and costs of individual diagnostics is to
estimate the location of a functional area based on data collected in a
reference group.


\subsubsection{Anatomical alignment}


\paragraph{Estimation via anatomical alignment}
%
A traditional procedure \citep{frost2012measuring, weiner2018defining,
zhen2017quantifying, zhen2015quantifying, rosenke2021probabilistic,
wang2015probabilistic} [check; cite just the best(s)] to estimate a person's
functional topography from a reference group starts by anatomically aligning a
person's structural and functional data to a \textit{common anatomical space}
(i.e. a brain template) in order to resolve anatomical variability across
persons.
% volume-based
Volume-based anatomical alignment (s. \citep{klein2009evaluation} for a review)
aligns voxel-wise data of individual subjects to a three-dimensional brain
template (e.g. MNI152 template; \citep{fonov2011unbiased}).
% surface-based
Surface-based anatomical alignment \citep{fischl1999cortical} aligns vertex-wise
data of individual subjects to a two-dimensional template (e.g. FreeSurfer
fsaverage template; \citep{fischl1999high}).
% estimation
In a second step, the most probable location of a functional area in a person's
anatomy is then estimated by projecting functional data from persons in the
reference group through the common anatomical space into the individual person's
brain anatomy.


\paragraph{studies that estimated functional areas using anatomical alignment}

\todo[inline]{2 examples should be enough}

For example,  \citep{frost2012measuring, weiner2018defining,
zhen2017quantifying, zhen2015quantifying, rosenke2021probabilistic,
wang2015probabilistic} [same refs as above] have shown that ...

\todo[inline]{most studies did binary estimation (voxel belongs to PPA or not)
based on subjectively chosen thresholds; we estimate unthresholded $Z$-maps!}

\todo[inline]{check that in other studies and make that point more clear in
(intro, method section, and) discussion}


\paragraph{Functional-anatomical correspondence}

% definition: functional-anatomical correspondence
However, location, size and shape of category-selective regions can differ
across individuals by millimeters or centimeters [\citep{zhen2017quantifying,
zhen2015quantifying}] \citep{feilong2018reliable}.
%
Surface-based inter-subject alignments / strategies for registering individual
brains together that respect cortical foldings \citep{fischl1999cortical,
yeo2009spherical} has been shown to reduce the variability in
\textit{functional--anatomical correspondence} \citep{feilong2018reliable,
kumar2020brainiak} across persons \citep{klein2010evaluation,
frost2012measuring} [but see \citep{langers2014assessment}]

%
Cortical surface-based alignment \citep{fischl2012freesurfer} that respects
sulcal locations can reduce the mismatch between brain function and anatomy of
category-selective regions \citep{duncan2009consistency, frost2012measuring,
weiner2018defining, weiner2014mid} [still similar phrasing to
\citep{feilong2018reliable}].
%
Nevertheless, surface-based alignment [\citep{fischl2012freesurfer}] does not
eliminate the mismatch between brain function and anatomy of category-selective
regions \citep{duncan2009consistency, frost2012measuring, weiner2018defining,
weiner2014mid}[still similar phrasing to \citep{feilong2018reliable}].

\todo[inline]{seems to work better with low-level areas}
%
``These systems provide better alignment across subjects for retinotopic visual
areas [\citep{fischl1999high}], but they do not do much better than Talairach
co-ordinates for category-selective regions in the temporal lobe
[\citep{spiridon2006location}]'' \citep{saxe2006divide}.


\subsubsection{Functional alignment (FA)}

\todo[inline]{I introduce the term ``common functional space'' as an umbrella
term for common model space (hyperalignment) and shared feature space / shared
response model (SRM)}

\paragraph{Explanation of functional alignment}

% intro
Since anatomical alignment addresses the issue of anatomical variability but
does not consider variability in functional-anatomical correspondence across
subjects, algorithms --- like \textit{hyperalignment} \citep{haxby2011common,
guntupalli2016model} or the \textit{shared response model}
\citep{chen2015reduced, zhang2016searchlight} --- have been developed that aim
to functionally align voxels (or surface vertices).
%
In order to preserve functional idiosyncrasies across persons,
\textit{functional alignment} algorithms do not align voxels (or surface
vertices) that share the same anatomical location, voxels are aligned that share
similar functional properties.
%
Therefore, functional topographies are not aligned to a three-dimensional,
common anatomical space but to a high-dimensional, \textit{common functional
space}.
%
The construction of the common functional space can be based on the maximization
of the inter-subject similarity of \ac{bold} response time series correlating
with a time-locked external stimulation \citep{haxby2011common, chen2015reduced,
sabuncu2010function}, or based on the inter-subject similarity of connectivity
profiles \citep{feilong2018reliable, guntupalli2018computational,
nastase2019leveraging}.
%
Additionally, functional alignment algorithms also compute subject-specific
transformation matrices that can be used to project functional data into the
common functional space \citep{haxby2020hyperalignment}.
%
Vice versa, the transpose of a subject-specific transformation matrix can be
used to project data from the common functional space into a subject's
anatomical space.


\paragraph{works better with naturalistic stimuli}

\todo[inline]{does not really fit in here; but needs to be mentioned somewhere}

%
Functional alignment based on response time series can be applied to fMRI data
from paradigms with simplified stimuli, but it has been shown that data from
naturalistic stimuli provide increased validity of common functional space and
increased generalizability of transformation matrices to novel stimuli or tasks,
presumably because naturalistic stimulus paradigms sample a broader range of
brain states \citep{haxby2011common, guntupalli2016model}.

\todo[inline]{check \citet{guntupalli2016model, haxby2011common} for phrasing}

e.g. \citet{guntupalli2016model}: ``The algorithm also can be applied to
simpler, controlled experimental data, but our previous results showed that the
sampling of response vectors from these experiments is impoverished and produces
a model representational space that does not generalize well to new stimuli in
other experiments (Haxby et al.  2011). Results show that the computational
principles underlying this common model have broad general validity for
representational spaces in occipital, temporal, parietal, and frontal cortices''
\citep{guntupalli2016model}.


\paragraph{Estimation via functional alignment (=current use case)}

\todo[inline]{streamline procedures of estimation via anatomical \& functional
alignment; use same phrasing in both parts}

%
Hence, a more recent procedure to estimate a person's functional topography from
a reference group first aligns structural and functional data to a common
anatomical space. Then, in order to preserve functional idiosyncrasies across
persons, the procedure additionally performs a functional alignment to
the higher-dimensional, common functional space.
% estimation
In the last step, the most probable location of a functional area in a person's
anatomy is not estimated by projecting functional data from persons in the
reference group through the common anatomical space but by projecting data
through a common functional space into the individual person's brain anatomy.


\paragraph{studies that estimated functional areas using functional alignment}

\todo[inline]{imo, explaining just one example, \citet{jiahui2020predicting}, is
enough}

\todo[inline]{shorten; same stuff is just stated differently}

Previous studies \citep{jiahui2020predicting, guntupalli2016model,
haxby2011common} have shown that ``idiosyncratic topographies for
category-selectivity and retinotopy can be estimated in individual brains with
high fidelity using hyperalignment to project other subjects' functional
localizer data into a target subject's ventral temporal and occipital cortical
anatomy'' \citep{jiahui2020predicting}.

%
For example, \citep{jiahui2020predicting} used hyperalignment to create a common
functional space and transformation matrices based on data from a) the movie
``Grand Budapest Hotel'' (~50 min; 1s repetition time), and b) Forrest Gump
(~120m; 2s repetition time) \citep{hanke2016simultaneous}.

%
Their ``findings show that individually-tailored maps estimated from other
subjects’ data after hyperalignment correlate much more highly with maps
estimated from that subject's own localizer data than does a group average map
based on anatomical normalization'' \citep{jiahui2020predicting}.

%
They showed ``that precise mapping of functional topographies in a new subject
can be achieved using hyperalignment and a database of movie and localizer data
from other subjects'' \citep{jiahui2020predicting}.

%
Their results demonstrate ``that a subject's idiosyncratic functional topography
can be estimated with high fidelity from that subject's fMRI data obtained while
watching a naturalistic movie using hyperalignment to project other subjects'
localizer data into that subject's idiosyncratic cortical anatomy''
\citep{jiahui2020predicting}.


\subsection{Here, we...}


\subsubsection{Overview of 2-3 sentences}

% summary in one sentence
Here, we estimate the results (statistical $Z$-maps) of a visual localizer
paradigm in one subject (i.e. the ``test subject'') based on data from other
subjects (i.e. the ``training subjects'').
%
Focussing on the \ac{ppa} as a \ac{roi}, we compare the prediction performance
of a volume-based, anatomical alignment procedure to a volume-based, functional
alignment procedure.

\todo[inline]{...and evaluate the necessary length needed}


\subsubsection{Shared response model}

%
We employ the \ac{srm} \citep{chen2015reduced, richard2019fast} in order to
estimate subject-specific $Z$-maps gained from visual localizer experiment that
aimed to localize the \ac{ppa} from a reference group.

%
The \ac{srm} is an unsupervised probabilistic latent-factor model that
decomposes \ac{bold} \ac{fmri} responses time series of participants
experiencing the same stimulus into a lower-dimensional space of shared feature
(i.e. the common functional space) and subject-specific orthogonal topographic
transformation matrices \citep{kumar2020brainiak, cohen2017computational}.
%
The dimensions of the common model space do not correspond to individual voxels
but \textit{shared features} that can be understood as a weighted sum of many
voxels distributed across the full voxel space of each subject
\citep{kumar2020brainiak}.
%
In contrast to hyperalignment, the number of dimensions of the shared feature
space is not set by the number of voxels (or surface vertices) but pre-specified
by the researcher, a procedure that filters out noise and reduces overfitting
\citep{chen2015reduced}.
%
As in hyperalignment, subject-specific transformation matrices perform the
mapping (i.e. the functional alignment) from each subject's idiosyncratic voxel
space into the common functional space \citep{kumar2020brainiak,
cohen2017computational}.
% transpose
The transpose of the matrix can be used to map data from shared space into a
subjects' anatomy.


\paragraph{the math shit}

\todo[inline]{merge into text above? or move into method section?}
\todo[inline]{at the moment, I think it's better placed in the methods}

%
More specifically, ``the brain data for each participant $i$ are represented as
an \textbf{$n$} voxel by $t$ time matrix  $X_{i}$. A pre-specified number of
features are used to learn a $k$ feature by  $t$ time shared space $S$ and a
participant-specific $n$ voxel by $k$  feature matrix $W_{i}$. This weight
matrix, reflecting the loading of voxels onto features, is randomly initialized
then fit over iterations to minimize the error in explaining participant data.
At the same time, the time course of the features in the shared space is
learned'' (from https://brainiak.org/tutorials/11-SRM/).

%
``SRM learns $N$ maps $W_{i}$ with orthogonal columns such that
$||X_{i}-W_{i}S||_{F}$ is minimized over $\left\{ W_{i}\right\} _{i=1}^{N},S$,
where $X_{i}\in\mathbb{R}^{v\times{T}}$ is the $i^{th}$ subject's fMRI response
($v$ voxels by $T$ repetition times) and $S\in\mathbb{R}^{k\times{T}}$ is a
feature time-series in a $k$-dimensional shared space''
\citep{vodrahalli2018mapping}.


\subsubsection{Our procedure}

\todo[inline]{I am confused, we have leave-one-out but k-folds?}

% creating the shared feature space
In the current study, we use publicly available fMRI data
\href{www.studyforrest.org}{studyforrest.org} of subjects (N=14) participating
in a visual localizer paradigm \citep{sengupta2016extension} but also watching a
Hollywood \citep{hanke2016simultaneous} movie and listening to the movie's
audio-description \citep{hanke2014audiomovie} in order to derive the common
functional space.
%
Results of our previous study suggest \citep{haeusler2022processing} that the
movie and the audio-description of Forrest Gump also sample the response vector
space of ``spatial responses'' time-locked across subjects similarly to the
visual localizer.
%
Following a k-fold leave-one-subject-out cross-validation, we compute a common
functional space from N-1 test subjects and use the subject-specific
transformation matrices to project the results of the visual localizer contrast
aimed at localizing the \ac{ppa} (s. \citet{sengupta2016extension} into the
common functional space.
% align left-out subject
Assuming that the naturalistic stimuli would trigger, among others, brain
responses that are similar to those triggered by the functional localizer, we
then used time series data of the naturalistic stimuli to align the left-out
test subject to the fixed common functional space.
% project into common functional space
This alignment procedure of the test subject to the common functional space
provides a transformation matrix whose transpose we use to project
the functional localizer results from the common functional space into the test
subject's voxel space.


\paragraph{partial alignment}

\todo[inline]{Jeez! Phrasing! Just shorten it}

\todo[inline]{also, speak of length and/or give number of minutes 'cause that's
critical}

%
Critically, we also evaluate the stimulus length (i.e. the amount of data) that
is needed to generate transformation matrices that ``outperforms'' a
prediction/estimation based on mere anatomical alignment.

% phrasing 1
For that reason, we assess the relationship between the prediction performance
based on the functional alignment and the quantity of data used to calculate
subject-specific transformation matrices that we use to project the functional
data from the common functional space into a subject's anatomy.

% phrasing 2
In other words, we test which amount of data is needed to perform an alignment
to the common functional space that provides transformation matrices that
outperform a prediction based on an anatomical alignment.


\paragraph{model validation; within experiment estimation}

\todo[inline]{well, it is interesting but not super necessary to test}
\todo[inline]{imo, it's too much to mention it here}
\todo[inline]{but mention in method, results (plots?!), discussion?}
\todo[inline]{if, then adjust scripts (and plot it, too)}

%
``Between-subject models with SRM can, in some cases, exceed the performance of
within-subject models because (a) the reduced-dimension shared space can
highlight stimulus-related variance by filtering out noisy or
non-stimulus-related features, and (b) the between-subject model can effectively
leverage a larger volume of data after functional alignment than is available
for any single subject'' \citep{kumar2020brainiak}.
%
Hence, we also test alignment based on 15 minutes (?) of data from the visual
localizer to acquire the transformation matrices and perform a
cross-subject-prediction (vs. cross-subject-cross-experiment-prediction)


\subsection{Hypotheses}

\todo[inline]{Phrasing!; cf. general introduction}

%
We hypothesized that increased quantity of data used to calculate the
transformation matrices of left-out subjects for a functional alignment would to
increased prediction performance.
%
Further, we hypothesized that functional alignment based on ``sufficient''
quantity of data would eventually perform ``better'' than an estimation based on
anatomical alignment.


\subsection{Summary of results}

%
cf. \citep{jiahui2020predicting}: ``Results show strong correlations of
face-selectivity topographic maps derived from a subject's own localizer data
with maps derived from other subjects' localizer data projected into that
subject's cortical anatomy. Both the two-step algorithm and the new one-step
algorithm, which we introduce here, produce high-fidelity, individualized
topographic maps, but the new one-step algorithm maps were superior''
\citep{jiahui2020predicting}.


\subsection{Vision}

\todo[inline]{cf. general introduction}

\todo[inline]{smallish text should be enough here}

Our results suggest that ``scan once, estimate many''...

%
Template from \citet{jiahui2020predicting}'s introduction: ``Movies engage
multiple brain systems in parallel. From a single movie dataset multiple
functional topographies can be estimated (Guntupalli et al., 2016), whereas
different localizers are typically required to map different functional
topographies, making a thorough mapping of selective topographies time-consuming
and inefficient. Movies also simulate better the statistics of natural viewing
and listening and may provide more ecologically valid maps. [...] Naturalistic
stimuli may better sample the full range of responses to faces and other stimuli
that contribute to face-selective topographies'' \citep{jiahui2020predicting}.
%
``These results lay a foundation for building a computational tool with a
database that could allow others to map multiple functional topographies in new
subjects using only data collected during movie viewing. Functional localizers
are inefficient because they only estimate one or a few topographies for each
localizer. Movies, by contrast, engage in parallel multiple neural systems for
vision, audition, language, person perception, social cognition, and other
functions.  Consequently, movies have the potential to estimate selective
topographies in all of these domains. Such a tool would require a database of
data for movies and a range of functional localizers in a normative group of
subjects. A new subject's functional topographies could be estimated based only
on that subject's movie data and other subjects' localizer data from the
normative database that could be projected into that subject's cortical anatomy
using hyperalignment transformation matrices derived from movie data. Such a
resource would be more efficient and replace tedious functional localizers with
an engaging movie and could enable mapping of multiple functional topographies
with data from a single fMRI using a naturalistic stimulus''
\citet{jiahui2020predicting}.


\section{Methods}

\todo[inline]{okay, it's pretty rigorously shortened now; I hope it's okay and
nothing that should urgently be mentioned is missing}

% we get the data from the naturalistic PPA paper (its subdataset)
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in\_bold3Tp2/sub-??\_task-a?movie\_run-?\_bold*.*

% reference to PPA-Paper
For the current study, we used the same subset of the
\href{http://www.studyforrest.org}{studyforrest.org} dataset that was used in
study 2 \citep{haeusler2022processing}:
%
the same fourteen subjects were
% VIS
a) participating in a dedicated six-category block-design visual localizer
\citep{sengupta2016extension}.
% AV
b) watching the audio-visual movie \citep{hanke2016simultaneous}, and
% AD
c) listening to the audio-description \citep{hanke2014audiomovie} of the movie
``Forrest Gump''.
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications, whereas a summary is provided in \citep{haeusler2022processing}.


\subsection{Preprocessing}

% data sources
The current analyses were carried out on the same preprocessed fMRI data (s.
\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned
}{\url{github.com/psychoinformatics-de/studyforrest-data-aligned}}) that were
used for
%
a) the technical validation of the dataset \citep{hanke2016simultaneous},
%
b) the localization of higher-visual areas \citep{sengupta2016extension}, and
%
c) the investigation of responses of the \ac{ppa} correlating with naturalistic
spatial information in study 2 \citep{haeusler2022processing}.

%
We reran the preprocessing and analyses step performed in
\citet{sengupta2016extension} and \citet{haeusler2022processing} using FEAT
v6.00 (FMRI Expert Analysis Tool \citep{woolrich2001autocorr}) as shipped with
FSL v5.0.9 (\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software Library}
\citep{smith2004fsl}) in order to reproduce both the time series data that
served as final input for the statistical analyses in the two original studies
publications as well as their results (i.e. statistical $Z$-maps).
% temporal filtering
In summary, high-pass temporal filtering was applied using a Gaussian-weighted
least-squares straight line to every run of the visual localizer (cutoff period
of \unit[100]{s}; sigma= \unit[100]{s}/2)[???], and every segment of the movie
and audio-description (\unit[150]{s}; sigma=\unit[75.0]{s}).
% brain extraction
Brains were extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
As in the original studies, data from all three experiments were spatially
smoothed (Gaussian kernel with full width at half maximum of \unit[4.0]{mm}),
and a grand-mean intensity normalization of each run's (or segment's) time
series was performed by a single multiplicative factor.

\todo[inline]{can't remember: is grand mean per run/across runs?; imo per run}


\subsection{Modeling of a shared feature space}

On these reproduced data, we then performed further analyses steps via
Python scripts that relied on
%
NiBabel v3.2.1 (\href{https://nipy.org}{\url{nipy.org}}),
%
NumPy v1.20.2 (\href{https://numpy.org}{\url{numpy.org}}),
%
Pandas v1.2.3 (\href{https://pandas.pydata.org}{\url{pandas.pydata.org}}),
%
Scipy v1.6.2 (\href{https://scipy.org}{\url{scipy.org}}),
%
scikit-learn v1.0 (\href{https://scikit-learn.org}{\url{scikit-learn.org}}),
%
BrainIAK v0.11 (\href{https://brainiak.org}{\url{brainiak.org}}),
%
Matplotlib v3.4.0 (\href{https://matplotlib.org}{\url{matplotlib.org}}),
%
seaborn v0.11.2 (\href{https://seaborn.pydata.org}{\url{seaborn.pydata.org}}),
%
and calling command line functions of FSL.

%\paragraph{Fixing FSL output}

% grand_mean_for_4d.py (formerly: data_normalize_4d.py):
% is not necessary anymore: FSL has applied grand mean scaling to
% 'filtered_func_data.nii.gz'

% input: 'sub-*/run-?.feat/filtered_func_data.nii.gz' (of VIS, AO & AV)
% output: saved to 'sub-??_task-*_run-?_bold_filtered.nii.gz'

% FSL adds back the mean value for each voxel's time course at the end of the
% preprocessing;
% hence, the script substracts that mean again but multiplies it by 10000
% (like FSL does it, too)

% definition of grand mean scaling for 4d data:
% voxel values in every image are divided by the average global mean
% intensity of the whole session. This effectively removes any mean global
% differences in intensity between sessions.

% FSL User Guide:
% filtered_func_data will normally have been temporally high-pass filtered,
% it is not zero mean; the mean value for each voxel's time course has been
% added back in for various practical reasons.
% When FILM begins the linear modeling, it starts by removing this mean.


\paragraph{Getting the data in shape}

% masks-from-mni-to-bold3Tp2.py:
% - merges unilateral ROIs overlaps (already in MNI) to bilateral ROI
% - output: 'masks/in_mni/PPA_overlap_prob.nii.gz'
% - warps union of ROIs from MNI into each subjects space
% output: 'sub-*/masks/in_bold3Tp2/grp_PPA_bin.nii.gz' + audio_fov.nii.gz dilate
% the ROI masks by 1 voxel; output: 'grp_PPA_bin_dil.nii.gz'

% masks-from-mni-to-bold3Tp2.py:
% warp MNI masks into individual bold3Tp2 spaces

% masks-from-t1w-to-bold3Tp2.py:
% transforms 'inputs/tnt/sub-*/t1w/brain_seg*.nii.gz'
% into individual's bold3Tp2
% output: 'sub-*/masks/in_bold3Tp2/brain_seg*.nii.gz'

% mask-builder-voxel-counter.py:
% builds different individual masks by dilating, merging other masks
% creates a FoV of AO stimulus for every subject from 4d time-series of AO run
% output: sub-*/masks/in_bold3Tp2/audio_fov.nii.gz'
% counts the voxels
% long story short: we cannot used all gyri that contain PPA to some degree
% even if the mask by FoV of AO stimulus and individual gray matter mask

% data_mask_concat_runs.py:
% masks are not dilated and not masked with subject-specific gray matter mask
% outputs:
% 'sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy
% 'sub-*_task_visloc_run-1-4_bold-filtered.npy'

\todo[inline]{problem 1: grpPPA contains N=14 subject, not N-1 subjects}

\todo[inline]{problem 2: there are voxels outside of PPA-mask; probably, because
of the warping procedures}

\todo[inline]{how to explain in one sentence that we need reduction of voxels?}

% reason why we do it
SRM algorithm needs (``way'') more samples (TRs) than features/voxels to train
the model.

% union of PPA masks
For each paradigm and subject, the corresponding time series of each run (or
segment) were masked with the union of individual PPA masks
\citep[s.][]{haeusler2022processing} that was warped from MNI space into each
subjects' voxel space.
% AO FoV
Each subjects' data were further masked with the subject-specific \ac{fov} of
the audio-description.
%
The number of remaining voxels for each subject can be seen in Table
\ref{tab:ppamaskvoxels}.

\todo[inline]{show table or just report range, mean \& SD?}

% normalization
Data of every run were independently normalized ($z$-scored) to a mean of zero
and a standard deviation of one ($\mu=0$, $\sigma=1$).


\todo[inline]{yeah, well, probably I should have cut the last 75TRs of AO first,
and then z-scored the last segment but anyway...; does not make much difference}

%
The last 75 TRs of the audio-description were missing in subject-04 due to an
image reconstruction problem \citep[s.][]{hanke2014audiomovie}.
%
The \ac{srm} allows the number of voxels to be different across subjects but the
number of samples must be the same.
%
Hence, we removed the last 75 TRs of the audio-description from the other
subjects' time series.
% summary; AO + AV = 7123 TRs (not 7198 TRs anymore); localizer has 4x156 TRs
As a result, the data to fit the \ac{srm} in the following step comprised 3599
TRs of the movie, 3524 TRs of audio-description, and 624 TRs of the visual
localizer experiment.


\begin{table*}[btp]
    \caption{Number of remaining voxels after time series data of each paradigm
    and subject were masked with the union of individual \acp{ppa} that was
    warped from MNI space into each individual's subjects-space and the
    subject-specific field of view of audio-description.}

\label{tab:ppamaskvoxels}
\begin{tabular}{ll}
\toprule
\textbf{Subject} & \textbf{no. of voxels} \\
\midrule
sub-01 & 1665 \tabularnewline
sub-02 & 1732 \tabularnewline
sub-03 & 1400 \tabularnewline
sub-04 & 1575 \tabularnewline
sub-05 & 1664 \tabularnewline
sub-06 & 1951 \tabularnewline
sub-14 & 1376 \tabularnewline
sub-09 & 1383 \tabularnewline
sub-15 & 1683 \tabularnewline
sub-16 & 1887 \tabularnewline
sub-17 & 1441 \tabularnewline
sub-18 & 1729 \tabularnewline
sub-19 & 1369 \tabularnewline
sub-20 & 1437 \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\paragraph{Fitting of shared response model: intro}

\todo[inline]{mention that just 'ao \& av' as input were also tested?}

We used the probabilistic \ac{srm} algorithm implemented in BrainIAK v.11 (Brain
Imaging Analysis Kit, \citet{kumar2020brainiak, kumar2020brainiaktutorial},
http://brainiak.org) in order to compute the common functional space and acquire
the transformation matrices for the ($N-1$) subjects.
%
This implementation approximates the \ac{srm} using the Expectation Maximization
(EM) algorithm proposed by \citep{chen2015reduced} optimized by
\citet{anderson2016enabling}.


\paragraph{Cross-validation stuff}

\todo[inline]{streamline with overview in introduction}

%
In order to avoid data leakage, we followed a leave-one-subject-out folding
scheme:
%
for every subject $n$, we used the other subjects' ($N-1$) concatenated
\ac{bold} \ac{fmri} responses to the movie, audio-description, and functional
localizer as training set to compute the shared feature space and transformation
matrices for $N-1$ subjects.



\paragraph{Concatenation \& (a second) z-scoring}

\todo[inline]{Additionally, I performed a z-scoring across all runs/paradigms}

% concatenate and z-score
The time series of all three paradigms were concatenated and z-scored.



\paragraph{number of dimensions}

% iterations:
% The number of iterations for the algorithm to minimize the error was set to 30

\todo[inline]{check spatial and temporal resolution of \citet{haxby2011common}}

% features
[Considering the size of the ROIs, and the spatial and temporal resolution of
the \ac{bold} \ac{fmri} data,] we chose a value of $k=10$ for the number of
features (i.e. the shared responses) to be computed.

\todo[inline]{following contains different quotes but all with similar phrasing}
%
We based our decision on findings of \citep{haxby2011common} who, using
hyperalignment, first created a common functional space based of 1,000
dimensions from voxels of the ventral temporal cortex.
%
Then, they ``reduced the dimensionality of the common space by performing a
\ac{pca} and determined the subspace that is sufficient to capture the full
range of response-pattern distinctions'' \citep{haxby2011common}.
%
Their findings showed ``that the cortical topographies associated with
well-known category selectivities are preserved in the 35-dimensional common
model space'' \citep{haxby2011common}.
%
``Applying \ac{pca} to the model space for ventral temporal cortex, developed
based on patterns of activity evoked by a naturalistic movie, revealed that
approximately 35 principal components (PCs) were sufficient to account for the
information content of one hour of the movie'' \citep{haxby2011common}.
%
``We performed a \ac{pca} of the mean responses to each movie time point in
common model space, averaging across subjects, then performed BSC of the movie,
face and object, and animal species data with varying numbers of top principal
components (PCs)'' \citep{haxby2011common}.
%
``The effect of number of PCs on BSC was similar for models that were based only
on Princeton (n = 10) or Dartmouth (n = 11) data, suggesting that this estimate
of dimensionality is robust across differences in scanning hardware and scanning
parameters (see Figure S3D)'' \citep{haxby2011common}.
%
``In our initial report on a common model of VT cortex, we found that ~35
dimensions were sufficient to capture the movie information content contained in
the fMRI data, as well as the information in 2 category perception experiments''
\citep{guntupalli2016model}.
%
``These dimensionality estimates are a function of the spatial and temporal
resolution of fMRI and the number and variety of response vectors used to derive
the common space'' \citep{guntupalli2016model}.
%
``The true dimensionality of representation in human cortex surely involves
vastly more distinct tuning functions. Estimates of the dimensionality of
cortical representation, therefore, will almost certainly be much higher as data
with higher spatial and temporal resolution for larger and more varied samples
of response vectors are used to build new common models''
\citep{guntupalli2016model}.

\paragraph{also tested:}
%
We also explored common functional spaces of $k=5,20,30,40,50$, but the results
barely varied from using $k=10$ dimensions [phrasing similar to
\citep{vodrahalli2018mapping}].


\paragraph{the math shit}

\todo[inline]{maybe, shit the math shit from intro to here}

%
During model fitting, the algorithm computes the shared feature space $S$ ($k$
features by $t$ time points) and calculates subject-specific, orthogonal
transformation matrices $W_{n}$ ($v$ voxel by $k$ features).
%
These transformation matrices (or weight matrices) reflect the loadings of
voxels onto features (i.e. subject-specific functional topographies), and allow
to project responses of voxels within each subjects' \ac{roi}(s) from anatomical
space into the $k$-dimensional shared feature space, and thus functionally align
subjects.


\paragraph{negative control}

\todo[inline]{report it? if, then correct script; plot it (all 0's), too?}

\todo[inline]{I need to check the code; probably, wrong indices are still used}

% shuffled data
As negative control, we shuffled the order of runs of audio-description and
movie (but not visual localizer!) before fitting a shared response model to the
time series [in order to assess the correlation between regressors of the
experiments and the computed shared responses in the corresponding TRs]



\subsection{Alignment of left-out subjects}

% AO: 0-451, 0-892, 0-1330, 0-1818, 0-2280, 0-2719, 0-3261, 0-3524
% AV: 3524-3975, 3524-4416, 3524-4854, 3524-5342, 3524-5804, 3524-6243,
%     3524-6785, 3524-7123
% AO+AV: 0-7123

\todo[inline]{Phrasing! well, at least, explanation makes sense in my head}

\todo[inline]{how is it down? srm.transform\_subject calls np.linalg.svd()}

\todo[inline]{also perform alignment with TRs of first run of localizer for mere
cross-subject-prediction? -> kind of model validation, ceiling ...}

%
To obtain the transformation matrix of the left-out subject, we then aligned the
left-out subject's data to the shared space $S$ [that was trained on the other
subjects' data].
%
The algorithm learns a mapping $W_{n}$ of the left-out subject's anatomical
space into the shared space $S$ that is kept fixed.

%
Critically, in order evaluate the relationship between the length of the
stimulus that is used to align an ``unknown'' test subject to a pre-existing
shared space and prediction performance, we performed a \textit{partial
alignment}:
%
We used an increasing number of segments (1 up to 8) of the naturalistic stimuli
per subject to let the algorithm learn the orthogonal mapping to the TRs of the
corresponding segment(s) in the shared space.
%
As a result, we obtained 8 transformation matrices per subject and per
naturalistic stimulus.
%
Each matrix has a size of $v$ voxels by $k$ features but is based on a different
quantity of data used to calculate the mapping [i.e. subject-specific functional
topographies].


\subsection{Estimation of scene-selective topography}


\todo[inline]{current procedure: mean was computed in subject-specific
anatomical space}

\todo[inline]{also report that I tested creating a template by computing the
mean in shared feature space? results were pretty similar}

\todo[inline]{what I did not do in case of prediction via anatomical alignment:
average the z-maps in group space -> estimation using anatomy has always
been mean of z-maps in subject-space}

\subsubsection{short overview}

%
Template from \citet{jiahui2020predicting}: ``We estimated each participant's
scene-selectivity map [= whole-cortex; we: values in the \ac{roi}] based on that
participant's localizer data and based on other participants' localizer data
projected into that participant's cortical anatomy using hyperalignment and
anatomical volume-based alignment (see Fig. 1)'' \citep{jiahui2020predicting}.

%
To predict the results of the functional localizer (statistical $Z$-maps
representing functional topographies) of a left-out subject, we projected all
other participants $Z$-maps from their subject-specific anatomy into the shared
feature space, and from the shared feature space into the left-out subject's
anatomy.
%
As a benchmark to evaluate the performance of the estimation using functional
alignment, we similarly estimated each left-out subject's $Z$-map using
volume-based anatomical alignment.

Hence, we masked the \ac{glm} univariate contrast maps provided by
\citet{sengupta2016extensiondata} with the union of the individual PPAs
(s.\citep{haeusler2022processing}) that was warped from group space into each
subjects' space.


\subsubsection{Prediction/estimation using functional alignment}

Template from \citet{jiahui2020predicting} ``We estimated a participant's map
from other participants' data by first projecting all other participants'
localizer data [we project the results] into the common functional space, then
averaging t-values across the maps (56 maps for studyforrest, 14 subjects x 4
runs for each)'' \citep{jiahui2020predicting}.
%
``We projected other participants' localizer data into each individual
participant's cortical anatomy using hyperalignment via the common model space''
\citep{jiahui2020predicting}.
%
``Thus, the map estimated from other participants' localizer data was a map of
average t-values across 56 or 80 maps (four runs per participant) for the
contrast faces vs all'' \citep{jiahui2020predicting}.

%
First, individual results from the visual localizer [and auditory naturalistic
stimulus] were masked (in each individuals' subject-space) with the union of PPA
masks group PPA (multiplied by FoV mask).
%
Then, data were transformed from subject-specific, anatomical spaces into the
shared response space [by calling srm.transform(masked\_zmaps)'].
%
Using the transpose of the left-out subject's transformation matrix, the
$Z$-maps were projected from shared feature space into the anatomical space of
the left-out subject.
%
The mean of the 13 projected $Z$-maps served as an estimation of the left-out
subject empirical $Z$-map (s. \citep{sengupta2016extension})


\subsubsection{Prediction using anatomical alignment}

\todo[inline]{check scripts again!}

\todo[inline]{tell more about how transformation matrices were created?
potential reviewer's question: your anatomical alignment might have been shitty,
linear etc. ? -> check TNT github/gin repo; what is the corresponding paper?}

\todo[inline]{``warp'' = non-linear registration with [?] degrees of freedom}

Template from \citet{jiahui2020predicting} ``We also estimated each
participant's face-selectivity map based on other participants' anatomically
aligned localizer data by simply averaging across the maps based on
anatomically-normalized data for each N-1 set of 14 participants''
\citep{jiahui2020predicting}.

%
We also estimated each left-out subject's $Z$-map using volume-based anatomical
alignment.
% into MNI
First, we aligned the results of the visual localizer from each subjects' space
(bold3Tp2) to grpbold3Tp2 (that is co-registered to MNI152) using precomputed
transformation matrices that are part of the studyforrest dataset [LINK; cf.
method section in PPA-Paper?].
% into left-out subject
Then, results of all subjects were warped from grpbold3Tp2 in to the left-out
subject's space
%
The transformed $Z$-maps were masked with mask of unions of PPAs and the
individual's \ac{fov}.
%
Finally, the mean of these $Z$-maps served as a prediction of the corresponding
left-out subject's empirical $Z$-map.


\subsection{Quantifying the performance}


\subsubsection{Correlation stuff}

Template from \citet{jiahui2020predicting}: ``We then calculated correlations
between the map based on a participant's own data and the maps estimated from
other participants' data and compared these correlations to the reliability of
the participant's map, indexed with Cronbach's Alpha''
\citep{jiahui2020predicting}.
%
``We tested the quality of the maps estimated from other participants' data by
calculating the correlation of each with the map estimated from a participant's
own data'' \citep{jiahui2020predicting}.
%
``We also gauged the reliability of the estimates based on participants' own
data by calculating Cronbach's alpha based on variability across runs''
\citep{jiahui2020predicting}.

%
To quantify the performance of our estimations based on different number of runs
of the movie and audio-description, we calculated the Pearson's correlation
coefficients between the empirical $Z$-maps from the localizer experiment [and
audio-description] and estimated $Z$-map based on other subject's data.
%
Further we compared these correlations to the reliability [internal
consistency?] of visual localizer using Cronbach's Alpha based on the
variability across runs [as did
\citep{jiahui2020predicting}].


\subsubsection{Cronbach's Alpha of localizer}

\todo[inline]{s. 'studyforrest-srm-movies/test/statistics\_cronbachs.py'}

\todo[inline]{script is apparently written; tbh, did not take a close look at
the result 'cause it was the last that I did regarding writing scripts}


\subsection{backup: alternative template creation}

\todo[inline]{is this supposed to be reported? imo, it should be dropped!}

\todo[inline]{in general, the problem was: it gets totally messy \&
computational intensive if one wants to test it using the different
transformation matrices (I only did it with one; imo, based on alignment using
the whole audio-description?}

\todo[inline]{I need to take a look in the scripts (in the draft directory); I
do not understand the scripts anymore (on a Sunday evening)}

\todo[inline]{I think I projected all subjects' localizer time series through
model space into the test subject voxel space; then, calculated the contrast
with these data}

\todo[inline]{results: performance was the same if not slightly worse}

%
We currently apply another method to get the ``z-map template'' on which the
prediction is based [s. 'test/data\_denoise-vis.py' \&
'test/data\_srm-vis-to-ind.py'].


\paragraph{just some templates from other studies}

Template from \citet{jiahui2020predicting}: ``We estimated a participant's map
from other participants' data by first projecting all other participants'
localizer data into that participant's cortical anatomy and calculating the GLM
univariate contrast map of faces versus all other categories for each run in
each other participant then averaging t-values across the maps (56 maps for
StudyForrest, 14 subjects x 4 runs = 56 maps)'' \citep{jiahui2020predicting}.

%
Template from \citet{guntupalli2016model}: ``To project category-selectivity
topographic maps based on other subjects into the cortical anatomy of a new
subject, we first transformed all subjects' data from the category-selective
localizer study into the common model space using the transformation matrices
derived from the full movie data. For each subject, the functional localizer
data from all other subjects were projected into that subject's brain using the
transposes of individual transformation matrices derived from the full movie
data. These data were then smoothed with a 6 mm FWHM Gaussian filter and mapped
to the curvature-aligned cortical surface mesh. Category-selective t-statistic
maps were then computed for selectivity for faces, places, objects, and bodies
using 3dDeconvolve and 3dREMLﬁt in AFNI on surface nodes based on each subject's
own data and, independently, on the data from other subjects in that subject's
anatomical space. For comparison, similar category-selective t-statistic maps
also were calculated for each subject based on other subjects' data in the
curvature-aligned cortical surface mesh before hyperalignment, thereby using
only anatomical alignment. The similarity of category-selectivity maps
calculated from subjects' own localizer data and from other subjects' localizer
data was computed by calculating correlations (Pearson's r) of the t-statistic
maps in a ventral visual pathway surface ROI that included VT and lateral
occipital cortices, testing the similarity of each individual to both the
hyperaligned and anatomically aligned data from other subjects''
\citep{guntupalli2016model}.
%
``We then contrasted the similarity of maps based on individual and other
subjects' data to the within-subject reliability of category-selective maps. We
estimated the within-subject reliability of the category-selectivity maps by
computing the correlation between t-statistic maps computed from odd and even
runs. To control for the effect of only using half of the localizer data, we
also computed the correlations between the t-statistic maps calculated from the
odd and the even runs in each subject and the maps calculated from other
subjects' data and averaged these correlations after Fisher transformation. We
tested the significance of differences between correlations by calculating 95%
CI of these differences using bootstrapping [Kirby and Gerlanc 2013]''
\citep{guntupalli2016model}''.


\section{Results}


\subsection{Template(s) from from \citet{jiahui2020predicting}}

1-step hyperalignment: ``We correlated the whole-cortex contrast map
(faces-vs-all) based on a participant's own data with the maps estimated from
other participants' data.  After 1-step hyperalignment, the mean Pearson
correlation values across participants were 0.58 (N = 15, S.D. = 0.08)''
\citep{jiahui2020predicting}.
%
``Hyperalignment greatly improved the prediction performance compared with
anatomical surface alignment. With surface alignment, the average Pearson
correlation values across participants were 0.40 (N = 15, S.D. = 0.08) in the
StudyForrest dataset (Fig. 3). The difference between the hyperaligned and the
surface-aligned mean correlation values was highly significant (StudyForrest:
t(14) = 17.39, p < 0.001)'' \citep{jiahui2020predicting}.

%
classic hyperalignment: ``We also estimated each participant's topography with
the 2-step hyperalignment algorithm that projects other participants' localizer
data into a common model information space then uses the transpose of the target
participant's transformation matrix to project other participants' data from the
common model space into his or her cortical anatomy. With this 2-step method, we
found similar but slightly weaker results. The mean Pearson correlation values
between face-selectivity maps based on a participant's own localizer data and
the predicted map after hyperalignment were 0.55 (N = 15, S.D. = 0.08).  These
correlations after hyperalignment were significantly better than the
correlations after surface alignment (t(14) = 15.78, p < 0.001)''
\citep{jiahui2020predicting}.


\subsection{My results}

Unthresholded $Z$-maps [in each subject's voxel space] can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}.


\subsection{Model-related}

\subsubsection{plot\_srm.py}

\todo[inline]{imo, plots are not very informative}

e.g. time series plot of features [first 800 TRs of SRM]; distance matrix of
time-points in shared space (all features combined)


\subsubsection{plot\_corr-of-glm-and-srm.py}

\todo[inline]{imo, plots are not very informative, too}

plots correlation matrix of shared features; correlation matrix of
AO-regressors (s. PPA-Paper) and shared responses (sliced to TRs of AO
stimulus); correlation matrix of AV-regressors (s. PPA-Paper) and shared
responses (sliced to TRs of AV stimulus)


\todo[inline]{possible plots:
correlate AO regressors (PPA-paper) with AO TRs;
correlate AV regressors (PPA-paper) with AV TRs;
correlate VIS regressors with VIS TRs;
add combi of regressors that were used on the positive side of contrasts;
do the same with the model based on shuffled runs}


\begin{figure*}[tbp]
\centering
    \includegraphics[width=\linewidth]{figures/corr_ao-regressors-vs-shared-resp}
    \caption{Pearson correlation coefficients of regressors used in the analysis
    of audio-description to model responses correlating with nouns spoken by the
    narrator and features of the \ac{srm} (i.e. shared responses).
    \texttt{geo\&groom} \texttt{geo\&groom\&furn} are combination of regressors
    (as used on the positive side of contrasts). The
    time series of the \ac{srm} were sliced to match the TRs of the
    audio-description.
      }
\label{fig:reg-corr}
\end{figure*}


\subsection{Prediction-related}

\todo[inline]{predict\_ppa.py: outputs also the Pearson correlations}

``We estimated each participant's face-selectivity map based on that
participant’s localizer data [anatomical alignment], based on other
participants' localizer data projected into that participant's cortical anatomy
using hyperalignment and anatomical surface alignment (see Fig. 1)
\citep{jiahui2020predicting}''.

``We correlated the whole-cortex contrast map (faces-vs-all) based on a
participant’s own data with the maps estimated from other participants’ data
separately for the StudyForrest and Grand Budapest datasets. After 1-step
hyperalignment, the mean Pearson correlation values across participants were
0.58 (StudyForrest, N = 15, S.D. = 0.08)'' \citep{jiahui2020predicting}.

``Hyperalignment greatly improved the prediction performance compared with
anatomical surface alignment. With surface alignment, the average Pearson
correlation values across participants were 0.40 (N = 15, S.D. = 0.08) and 0.50
(N = 21, S.D. = 0.06) in the StudyForrest and Grand Budapest datasets,
respectively (Fig. 3). The difference between the hyperaligned and the
surface-aligned mean correlation values was highly significant (StudyForrest:
t(14) = 17.39, p < 0.001)'' \citep{jiahui2020predicting}.




\subsubsection{plot\_stripplot.py}

%
Pearson correlation coefficients between empirical $Z$-maps (results of
localizer; y-axis) and estimated $Z$-maps.
%
Green dots: A left-out subject's $Z$-map was estimated by projecting all other
subjects ($N = 13$) $Z$-maps through the MNI152 space into the left-out subject
space and averaging values across subject.
%
Blue dots: transformation matrices computed based on an increasing number of
segments of the audio-description.
%
Orange dots: transformation matrices computed based on an increasing number of
segments of the audio-description.


For every subject, we see the correlation of z-maps that tell us the, quote
``real'' unquote, PPA and predicted PPA
%
In green, we see the correlations between empirical values from the localizer \&
the predicted values using anatomical alignment
%
In orange, we see the correlations between empirical values \& the predicted
values using parts of the movie
%
In blue, we see the correlations between empirical values \& the predicted
values using parts of the audio-description


\subsubsection{statistics\_t-test-correlations.py}

Tests whether differences between prediction using anatomy and prediction using
\ac{srm} is significant.


\subsubsection{show plots of brain slices?}

\todo[inline]{talk during seminar showed (quick screenshot in FSL) from sub-04
as an example}

%% We have these nice blurry EPI-images and all z-maps are threshold at a value of bigger than 2.3.

% always in red, we can see the z-map from the localizer experiment across the whole brain,

% the region of interest that we used is white and the predicted values, are blue

% The prediction using anatomical alignment and the prediction using 15 minutes of movie data show a correlation of about .7

% the prediction using 15 minutes of the audio-description correlates about 0 with the empirical z-map

% The last one is the extreme case, but it can give you an idea of how the z-maps look in a slice of the brain

\todo[inline]{I guess, I could adjust script that plots the brain slices for the
PPA-paper (without losing my mind?); but script expects standard brain in MNI
space; transformation of prediction into MNI (again?); and it is just about the
ROI not about the whole brain; standard space could be substituted with (nice
blurry) subject-specific functional template}


\subsubsection{plot\_bland-altman.py}

\todo[inline]{I hate that script}

% corrstats.py; not necessary anymore; calculates the statistical significant
% differences between two dependent or independent correlation coefficients


\subsubsection{Estimation of localizer from localizer TRs}

just "cross-subject" but not "cross-experiment-cross-subject" prediction"


\subsubsection{Cronbach's Alpha of localizer}

\todo[inline]{is Fig 2 in \citet{jiahui2020predicting} any good?}

all from \citep{jiahui2020predicting}: ``We compared the correlations between
maps estimated from a participant's own data and maps estimated from other
participants' data to the reliability of the localizer. We computed the
reliability of the contrast maps with Cronbach's Alpha based on variability
across the four localizer runs for each set. The mean Cronbach's Alpha between
the four localizer runs was 0.60 (N = 15, S.D. = 0.14) in the StudyForrest
dataset [...](Fig. 2)''.
%
``These results mean that if we scan each participant for another 4 localizer
runs, and compute the correlation between the two maps (4 runs vs. 4 runs), the
correlation would be 0.60 on average in the StudyForrest''.
%
``Cronbach's alpha indicates that the predicted contrast map based on
hyperalignment is close to or as good as the real contrast map based on four
localizer runs (StudyForrest: t(14) = 0.61, p = 0.55; Grand Budapest: t(20) =
3.02, p = 0.007)''.
%
``In the StudyForrest dataset, the predicted contrast map based on
hyperalignment was better than the contrast map based on data from three out of
four localizer runs in other participants (t(14) = 2.36, p = 0.03) and in Grand
Budapest, the predicted contrast map was comparable to the contrast map based on
three localizer runs in other participants (t(20) = 0.48, p = 0.63)''.
%
``A scatterplot of the individual correlation values with hyperalignment and
with surface alignment (Fig 3), shows that predicted maps based on
hyperalignment were more accurate than those based on surface alignment in every
participant''.


\section{Discussion}


\subsection{Short Summary}

\subsubsection{Aim}

\subsubsection{Methods}

\subsubsection{Results}


\subsection{Discussion of current results}


We have cross-subject, cross-experiment, and cross-scanner prediction


AV \& VIS are from the same scanner, if not same session?

\subsubsection{Model Space}
%
Results indicate that we are able to use multiple subjects to learn a
10-dimensional shared space for the fMRI data that increases performance on our
experiments.

%
Asymptotic ``performance curve'' might be different for another brain region
(temporal receptive fields?).
%
What about rentinotopic mapping?
%
What about ``higher'' cognition?
%
What about executive functions?


\subsubsection{Partial Alignment}
%
15 min of movie watching used for functional alignment outperform prediction
using anatomical alignment
%
30 minutes of movie watching outperform 15 minutes of movie watching
%
more than 30 minutes do not lead to a significantly improved prediction
performance.


\paragraph{what partial alignment might be good for}
%
Reduced costs.
%
There is a benefit of shared parts of naturalistic stimuli across datasets;
shared stimulus part is easier than shared subjects (e.g.
\citep{zhang2018transfer}).


\subsubsection{Audio PPA}

%
Response in PPA during AO might be different.
%
There are some subjects which do not have a AO PPA 'cause they just do not give
a shit about spatial information in AO? Might be an issue of missing task?


``Notwithstanding their non-conformation to a theoretically universal map of the
brain, such topological outliers, if they do not result from artefacts, can also
be considered to be interesting cases of inter-individual variability to
understand brain–phenotype relationships [98].Indeed, recent studies have
suggested that the topography (location and size) of individual-specific brain
parcellations is predictive of individual differences in demographics,
cognition, emotion and personality [3,5,99]. In this context, we would argue
that the quest to under- stand robust patterns of brain topography across
different markers and the investigation of inter-individual differences are
closely intertwined challenges. Only by understanding the generic characteristic
of topographic organization can we start to appreciate idiosyncrasies and their
relationships to socio-demographic, cognitive or affective profiles''
\citep{eickhoff2018imaging}.

\subsection{Self-critique \& short comings}


\subsubsection{sample size}

``Most studies that use small samples are likely to exhibit highly variable
estimates. This finding suggests that many of the claims of predictive accuracy
in the neuroimaging literature may be exaggerated and/or not valid''
\citep{poldrack2019establishment}.


\subsubsection{localizer is ``ground truth''}

%
``The dynamic localizer (in Grand Budapest data set) was significantly more
reliable than the static localizer (in StudyForrest data set) (t(34) = 3.76, p <
0.001) despite its shorter length (four 234s runs versus four 312s runs,
respectively)'' \citep{jiahui2020predicting}.

%
From \citep{weiner2018defining}: ``the identification of the PPA is complicated
by (at least) four methodological considerations. First, the PPA definition may
depend on the type of experiment, task, and stimuli used. Second, the boundaries
of the PPA may depend on the statistical threshold used. Third, the spatial
extent and localization of the PPA may vary if defined within the native brain
space of an individual or based on a group analysis. Fourth, the size of the PPA
may depend on data acquisition choices (e.g. large vs. small voxels) and data
analysis choices (e.g. liberal smoothing vs. no spatial smoothing). The present
study aims to identify and to predict the most probable location of
place-selective voxels within medial VTC of an individual brain that is
impervious to these methodological decisions'' \citep{weiner2018defining}.

%
\citet{lilienfeld2015fifty} on ``gold standard'': ``In the domains of
psychological and psychiatric assessment, there are precious few, if any,
genuine ``gold standards''. Essentially all measures, even those with high
levels of validity for their intended purposes, are necessarily fallible
indicators of their respective constructs [Cronbach and Meehl, 1955; Faraone and
Tsuang, 1994]. As a consequence, the widespread practice referring to even
well-validated measures of personality or psychopathology, such as Hare’s
(1991/2003) Psychopathy Checklist-Revised, as ``gold standards'' for their
respective constructs [Ermer et al., 2012] is misleading [see Skeem and Cooke,
2010]. If authors intend to refer to measures as ``extensively validated'', they
should simply do so'' \citep{lilienfeld2015fifty}

%
\citet{scheinost2019ten} in context of phenotypic measures: ``Predictive models
based on neuroimaging data will only ever account for a fraction of the
variance. Neuroimaging studies are limited by how much information the signal
can capture about the measure of interest. At the same time, these studies are
also limited by the chosen phenotypic measure used.  While the success of a
model is evaluated by how well it predicts a phenotypic measure (and these
phenotypic measures have to be treated as gold standards), it is well known that
such measures are not always the ground truth but themselves suffer from
confounds and noise.  When studying brain-behavior associations, one must keep
in mind how extraordinary it is that neuroimaging data can be distilled to
approximate phenotypic measures that reflect a simplification of multiple
complex features. Thus, even modest results are reasonable and remarkable. For a
discussion on the reliability of phenotypic measures in the context of
predictive modeling, we point the interested reader to: [Dubois et al., 2018a,
2018b; Gignac and Bates, 2017]'' \citep{scheinost2019ten}.


\subsubsection{ROI creation procedure}

we have a little leakage of test data; especially at the borders of the ROI we
might "miss" some voxels cause the ROIS is based on thresholded mask (performing
com cutting on the edges).

\todo[inline]{check paper regarding ``probabilistic functional atlases''}


\subsubsection{Volume-based registration vs. surface-based normalization}

%
Sengupta did the published analyses $Z$-maps in volume space
%
I could have run the analyses/contrasts on the surface but opportunity costs.
%
\todo[inline]{Volumen-based anatomical alignment was compared to volume-based
functional alignment}

Volume vs. surface: e.g. \citep{desai2005volumetric}

\citep{weiner2018defining} compares cortex-based alignment
\citep{fischl1999high} with volume-based (Talairach) alignment: ``we repeated
our leave-one-out cross-validation procedure across all 24 participants with an
affine volume-based registration to the Talairach brain and compared this
performance to the same procedure implemented with CBA in FreeSurfer
\citep{weiner2018defining}''




\subsection{Future questions}

\subsubsection{Predict other t-contrasts of higher-visual area localizer}


\paragraph{Predict other functional areas / other localizers}


e.g. retinotopy, language areas

%
Again, data from the studyforrest project will be used to perform assessments
analog to WP1e for a range of additional individually determined regions based
on established localizer paradigms (Sengupta, et al., 2016).
%
These regions include: the \ac{ffa}, \ac{ppa}, and \ac{eba}  which are
associated with face perception \citep{kanwisher1997ffa,
pitcher2011occipitalfacearea}, scene perception \citep{epstein1998ppa}, and the
perception of human bodies \citep{downing2001bodyarea}, respectively.

``First, our brains directly process exogenous information about the external
environment by transducing physical phenomena (e.g., changes in energy,
molecular concentrations, etc.) into sensory perceptions that allow us to
generate and maintain a sense of what is happening around us (1, 2). Mental
representations that are directly driven by the external world are likely to be
highly similar across individuals who share the same sensory experience. Second,
our brains also process endogenous information that reflects our current
internal homeostatic states, past experiences, and future goals (3). The
integration of exogenous and endogenous informa- tion allows us to meaningfully
interpret our surroundings, prioritize information that is relevant to our
goals, and develop action plans (4). Given the same input information,
individuals may have unique interpretations, feelings, and plans, often leading
endogenous rep- resentations to be idiosyncratic across individuals''
\citep{chang2021endogenous}.

``Human brains have much in common with one another. Similarities exist not only
at the anatomical level, but also in terms of functional organization. Given the
same stimulus—an expanding ring, for example—regions of the brain that process
sensory (visual) stimuli will respond in a highly predictable and similar manner
across differ- ent individuals. This predictability is not limited to sensory
systems: shared activity across people has also been observed in higher-order
brain regions (for example, the default mode network6, or DMN) dur- ing the
processing of semantically complex real-life stimuli such as movies and
stories7–13. Notably, shared responses in these high-order areas seem to be
associated with narrative content and not with the physical form used to convey
it11,14,15. It is unknown, at any level of the cortical hierarchy, to what
extent the similarity of human brains during shared perception is recapitulated
during shared recollection. This prospect is made especially challenging when
recall is spontane- ous and spoken, and the selection of details is left up to
the remem- berer (rather than the experimenter), as is often the case in real
life'' \citep{chen2017shared}.



\subsubsection{Create CMS from other study}

\todo[inline]{a.k.a. ``more subjects''}
%
Create a CMS from another experiment's data,
using another scanner and hopefully more subjects
%
In case of an alignment of time series,
that experiment needs, at least, a part of Forrest Gump as an intersection


\subsubsection{Other functional alignment algorithms}

\todo[inline]{cf. \citep{bazeille2021empirical}}

\todo[inline]{Multimodal Surface Matching (Robinson et al. 2014)?}

``As we use inter-subject decoding to compare functional alignment methods, we
only consider methods that meet the following two criteria. First, the alignment
transformations should be learned on activations evoked during temporally
synchronized (i.e., co-occuring) task data, or on contrasts matched across
individuals. Second, the learned transformations must be invertible or almost
invertible linear mappings and applicable as-is on unseen data with a different
task structure. These two criteria exclude several methods currently used in the
literature such as regularized canonical correlation analysis (rCCA; Bilenko and
Gallant, 2016), gradient hyperalignment (Xu et al., 2018), connectivity
hyperalignment (Guntupalli et al., 2018), and methods based on Laplacian
embeddings (Langs et al., 2014)'' \citep{bazeille2021empirical}.

``Nonetheless, it remains unclear how researchers should choose among the
available functional alignment methods for a given research application''
\citep{bazeille2021empirical}.


\subsubsection{ROI vs. searchlight}

\todo[inline]{searchlight SRM \citep{zhang2016searchlight}}

\todo[inline]{Hyperalignment searchlight paper?}

%
Our union of PPA is XX voxels big.
%
Influence of voxel count on performance; take searchlight (but how big should it
be?

% Guntupalli's searchlight paper
Later, \citet{guntupalli2016model} showed that hyperalignment can be extended to
predict functional organization across large proportions of the cortical
surface, for example to predict the represented visual field coordinate in
visual cortex based on retinotopic mapping scans of other individuals.

``Applying SRM to a large swath of the brain means that all voxels within the
region contribute to the final derived metric. This can conflict with the goal
of associating spatially local activity with specific cognitive functions. To
address such issues, SRM can be applied in small overlapping searchlights to
obtain localized metrics of shared information [71,99]
\citep{cohen2017computational}''.

``As piecewise alignment is learned within a parcellation, an important question
is: which brain atlas should be used for piecewise alignment? In Supplementary
Figure S2 we compare results from the Schaefer et al. (2018) atlases to those
from parcellations derived directly on the alignment data. By default, the
results presented piecewise alignment is learned within a parcellation, an
important question is: which brain atlas should be used for piecewise alignment?
In Supplementary Figure S2 we compare results from the Schaefer et al. (2018)
atlases to those from parcellations derived directly on the alignment data. By
default, the results presented below are derived with the 300 ROI parcellation
of the Schaefer atlas unless noted otherwise. In the case of searchlight
Procrustes, we selected searchlight parameters to match those used in Guntupalli
et al. (2016); that is, each searchlight had 5 voxel radius, with a 3 voxel
distance between search- light centers. All searchlight analyses were
implemented using PyMVPA [Hanke et al., 2009].ted below are derived with the 300
ROI parcellation of the Schaefer atlas unless noted otherwise. In the case of
searchlight Procrustes, we selected searchlight parameters to match those used
in Guntupalli et al. (2016); that is, each searchlight had 5 voxel radius, with
a 3 voxel distance between search- light centers. All searchlight analyses were
implemented using PyMVPA (Hanke et al., 2009)'' \citep{bazeille2021empirical}.

``An alternative scheme, piecewise alignment (Bazeille et al., 2019), uses
non-overlapping neighborhoods either learnt from the data using a parcellation
method—such as k-means—or derived from an a priori functional or anatomical
atlas. Local transforms are derived in each neighborhood and concatenated to
yield a single large-scale transformation. Unlike searchlight, this returns a
transformation matrix with the desired regularities. This framework might induce
staircase effects or other functionally-irrelevant discontinuities in the final
transformation due to the underlying boundaries'' \citep{bazeille2021empirical}.

``To align the entire cortex across subjects, two main frameworks have been
proposed: searchlight and piecewise aggregation schemes. Each of these
frameworks use functional alignment methods to learn local transformations and
aggregate them into a single large-scale alignment; however, search- light and
piecewise differ in how they aggregate transforms, as illustrated in Figure 2.
The searchlight scheme [Kriegeskorte et al., 2006], popular in brain imaging
[Guntupalli et al., 2018; 2016], has been used as a way to divide the cortex
into small overlapping spheres of a fied radius. This method allows researchers
to remain agnostic as to the lo- cation of functional or anatomical boundaries,
such as those suggested by parcellation-based approaches. A local transform can
then be learnt in each sphere and the full alignment is obtained by aggregating
[e.g. summing as in Guntupalli et al., 2016 or averaging] across overlapping
transforms. Importantly, the aggregated transformation produced is no longer
guaranteed to bear the type of regularity (e.g orthogonality, isometry, or
diffeomorphicity enforced during the local neighborhood fit''
\citep{bazeille2021empirical}.


\subsubsection{Time series vs connectivity-based}

\todo[inline]{kind of a killer cause you do not need intersection of
time series; but s. Guntupalli's paper: time series hyperalignment outperforms
connectivity-based hyperalignment (?)}

%
``Hyperalignment models shared information that is embedded in idiosyncratic
cortical patterns across brains. Modeling shared information makes it possible
to compare functional anatomy across brains at a fine spatial scale.
Hyperalignment projects cortical pattern vectors into a common, high-dimensional
information space [Haxby et al., 2020]. Derivation of this common space can be
based on either neural response profiles (e.g. data collected during tasks, such
as movie viewing (Haxby et al., 2011)) or functional connectivity profiles files
[Guntupalli et al., 2018]'' \citep{busch2021hybrid}.

``Estimating the parameters to transform high-dimensional spaces from individual
brains into a common high-dimensional space requires a rich set of data that
samples a wide variety of cortical patterns in order to generalize to novel
stimuli or tasks. For response hyperalignment, a rich variety of stimuli or
conditions are necessary to sample the response vector space. For connectivity
hyperalignment, the sampling of connectivity vector space is defined by the
selection of connectivity targets, but the richness and reliability of
connectivity estimates depends on the variety of brain states over which
connectivity is estimated'' \citep{haxby2020hyperalignment}.

%
``Results showed that both CHA and RHA increased ISCs and bsMVPC classification
accuracies significantly over anatomy-based alignment, but each algorithm
achieves better alignment for the information that it uses to derive a common
model, namely connectivity profiles and patterns of response, respectively. ISCs
of connectivity profiles are significantly higher in a common model based on CHA
than in a common model based on RHA (ROI mean ISCs = 0.67 and 0.575,
respectively; CHA-RHA difference = 0.095 [0.081, 0.112])(S1 Fig). By contrast,
RHA marginally but significantly outperforms CHA on some validations based on
response tuning functions, namely ISCs of representational geometry (ROI means =
0.322 and 0.308, respectively; RHA-CHA difference = 0.014 [0.007, 0.019])(S2
Fig), and bsMVPC of movie segments (ROI mean accuracies = 13.65% and 10.37%,
respectively; RHA-CHA differ- ence = 3.28\% [2.76%, 3.78%])(S3 Fig)
\citep{guntupalli2018computational}.

% from project proposal
From project proposal: ``In his doctoral thesis, recently submitted to the
Faculty of Natural Sciences in Magdeburg, Falko Kaule showed that congruent
time-locked BOLD responses across subjects (i.e. all subjects watching the exact
same full-length movie) as used by Haxby and colleagues are not required to
derive a valid alignment of individuals with a common representational space
\citep{kaule2017examination}.
%
Comparable prediction performance can be achieved by using \textbf{functional
connectivity patterns} (correlation of a voxel's time series with reference
regions in the same brain)'' (s. dissertation project proposal).
%
It should be noted that the number of voxels that can be considered
simultaneously for functional BOLD response time series alignment is limited by
the number of timepoints in the calibration scan (about 300-400 voxels for a
15min scan with a 2s TR, corresponding to a local cortical neighborhood of about
1cm in diameter for a standard resolution).
%
This limitation does not exist in this form for a functional alignment that is
based on connectivity vectors.
%
The length of these connectivity vectors is determined by the number of
reference (or seed) regions in the brain.

\todo[inline]{wtf did \citep{nastase2019leveraging} do?}

% Nastase's ugly mofo paper
``Finally, estimating the SRM from functional connectivity
data rather than response time series circumvents the need for a single shared
stimulus across subjects; connectivity SRM allows us to derive a single shared
response space across different stimuli with a shared connectivity profile
\citep{nastase2019leveraging}'' \citep{kumar2020brainiak}.


\subsubsection{Individual residuals?}

\todo[inline]{We have transformation matrices for individual mapping; but just
onto shared responses}

%
``Furthermore, in cases where each subject's unique response is of more interest
than the shared signal, SRM can be used to factor out the shared component
thereby isolating the idiosyncratic response for each subject [13]''
\citep{kumar2020brainiak}.

From \citep{cohen2017computational}: ``The flip side of focusing on shared
responses is to focus on responses that are idiosyncratic to individuals.
Although these responses are excluded in SRM, they are not necessarily noise and
may in fact be highly reliable within participants.  SRM can be used to isolate
participant-unique responses by examining the residuals after removing shared
group responses, or it can be applied hierarchically to the residuals to
identify subgroups \citep{chen2017shared} \citep{cohen2017computational} [this
other ``Chen'' is fucking up the aethestics of references].  More generally,
there is a growing trend toward investigating individual differences as another
source of meaningful variance in fMRI [73].  Recognizing that signal exists
beyond the average or shared response of a group, such studies exploit
idiosyncratic but stable responses to account for previously unexplained
variance in brain function, behavioral performance and clinical measures
[70,74]'' \citep{cohen2017computational}.

\todo[inline]{but extension of SRM: ``Capturing Shared and Individual
Information in fMRI Data'' \citep{turek2018capturing}}


\subsection{Vision}

\todo[inline]{mih: das Kapitel braucht aus meiner Sicht keinen eigenen
"Outlook"; kommt eh direkt danach für die ganze Arbeit \&  ist da viel
interessanter}

\todo[inline]{IBC nennen?}


\subsubsection{Multiple localizer}

``Calibration scan'' to align to \ac{cms} and atlas of reference group.
%
From project proposal: ``Once a valid alignment is established, known functional
properties of a (normative) reference, derived from extensive scans and analysis
of other subjects, can then be projected into the respective individual voxel
space (s.  Fig. 1 in \citep{nishimoto2016lining})''.

%
``Movies engage multiple brain systems in parallel. From a single movie dataset
multiple functional topographies can be estimated [Guntupalli et al., 2016],
whereas different localizers are typically required to map different functional
topographies, making a thorough mapping of selective topographies time-consuming
and inefficient'' \citep{jiahui2020predicting}.

\todo[inline]{cf. general introduction}

\paragraph{Validity \& generalizability}

\todo[inline]{generalizability: scan once, estimate many}
% from project proposal
Naturalistic stimuli promise an ``increased validity of derived transformation
for functional alignment by sampling a more diverse set of mental states that
reflect (confound) statistics of the natural environment, and enable
investigation of the acquired data for a variety of research questions (e.g.
visual or auditory perception, spatial cognition; emotion; music, speech or
social perception)'' [project proposal].

% from Haxby 2020
``Estimating the parameters to transform high-dimensional spaces from individual
brains into a common high-dimensional space requires a rich set of data that
samples a wide variety of cortical patterns in order to generalize to novel
stimuli or tasks. For response hyperalignment, a rich variety of stimuli or
conditions are necessary to sample the response vector space. For connectivity
hyperalignment, the sampling of connectivity vector space is defined by the
selection of connectivity targets, but the richness and reliability of
connectivity estimates depends on the variety of brain states over which
connectivity is estimated'' \citep{haxby2020hyperalignment}.


\paragraph{additional benefit: compliance}
%
``As a rule of thumb, SRM will improve sensitivity for detecting a cognitive
process of interest in the test data if the training stimuli or trials strongly
and variably engage that process in a way that is reliable across participants''
\citep{cohen2017computational}.

%
``Movies are more engaging and result in better compliance
\citep{vanderwal2015inscapes}. Movie viewing can also be used in subject
populations, such as children \citep{richardson2018development} or patients,
that may have trouble maintaining attention during repetitions of a tedious
localizer task'' \citep{jiahui2020predicting}.

%
``Movies engage multiple brain systems in parallel. From a single movie dataset
multiple functional topographies can be estimated (Guntupalli et al., 2016),
whereas different localizers are typically required to map different functional
topographies, making a thorough mapping of selective topographies
time-consuming and inefficient. Movies also simulate better the statistics of
natural viewing and listening and may provide more ecologically valid maps.
Analogously, the introduction of dynamic videos of faces and control categories
to localize face-selective topographies provides more reliable maps and better
estimate the extent of face-selective regions than do localizers with still
image stimuli (Fox et al., 2009; Pitcher et al., 2011). Similarly, naturalistic
stimuli may better sample the full range of responses to faces and other stimuli
that contribute to face-selective topographies'' \citep{jiahui2020predicting}.

\paragraph{in general}

Last paragraph of \citet{jiahui2020predicting}'s introduction: ``These results
lay a foundation for building a computational tool with a database that could
allow others to map multiple functional topographies in new subjects using only
data collected during movie viewing. Functional localizers are inefficient
because they only estimate one or a few topographies for each localizer. Movies,
by contrast, engage in parallel multiple neural systems for vision, audition,
language, person perception, social cognition, and other functions.
Consequently, movies have the potential to estimate selective topographies in
all of these do- mains. Such a tool would require a database of data for movies
and a range of functional localizers in a normative group of subjects. A new
subject’s functional topographies could be estimated based only on that
subject’s movie data and other subjects’ localizer data from the normative
database that could be projected into that subject’s cortical anatomy using
hyperalignment transformation matrices derived from movie data. Such a resource
would be more efficient and replace tedious functional localizers with an
engaging movie and could enable mapping of multiple functional topographies with
data from a single fMRI using a naturalistic stimulus''
\citet{jiahui2020predicting}.

``The algorithm also can be applied to simpler, con- trolled experimental data,
but our previous results showed that the sampling of response vectors from these
experiments is impoverished and produces a model representational space that
does not generalize well to new stimuli in other experiments (Haxby et al.
2011). Results show that the computational principles underlying this common
model have broad general validity for representational spaces in occipital,
temporal, parietal, and frontal cortices'' \citep{guntupalli2016model}.



\subsubsection{Functional atlas}

\todo[inline]{see also \citep{bazeille2019local}; who is quoting him?}


%
``While the functional alignment can also be applied to fMRI data from
stimulation paradigms with simplified stimuli, the transformations for
functional alignment have greatly diminished general validity
\citep{haxby2011common}, presumably because such experiments sample a sparser
range of brain states \citep{guntupalli2016model}'' [project proposal].

%
Moreover, naturalistic stimuli sample a broader range of brain states paradigms
with simplified stimuli \citep{guntupalli2016model, haxby2011common} promising
an increased validity of transformations of functional alignment and increased
generalizability to [research questions/domains/paradigms].

%
Naturalistic stimuli promise an ``increased validity of derived transformation
for functional alignment by sampling a more diverse set of mental states that
reflect (confound) statistics of the natural environment, and enable
investigation of the acquired data for a variety of research questions (e.g.
visual or auditory perception, spatial cognition; emotion; music, speech or
social perception)'' [project proposal].

``This result suggests that the validity for a model of a specific subspace may
be enhanced by designing a stimulus paradigm that samples the brain states in
that subspace more extensively'' \citep{haxby2011common}.

``hyperalignment of data using a set of stimuli that is less diverse than the
movie is effective, but the resultant common space has validity that is limited
to a small subspace of the representational space in VT cortex''
\citep{haxby2011common}.

``Initially, the common space produced with hyperalignment has the same number
of dimensions as the number of voxels in each individual’s native space. We
asked how many distinct common response-tuning functions are needed to contain
the information that affords the full range of fine-grained distinctions among
complex, visual stimuli. We tested the sufficiency of lower-dimensional
subspaces and found that BSC accuracies continued to increase with more than 20
common response- tuning functions. We present a 35-dimensional common model
space that afforded BSC for all three experiments at levels of accuracy that
were equivalent to BSC with all 1,000 hyperaligned dimensions or WSC with 1,000
voxels. Ten dimensions were sufficient within the limited stimulus domains of
each category perception experiment, but these sets of ten dimensions did not
afford high levels of BSC for the other experiment or for the movie. Thus, these
lower-dimensional models are subspaces of the full model and are valid only for
more limited stimulus domains'' \citep{haxby2011common}.


``Estimating the parameters to transform high-dimensional spaces from individual
brains into a common high-dimensional space requires a rich set of data that
samples a wide variety of cortical patterns in order to generalize to novel
stimuli or tasks. For response hyperalignment, a rich variety of stimuli or
conditions are necessary to sample the response vector space. For connectivity
hyperalignment, the sampling of connectivity vector space is defined by the
selection of connectivity targets, but the richness and reliability of
connectivity estimates depends on the variety of brain states over which
connectivity is estimated'' \citep{haxby2020hyperalignment}.

%
From project proposal: ``Moreover, beyond the scope of this project the targeted
homogenization of acquisition procedures will further the goal of large scale
data collection for the purpose of producing a normative reference of brain
function as measured by fMRI''.

%
From project proposal: ``The availability of such a reference would enable
quantitative and qualitative description of an individual's brain function with
respect to such a norm, and consequently progress the field towards neuroimaging
studies of individual differences that more closely resemble their psychological
counterparts''.

%
From talk at INM-7 seminar: ``Imagine you scan a new, unknown subject for just
15 minutes more, and you additionally get results from a whole variety of other
paradigms mapped onto that brain: results from localizers of low-level
perceptual processes, but also higher-level cognitive processes like language,
memory, emotions and so on. And kinda ``adventurous'': If you have different
common model spaces for different subgroups, you can investigate which alignment
onto which ``subgroup common model space'' results in less error, that lets you
classify to which subgroup your new subject might belong''.

%
From \citep{jiahui2020predicting}: ``results lay a foundation for building a
computational tool with a database that could allow others to map multiple
functional topographies in new subjects using only data collected during movie
viewing. Functional localizers are inefficient because they only estimate one or
a few topographies for each localizer. Movies, by contrast, engage in parallel
multiple neural systems for vision, audition, language, person perception,
social cognition, and other functions. Consequently, movies have the potential
to estimate selective topographies in all of these domains. Such a tool would
require a database of data for movies and a range of functional localizers in a
normative group of subjects. A new subject's functional topographies could be
estimated based only on that subject's movie data and other subjects’ localizer
data from the normative database that could be projected into that subject’s
cortical anatomy using hyperalignment transformation matrices derived from movie
data. Such a resource would be more efficient and replace tedious functional
localizers with an engaging movie and could enable mapping of multiple
functional topographies with data from a single fMRI using a naturalistic
stimulus'' \citep{jiahui2020predicting}.

``Characterizing this functional variability, particularly when considering the
genetic level, ideally requires acquiring functional imaging data from hundreds
of sub- jects and organizing these data into a large-scale database, together
with genetic, behavioral and biomorphological data. Databasing and analysis of
structural magnetic reso- nance images has already resulted in probabilistic
ana- tomical atlases [Toga, 2001, Probabilistic approaches; Van Essen, 2002,
Windows on the brain]. However, a similar large scale description of
functional networks is still lacking'' \citep{pinel2007fast}.


\subsection{Conclusion}


\section{Data Availability}

\todo[inline]{all from PPA-Paper but with new GIN link leading to an empty repo}

% \href{https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-analysis}}

% new; PPA analysis
All fMRI data and results are available as Datalad \citep{halchenko2021datalad}
datasets, published to or linked from the \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).
% original
Raw data of the audio-description, movie and visual localizer were originally
published on the \emph{OpenfMRI} portal
(\url{https://legacy.openfmri.org/dataset/ds000113}; \citep{Hanke2014ds000113},
\space \url{https://legacy.openfmri.org/dataset/ds000113d};
\citep{hanke2016ds000113d}).
% visual localizer
Results from the localization of higher visual areas are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-visualrois}{\url{github.com/psychoinformatics-de/studyforrest-data-visualrois}}).
% raw data
The realigned participant-specific time series that were used in the current
analyses were derived from the raw data releases and are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned}{\url{github.com/psychoinformatics-de/studyforrest-data-aligned}}).
% OpenNeuro
The same data are available in a modified and merged form on OpenNeuro at
\url{https://openneuro.org/datasets/ds000113}.
% NeuroVault for z-maps of SRM
Unthresholded $Z$-maps of all contrasts can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}.


\section*{Code Availability}

Scripts to generate the results as Datalad \citep{halchenko2021datalad} datasets
are available in a \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).
