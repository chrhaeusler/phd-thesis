\section{Introduction}

\todo[inline]{mih: Dieses Kapitel sollte konzeptuell und inhaltlich deutlich als
"und jetzt alles zusammen" erscheinen}

\todo[inline]{what about prediction of "auditory PPA" localized in PPA-Paper?}

\todo[inline]{Definition of functional topography, functional anatomy}

\todo[inline]{Definition of PPA isn't explicitly given (cf. PPA-Paper)}

% brain mapping
Neuroimaging studies have parcellated the cerebral cortex into distinctive
\textit{functional areas} by topographically mapping brain functions, perceptual
(e.g. perception of different object categories; \citet{kanwisher1997ffa}) or
cognitive processes (e.g. theory of mind; \citet{spunt2014validating}) on to the
brain's anatomy.
\todo[inline]{give other examples; these are individual-level studies!}
% higher visual areas
For example, previous studies presented isolated \textit{higher-level visual
categories} such as pictures of scenes, faces, human bodies or tools to
investigate brain regions whose statistically increased \ac{bold} activity is
correlated with the perception of a specific stimulus type.
% results (mostly group average studies)
Results suggest that category-specific brain regions like the \ac{ppa}
\citep{epstein1998ppa}, \ac{ffa} \citep{kanwisher1997ffa}), the occipital face
area \ac{ofa} \citep{pitcher2011occipitalfacearea}, the \ac{eba}
\citep{downing2001bodyarea}), and the \ac{loc} \citet{malach1995loc} exist in
the human brain.
%
Nevertheless, typical analysis procedures of neuroimaging studies today average
(voxel-wise) data of at least 10-15 subjects to improve the \ac{snr}.
%
Consequently, studies employing an averaging approach probably just ``capture
the common denominator of each individual cognitive circuit and lose a large
amount of information'' \citep{pinel2007fast}.
%
However, ``interpretation of fMRI data at the level of individual brains is
essential for characterizing brain function in health and disease''
\citep{dubois2016building}.


\subsection{Functional localizer}
% example: higher functional areas
For example, the ``topographies of category-selective areas are mostly
distributed similarly across individuals, but great individual variability
exists in the locus, the size, and the shape of the category-selective areas
[Zhen et al., 2015, 2017]'' \citep{jiahui2020predicting}.
%
``To deal with the idiosyncratic topography of functional areas,
category-selective areas are identified separately in each individual using a
``functional localizer'' fMRI scan'' \citep{jiahui2020predicting}.
% localizer
Traditionally, location, size, and shape of functional areas are determined by
letting study participants perform a task during a \textit{functional localizer}
paradigm.
%
``Functional localizers use a simple contrast between responses to different
categories of stimuli, such as responses to faces versus responses to objects,
to identify category-selective areas or to map a category-selectivity topography
[Saxe et al., 2006]'' \citep{jiahui2020predicting}.


\subsubsection{The problem (with functional localizers)}

% problem: one localizer for one domain
Functional localizers are designed to maximize detection power and thus
dedicated to map just one domain of brain functions.
% which gets messy
Consequently, if a researcher or practitioner wants to map a variety of domains,
the approach ``one paradigm in order to map one domain of functions'' gets
time-consuming and inefficient.
% localizer batteries: intro
Researchers have tried to tackle this issue by creating time-efficient
multi-functional \textit{localizer batteries} \citep{barch2013function,
drobyshevsky2006rapid, pinel2007fast}.
% localizer batteries: example
For example, \citet{pinel2007fast} employs a range of dedicated stimuli and
specific tasks in a 5-minute routine to map processes of ``auditory and visual
perception, motor actions, reading, language comprehension, and mental
calculation at an individual level'' \citet{pinel2007fast}.
% task based = shit
Nevertheless, the diagnostic quality of localizer batteries relies heavily on
the participant's comprehension of the task instruction and general compliance,
a criterion that can be difficult to meet in clinical or pediatric populations.


\subsection{The solution: estimation from reference data}

\todo[inline]{there is ``real'' prediction from anatomy (e.g.  cortical
folding) not using anatomical template to project data into/out of}

% prediction: general idea
One approach to reduce time and costs of individual diagnostics is to estimate
the result of a functional localizer/the functional topography in one person
based on data collected in a reference group (e.g. \citet{weiner2018defining}).


\subsubsection{Anatomical alignment}
% definition: antomical alignment
In a first step, an \textit{anatomical alignment} is performed to resolve
anatomical variability (e.g. brain size or cortical folding) across persons by
(usually) warping a person's structural and functional data to a \textit{common
anatomical space}.
% volume-based
For example, volume-based anatomical alignment (s. \citep{klein2009evaluation}
for a review) aligns voxel-wise data of individual subjects to a
three-dimensional brain template (e.g. MNI152 template;
\citep{fonov2011unbiased}).
% surface-based
Surface-based anatomical alignment \citep{fischl1999cortical} aligns vertex-wise
data of individual subjects to a two-dimensional template (e.g. FreeSurfer
fsaverage template; \citep{fischl1999high}).
% estimation
In a second step, the most probable functional topography (i.e. anatomical
location, size, and shape of a functional area as in \citep{weiner2018defining})
of the new subject is estimated by mapping/projecting averaged functional data
of the reference group through the common anatomical space (i.e. brain template)
into the subject's subject-specific anatomy.

From \citep{saxe2006divide}: ``More sophisticated strategies for registering
individual brains together use reconstructed cortical surfaces that respect
sulcal locations. These systems provide better alignment across subjects for
retinotopic visual areas (Fischl et al., 1999), but they do not do much better
than Talairach co-ordinates for category-selective regions in the temporal lobe
(Spiridon et al., 2005).  Moreover, post-mortem histology shows that sulcal
borders do not reliably coincide with other, well-established anatomical
divisions based on cytoarchitecture (Amunts and Zilles, 2001; Amunts et al.,
1999)'' \citep{saxe2006divide}.

%
``Thus, neither standard stereotaxic registration methods, nor more
sophisticated coordinate systems that respect sulcal landarks, are likely to
bring distinct functional regions perfectly into register across subjects. As a
result, nearby brain regions with different functional profiles will be averaged
together across individuals, reducing both the resolution and sensitivity of
subsequent functional analyses (Swallow et al., 2003; see also Fig. 2)''
\citep{saxe2006divide}.



\paragraph{Functional-anatomical correspondence}

\todo[inline]{shorten and rephrase}

% definition: functional-anatomical correspondence
However, ``[fine-grained] functional organization of cortex is not
well-conserved across individuals. As a result, individual differences in
cortical functional architecture are confounded by topographic idiosyncrasies
--- i.e., differences in functional--anatomical correspondence''
\citep{feilong2018reliable}.
%
``Functional architecture of the human brain is relatively consistent across
individuals at a coarse scale, but idiosyncrasies in functional topography
become increasingly apparent at finer scales''\citep{feilong2018reliable}.
% example category-selective areas
``At the areal level, category-selective regions can be localized to anatomical
landmarks [Weiner et al., 2018, 2014], though the locus can differ across
individuals by millimeters or centimeters, along with variability in size and
shape [Zhen et al., 2017, 2015]'' \citep{feilong2018reliable}.

%
``With state-of-the-art cortical surface-based alignment [Fischl, 2012], the
mismatch between brain function and anatomy can be reduced but not eliminated
[Duncan et al., 2009; Frost and Goebel, 2012; Weiner et al., 2018]. Therefore,
it is problematic to assume that a given anatomical location or topographic
conformation will have the same functional role across brains''
\citep{feilong2018reliable}.

%
``Anatomical variability and limited structure-function correspondence across
cortex [Paquola et al., 2019, Microstructural and functional gradients;
Vázquez-Rodríguez et al., 2019, Gradients of structure–function tethering...]
make this goal challenging [Rademacher et al., 1993, Topographical Variation of
the Human Primary Cortices; Thirion et al., 2006, Dealing with the shortcomings
of spatial normalization]'' \citep{bazeille2021empirical}.

%
``Even after state-of-the-art anatomical normalization to a standard space, we
still observe differences in individual-level functional activation patterns
that hinder cross-subject comparisons [Langs et al., 2010, Functional geometry
alignment and localization; Sabuncu et al., 2010, Function-based intersubject
alignment]'' \citep{bazeille2021empirical}.
%
``With standard processing pipelines, it is therefore difficult to disentangle
whether individuals are engaging in idiosyncratic cognitive experience or if
they are engaging in shared functional states that are differently encoded in
the supporting cortical anatomy'' \citep{bazeille2021empirical}.

%
Such registration is appropriate for subcortical structures which are inherently
volumetric; by contrast, the cortex is a 2D structure and volumetric alignment
does not properly align folding patterns across subjects. Although switching to
cortical folding-based inter-subject alignment [Fischl et al., 1999, Inflation,
flattening; Yeo et al., 2010, Spherical demons] has been shown to somewhat
reduce functional mismatch [Klein et al., Evaluation of volume-based and
surface-based...; Frost et and Goebel, 2012, Measuring structural-functional
correspondence] [but see Langers, 2014, Assessment of tonotopically
organised...], this has not yet become common practice''
\citep{dubois2016building}.


\paragraph{Summary of anatomical alignment}

%
``Variability in functional-anatomical correspondence across individuals means
that even high-performing anatomical alignment does not ensure fine-grained
functional alignment [e.g., \citet{frost2012measuring}]''
\citep{kumar2020brainiak}.

% interim summary
In summary, ``anatomical alignment methods aim at solving the anatomical
structure problem using specific anatomical features for alignment [1, 2, 3].
Yet, they fail to align functional topographies satisfactorily [Mazziotta et
al., 2001, A probabilistic atlas and reference system; Brett et al., 2002, The
problem of functional localizer]'' \citep{turek2017semi}.


\subsubsection{Functional alignment (FA)}

\todo[inline]{is additional step after ``rough'' anatomical alignment}


\paragraph{intro}

\todo[inline]{cite reviews}

% intro
Since anatomical alignment addresses the issue of anatomical variability but not
functional variability across subjects, different methods of \textit{functional
alignment} have been developed in order to preserve functional
idiosyncrasies/topographies.

% definition
In principle, functional alignment procedures align brain activation patterns
(time series or connectivity-patterns) based on their similarity to a
high-dimensional \textit{common functional space}.
%
Functional alignment algorithms align cortical anatomy of different subjects
based on the maximization of the inter-subject similarity of \ac{bold} responses
\citep{haxby2011common, chen2015reduced, sabuncu2010function} correlating with a
time-locked external stimulation (e.g. movie or auditory narrative), or
connectivity profiles \citep{feilong2018reliable, guntupalli2018computational}.

% examples of algorithms
EXAMPLES of algorithms: ``From the initial introduction of hyperalignment in
Haxby et al.  2011, the range of associated methods has grown to include Shared
Response Modelling [SRM; Chen et al., 2015] and Optimal Transport [Bazeille et
al., 2019] with many variations thereof [see e.g. Xu et al. 2018; Yousefnezhad
and Zhang 2017, among others] \citep{bazeille2021empirical}''.

%
``Functional alignment algorithms that are based on common temporal response
profiles such as Hyperalignment [Haxby, 2011; Guntupalli, 2016] and the Shared
Response Model (SRM) [Chen, 2016] ``attempt to remap regions across participants
and can be highly effective at improving spatial alignment across people based
on common functional responses while still maintaining individual differences
[\citep{feilong2018reliable}]'' \citep{chang2021endogenous}.


\paragraph{Application: estimation of functional areas}

\todo[inline]{that's the only use case now that gets mentioned}

\todo[inline]{\citep{haxby2011common} used localizer for category decoding from
VT (in voxel space)}

\todo[inline]{\citep{guntupalli2016model} in surface space; predict
individual-specific topography; s. Supplementary Fig. 8}

% intro
An alternative approach, to individual localization has been proposed by
\citet{haxby2011common}.
%
They (and e.g. \citet{jiahui2020predicting}) also predicted the location, form,
and size of target brain areas in medial ventral temporal cortex from dedicated
localizer scans of other individuals.
%
The key difference of this approach is to rely on similarity of representational
geometry of brain activity patterns and aligning individual brains into a
multi-dimensional functional group space, termed \ac{cms}.

%
For example, \citet{haxby2011common, guntupalli2016model} have shown that
``idiosyncratic topographies for category-selectivity and retinotopy can be
estimated in individual brains with high fidelity using hyperalignment to
project other subjects' functional localizer data into a target subject's
ventral temporal and occipital cortical anatomy'' \citep{jiahui2020predicting}.
%
``Our findings show that individually-tailored maps estimated from other
subjects’ data after hyperalignment correlate much more highly with maps
estimated from that subject's own localizer data than does a group average map
based on anatomical normalization'' \citep{jiahui2020predicting}.


\subsubsection{Hyperalignment}

\todo[inline]{imo, hyperalignment can be dropped almost completely}

%
Hyperalignment, an algorithm that was pioneered by \citet{haxby2011common}, uses
\ac{bold} response patterns to derive a \ac{cms} using a variant of Procrustes
analysis and computes invertible (orthonormal) transformations from each
individual brain's voxel-space into the \ac{cms}.

%
``Hyperalignment [Guntupalli et al., 2016, 2018; Haxby et al., 2011] is a family
of methods that can disentangle functional variability from anatomical
variability. Hyperalignment projects features (voxels or surface vertices) from
a brain into a common high-dimensional space through linear transformations. In
this common space, the same features from different individuals will share
similar functional properties instead of the same anatomical locations or
topographic conformations'' \citep{feilong2018reliable}.

%
``A major objective of the hyperalignment algorithm is to map the shared
information originally found in idiosyncratic cortical topographies into a
common space in which this information is better aligned across participants''
\citep{busch2021hybrid}.

%
``The dimensions of this common model capture functional profiles that are
shared across individuals such as cortical response profiles collected during a
common time-locked stimulus presentation (e.g. movie viewing) or functional
connectivity profiles'' \citep{busch2021hybrid}.

%
``Hyperalignment models shared information that is embedded in idiosyncratic
cortical patterns across brains. Modeling shared information makes it possible
to compare functional anatomy across brains at a fine spatial scale.
Hyperalignment projects cortical pattern vectors into a common, high-dimensional
information space [Haxby et al., 2020]. Derivation of this common space can be
based on either neural response profiles (e.g. data collected during tasks, such
as movie viewing (Haxby et al., 2011)) or functional connectivity profiles files
[Guntupalli et al., 2018]'' \citep{busch2021hybrid}.

%
''Hyperalignment decomposes the original fMRI data of
each individual into two parts: a transformation matrix, which reflects
topographic properties of the individual's functional activations; and a new
data matrix in the common space, which reflects shared, stimulus-driven
responses. This hyperaligned data matrix provides an opportunity to study brain
functions without confounds from topographic variability''
\citep{feilong2018reliable}.

%
``Hyperalignment derives individual transformation matrices to project
information encoded in idiosyncratic topographies into a common model
information space. These matrices are derived based either on responses to a
naturalistic stimulus, such as a movie, or on functional connectivity
\citep{guntupalli2018computational}'' \citep{jiahui2020predicting}.


\subsubsection{Shared response model}

\paragraph{Intro}

Another functional alignment algorithm is the Shared Response Model (SRM)
\citep{chen2015reduced, richard2019fast}.
%
The unsupervised probabilistic latent-factor model decomposes fMRI responses
time series of participants experiencing the same stimulus into a
lower-dimensional space of shared feature time series and subject-specific
orthogonal topographic transformation matrices \citep{kumar2020brainiak,
cohen2017computational}.


\paragraph{reduced-dimensional shared space}
%
``These shared features do not correspond to individual voxels; rather, they are
distributed across the full voxel space of each subject; each shared feature can
be understood as a weighted sum of many voxels'' \citep{kumar2020brainiak}.
%
``Estimating a lower-dimensional common model space can potentially aid in
filtering out measurement noise that is assumed to be independent across
individuals'' \citep{chang2021endogenous}.
%
``SRM isolates the shared response while accommodating misalignment across
subjects'' \citep{kumar2020brainiak} [yeah, but \citep{nastase2019leveraging}?].


\paragraph{transformation matrices}
%
The subject-specific transformation matrices perform a mapping from each
specific's idiosyncratic voxel space into the shared feature space
\citep{kumar2020brainiak, cohen2017computational}.
%
``Each of these topographic transformations effectively rotates and reduces each
subject's voxel space to find a subspace of shared features where the
multivariate trajectory of responses to the stimulus is best aligned''
\citep{kumar2020brainiak}.


\paragraph{the math shit}
%
More specifically, ``the brain data for each participant $i$ are represented as
an \textbf{$n$} voxel by $t$ time matrix  $X_{i}$. A pre-specified number of
features are used to learn a $k$ feature by  $t$ time shared space $S$ and a
participant-specific $n$ voxel by $k$  feature matrix $W_{i}$. This weight
matrix, reflecting the loading of voxels onto features, is randomly initialized
then fit over iterations to minimize the error in explaining participant data.
At the same time, the time course of the features in the shared space is
learned'' (from https://brainiak.org/tutorials/11-SRM/).

%
``SRM learns $N$ maps $W_{i}$ with orthogonal columns such that
$||X_{i}-W_{i}S||_{F}$ is minimized over $\left\{ W_{i}\right\} _{i=1}^{N},S$,
where $X_{i}\in\mathbb{R}^{v\times{T}}$ is the $i^{th}$ subject's fMRI response
($v$ voxels by $T$ repetition times) and $S\in\mathbb{R}^{k\times{T}}$ is a
feature time-series in a $k$-dimensional shared space''
\citep{vodrahalli2018mapping}.


\paragraph{why / how we use it}

\todo[inline]{cf. text in method section and put it here, too}


%
``Between-subject models with SRM can, in some cases, exceed the performance of
within-subject models because (a) the reduced-dimension shared space can
highlight stimulus-related variance by filtering out noisy or
non-stimulus-related features, and (b) the between-subject model can effectively
leverage a larger volume of data after functional alignment than is available
for any single subject'' \citep{kumar2020brainiak}.

%
``The learned subject-specific topographic bases can be used to project test
data into the shared space. This projection functionally aligns the test data''
\citep{kumar2020brainiak}.

% transpose
But: the transpose of the matrix can be used to map data from shared space into
the subjects anatomy.


\subsection{Naturalistic stimuli}

\todo[inline]{cf. general introduction}

\paragraph{increased accuracy of functional alignment}
%
While the functional alignment can also be applied to fMRI data from stimulation
paradigms with simplified stimuli, the transformations for functional alignment
have greatly diminished general validity \citep{haxby2011common}, presumably
because such experiments sample a sparser range of brain states
\citep{guntupalli2016model}.

%
Increased validity of derived transformation for functional alignment by
sampling a more diverse set of mental states that reflect (confound) statistics
of the natural environment, and enable investigation of the acquired data for a
variety of research questions (e.g. visual or auditory perception, spatial
cognition; emotion; music, speech or social perception)

%
``Naturalistic stimuli such as movies and stories are often used to generate
such training data, though any study design in which participants perform the
same sequence of trials --- or for which a common sequence can be spliced
together from the same set of trials --- could be used (for example, a battery
of cognitive tasks)'' \citep{cohen2017computational}


\paragraph{compliance}
%
``As a rule of thumb, SRM will improve sensitivity for detecting a cognitive
process of interest in the test data if the training stimuli or trials strongly
and variably engage that process in a way that is reliable across participants''
\citep{cohen2017computational}.

%
``Movies are more engaging and result in better compliance
\citep{vanderwal2015inscapes}. Movie viewing can also be used in subject
populations, such as children \citep{richardson2018development} or patients,
that may have trouble maintaining attention during repetitions of a tedious
localizer task'' \citep{jiahui2020predicting}.


\subsection{"Here, we..."}

\todo[inline]{we use pre-computed results $Z$-maps in voxel-space}

\todo[inline]{why did we use SRM? performance \citep{chen2015reduced,
bazeille2021empirical}, ``nicely'' implemented in BrainIAK}

\todo[inline]{probably faster, computationally less demanding than
hyperalignment; ``computational efficiency, as the latter is an important
consideration for scientists who may not have access to specialized hardware''
\citep{bazeille2021empirical}}

\todo[inline]{ROI = PPA; PPA should probably already be defined above}

% summary in one sentence
Here, we compare the estimation performance of localizing a scene-selective area
in one individual subject from other subjects' data based on anatomical
alignment to an estimation based functional alignment using the \ac{SRM}.
% creating the shared feature space
Following a leave-one-subject-out cross-validation, we created a shared feature
space based on publicly available fMRI data
\href{www.studyforrest.org}{studyforrest.org} of participants watching a
Hollywood \citep{hanke2016simultaneous} movie, listening to the movie's
audio-description \citep{hanke2014audiomovie}, and participating in a
traditional functional localizer paradigm \citep{sengupta2016extension}.

% project into common functional space
All other subjects' data of functional localizer are projected into the shared
feature space

% align left-out subject
Assuming that the naturalistic stimuli would trigger, among others, brain
responses that are similar to those triggered by the functional localizer, we
used time series data of the naturalistic stimuli to align the left-out subject
to the fixed common functional space.
%
Finally, we projected from the common functional space into a left-out subject's
cortical anatomy

% partial alignment
Critically, we also assess the relationship between the prediction performance
based on the functional alignment and the quantity of data used to calculate
subject-specific transformation matrices that project functional data into a
subject's anatomy.
%
We test which amount of data is needed to do an alignment to the common model
space that provides transformation matrices that outperform a prediction using
just an anatomical alignment.
%
Question: how much data do we need to align an individual subject to a/the
\ac{cms} and outperform a prediction based on anatomical alignment?


\paragraph{model validation}

\todo[inline]{maybe does not need to mentioned here}

Predict localizer results using times-series of localizer to calculate the
transformation matrices:
%
not cross-subject-cross-experiment-prediction but just cross-subject.


\paragraph{Cronbach's Alpha as reliability of localizer?}

\todo[inline]{cf. \citep{jiahui2020predicting}}


\subsection{Hypotheses}
%
We hypothesized that increased quantity of data used to calculate the
transformation matrices of left-out subjects for a functional alignment would to
increased prediction performance.
%
Further, we hypothesized that functional alignment based on ``sufficient''
quantity of data would eventually perform ``better'' than an estimation based on
anatomical alignment.\todo{Jesus! Phrasing!}


\subsection{Summary of results}

\todo[inline]{cf. \citep{haxby2011common, guntupalli2016model}}

%
``Results expands previous analysis from ventral temporal cortex to the whole
cortex, showing strong correlations of face-selectivity topographic maps derived
from a subject's own localizer data with maps derived from other subjects'
localizer data projected into that subject's cortical anatomy. Both the two-step
algorithm and the new one-step algorithm, which we introduce here, produce
high-fidelity, individualized topographic maps, but the new one-step algorithm
maps were superior'' \citep{jiahui2020predicting}.


\subsection{What it will be good for}

\todo[inline]{smallish text should be enough here; cf. (to be written)
discussion}




\section{Methods}

\todo[inline]{mih: braucht nicht wie Standalone-Paper geschrieben werden; statt
Kopien der "Participants" etc, einfach sagen, dass es die gleichen Daten sind
bzw. was abweicht; insbesondere keinen Overlap mit den anderen papers
herstellen}

\todo[inline]{mih: Eindruck vermeiden, dass das Kapitel aufgeblasen ist; wir
wollen, ist zu zeigen, wie es weiter geht und ging, nachdem Paper draussen
waren; optimaler Eindruck ist der eines Forschungsprogramms in voller Fahrt}

% we get the data from the naturalistic PPA paper (its subdataset)
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in\_bold3Tp2/sub-??\_task-a?movie\_run-?\_bold*.*

\todo[inline]{following phrasing is almost identical to PPA-Paper}


% input data
We spliced the data together: ``Naturalistic stimuli such as movies and stories
are often used to generate such training data, though any study design in which
participants perform the same sequence of trials—or for which a common sequence
can be spliced together from the same set of trials—could be used (for example,
a battery of cognitive tasks)'' \citep{cohen2017computational}

% reference to PPA-Paper
As in study 2, we
%
used components of the publicly available
\href{http://www.studyforrest.org}{studyforrest.org} dataset that has been
repeatedly used by other research groups in independent studies
(\citep[e.g.,][]{ben2018hippocampal, jiahui2020predicting, hu2017decoding,
lettieri2019emotionotopy, nguyen2016integration}).
% used studies
The same participants were
% VIS
a) participating in a dedicated six-category block-design visual localizer
\citep{sengupta2016extension}.
% AV
b) watching the audio-visual movie \citep{hanke2016simultaneous}, and
% AD
c) listening to the audio-description \citep{hanke2014audiomovie} of the movie
``Forrest Gump'',
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications. Following is a summary of the most important aspects.


\subsection{Participants}

\todo[inline]{following phrasing is almost identical to study2}
% VIS study
In the block-design localizer study \citep{sengupta2016extension}, 15
participants took part in a six-category block-design visual localizer.
% AV study
In the movie study \citep{hanke2016simultaneous}, the same 15 participants
(21–39 years, mean age 29.4, six female), watched the audio-visual movie
``Forrest Gump'' \citep{ForrestGumpMovie} with dubbed German audio track
\citep{ForrestGumpDVD}.
% AD study
In the audio-description study \citep{hanke2014audiomovie}, 20 German native
speakers (all right-handed, age 21–38 years, mean age 26.6 years, 12 male), of
whom 15 participants also took part in the localizer and movie study, listened
to the German audio-description \citep{ForrestGumpGermanAD} of the movie.
% participants' health
All participants reported to have normal hearing, normal or corrected-to-normal
vision, and no known history of neurological disorders.
% compensation, consent and shit
In all studies, participants received monetary compensation and gave written
informed consent for their participation and for public sharing of obtained data
in anonymized form. The studies had prior approval by the Ethics Committee of
Otto-von-Guericke University of Magdeburg, Germany.


\subsection{Stimuli and procedure}

\todo[inline]{following phrasing is almost identical to PPA-Paper}

\subsubsection{Functional localizer}

% VIS study picture categories
Stimuli for the block-design localizer study were 24 unique grayscale images of
faces, bodies, objects, houses, outdoor scenes and scrambled images, matched in
luminance and size, that were previously used in other studies
\citep[e.g.,][]{haxby2011common}.
% procedure: presentation & instructions
Participants performed a one-back image matching task for four block-design
runs, with two \unit[16]{s} blocks per stimulus category in each run.
%
For details on stimulus creation and presentation see
\citet{sengupta2016extension}.


\subsubsection{Naturalistic stimuli}
% AD & AV stimulus name & references
The German DVD release \citep{ForrestGumpDVD} of the movie ``Forrest Gump''
\citep{ForrestGumpMovie} and its temporally aligned audio-description
\citep{ForrestGumpGermanAD} served as naturalistic stimuli, with an approximate
duration of two hours, split into eight consecutive segments of
\unit[$\approx$15]{minutes}.
% AD: additional narrator
The audio-description adds another male narrator to the voice-over narration of
the main character Forrest Gump. This additional narration describes essential
aspects of the visual scenery when there is no off-screen voice, dialog, or
other relevant auditory content.
% task
For all sessions with naturalistic stimuli, participants were instructed to
inhibit physical movements except for eye-movements, and otherwise to simply
``enjoy the presentation''.
%
For details on stimulus creation and presentation see
\citet{hanke2014audiomovie, hanke2016simultaneous}.


\subsection{Stimulation setup}

\todo[inline]{following phrasing is almost identical to PPA-Paper}

% instructions and distance; screen size \unit[23.75 $\times$ 10.25]{cm}
In the block-design localizer, movie study, and audio-description study, visual
instructions were presented on a rear-projection screen inside the scanner bore
at a viewing distance of \unit[63]{cm}.
%
Stimulus images of the localizer paradigm were displayed at a size of
approximately \unit[10]$^{\circ}$ $\times$ \unit[10]$^{\circ}$ of visual angle.
%
Movie frames of the movie paradigm were displayed at a size of approximately
\unit[21.3]$^{\circ}$ $\times$ \unit[9.3]$^{\circ}$.
% AD
During the functional scans of the audio-description study, the projector
presented a medium gray screen with the primary purpose to illuminate a
participant's visual field in order to prevent premature fatigue.
% AD & AV: auditory stimulation
Auditory stimulation was implemented using custom in-ear (audio-description), or
over-the-ear headphones (movie), which reduced the scanner noise by at least
\unit[20–30]{dB}.


\subsection{fMRI data acquisition}
%
\todo[inline]{following phrasing is almost identical to PPA-Paper}
% AV & VIS
In the block-design localizer and the movie study, a \unit[3]{Tesla} Philips
Achieva dStream MRI scanner with a 32 channel head coil acquired gradient-echo
fMRI data at \unit[2]{s} repetition time with
% slices
35 axial slices (thickness \unit[3.0]{mm}, \unit[10]{\%} inter-slice gap) with
\unit[80 $\times$ 80]{voxels} (\unit[3.0 $\times$ 3.0]{mm} of in-plane
resolution, \unit[240]{mm} field-of-view) and an anterior-to-posterior phase
encoding direction, recorded in ascending order.
% visual localizer: 4 x 156 TR
A total of 624 volumes were recorded for each participant across the four runs
of the visual localizer experiment.
%
A total 3599 volumes were recorded for each participant across the eight runs of
the movie study.

% AD
In the audio-description study gradient-echo fMRI data were acquired using a
\unit[7]{Tesla} Siemens MAGNETOM magnetic resonance scanner equipped with a 32
channel brain receive coil at \unit[2]{s} repetition time (TR) with 36 axial
slices (thickness \unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane
resolution, \unit[224]{mm} field-of-view, anterior-to-posterior phase encoding
direction) and a \unit[10]{\%} inter-slice gap, recorded in ascending order.
% slice orientation
Slices were oriented to include the ventral portions of frontal and occipital
cortex while minimizing intersection with the eyeballs.
% FOV
The field of view was centered on the approximate location of Heschl's gyrus.
% motion correction
EPI images were online-corrected for motion and geometric distortions.
%
An number of 3599 volumes, identical to the movies study, was recorded for each
participant in the audio-description study.

%
High-resolution T1-weighted structural images with 274 sagittal slices (FoV
191.8 $\times$ 256 $\times$ \unit[256]{mm}) and an acquisition voxel size of
\unit[0.7]{mm} with a 384 $\times$ 384 in-plane reconstruction matrix
(\unit[0.67]{mm} isotropic resolution) were recorded using a 3D turbo field echo
(TFE) sequence (TR = \unit[2500]{ms}, TE = \unit[5.7]{ms}, TI = \unit[900]{ms},
flip angle = 8 $^{\circ}$, bandwidth 144.4 Hz/px, Sense reduction AP 1.2, RL
2.0.

\subsection{Preprocessing}

\todo[inline]{following phrasing is almost identical to PPA-Paper}

% data sources
The current analyses were carried out on the same preprocessed fMRI data
\citep{hanke2016aligned} that were used for the technical validation analysis
presented in \citet{hanke2016simultaneous}, the localization of higher-visual
areas \citep{sengupta2016extension}, and investigation of responses of the PPA
correlating with naturalistic spatial information \citep{sengupta2016extension}.
% exclusion of VP 10
Of those 15 participants in the studyforrest dataset that took part in all three
experiments, data of one participant were dropped due to invalid distortion
correction during scanning of the audio-description stimulus.
% preprocessing of pre-aligned data
Data were corrected for motion, aligned with and re-sliced onto a
participant-specific BOLD template image \citep{sengupta2016extension} (uniform
spatial resolution of \unit[2.5$\times$2.5$\times$2.5]{mm} for both
audio-description and movie data).
% preprocessing intro
On these data, the further preprocessing steps were performed by FEAT v6.00
(FMRI Expert Analysis Tool \citep{woolrich2001autocorr}) as shipped with FSL
v5.0.9 (\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software Library}
\citep{smith2004fsl}) to create the input data for the subsequent shared
response modelling to create the \ac{cms}.
%
The preprocessing steps closely resemble the processing that was previously
performed in \citet{sengupta2016extension} and in \citet{haeusler2022processing}
responsively.

%
\todo[inline]{make sure that all mentioned steps have actually been applied to
\\ 'filtered\_func\_data.nii.gz' serving as input for custom SRM-scripts}

\subsubsection{Functional localizer data}

\todo[inline]{cf. with VIS paper; see design-files}

% despiking
Every run of the functional localizer paradigm was despiked using AFNI's
\citep{cox1996afni, cox1997software} '3dDespike' command, and
slice-time-corrected.
% temporal filtering
High-pass temporal filtering was applied to every run using a Gaussian-weighted
least-squares straight line with a cutoff period of \unit[100]{s} to remove
low-frequency confounds.
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
Data were spatially smoothed applying a Gaussian kernel with full width at half
maximum (FWHM) of \unit[4.0]{mm}
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% rerun the analysis: ./code/generate\_1st\_level\_design.sh condor\_submit
% .code/compute\_1stlvl\_glm.submit

\subsubsection{Naturalistic stimuli data}
%% install naturalistic ppa analysis as subdataset comprises  "aligned-data",
% templates etc. as subdatasets datalad install -d . -s
% https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis

% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in_bold3Tp2/sub-??_task-a?movie_run-?_bold*.*

% get motion correction parameters for AO data
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/phase1
% datalad get inputs/studyforrest-ppa-analysis/inputs/phase1/sub???/BOLD/task001\_run00?/bold\_dico\_moco.txt

\todo[inline]{following phrasing is almost identical to PPA-Paper}

% temporal filtering
To every segment of the movie and audio-description, high-pass temporal
filtering was applied using a Gaussian-weighted least-squares straight line with
a cutoff period of \unit[150]{s} (sigma=\unit[75.0]{s}).
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet},
% spatial smoothing
and data were spatially smoothed (Gaussian kernel, \unit[4.0]{mm}, FWHM).
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% pre-whithening
Correction for local autocorrelation in the time series (prewhitening) was
applied using FILM (FMRIB's Improved Linear Model \citep{woolrich2001autocorr})
to improve estimation efficiency.

%% templates and transforms
% datalad get
% inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/sub-*/bold3Tp2/;
% datalad get
% inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/templates/*

% rerun second-level analysis to obtain the (un)thresholded $z$-maps?
% was it even necessary?


%
Results from \citet{sengupta2016extension} and in
\citet{haeusler2022processing} were simply downloaded.
%
``We estimated the face-selectivity map for a participant from his or her own
localizer data by calculating the GLM univariate contrast map of faces vs. all
the other categories (e.g., body, place) for each run and averaging t-values
across the four maps'' \citep{jiahui2020predicting}.


\subsection{Modeling of a shared response space}

\todo[inline]{how to reliably (!) hyphenate links??}

Further analyses steps were performed via Python scripts that relied on
%
NiBabel v3.2.1 (https://nipy.org),
%
NumPy v1.20.2 (https://numpy.org),
%
Pandas v1.2.3 (https://pandas.pydata.org),
%
Scipy v1.6.2 (https://scipy.org),
%
scikit-learn v1.0 (https://scikit-learn.org),
%
BrainIAK v0.11 (https://brainiak.org),
%
Matplotlib v3.4.0 (https://matplotlib.org),
%
seaborn v0.11.2 (https://seaborn.pydata.org),
%
and calling command line functions of FSL.

\paragraph{Fixing FSL output}

\todo[inline]{grand\_mean\_for\_4d.py (formerly: data\_normalize\_4d.py) not
necessary anymore 'cause FSL seems to have applied grand mean scaling to
'filtered\_func\_data.nii.gz)'}
\todo[inline]{the script now just adds the mean again and scales by factor
10000; s. below}

% grand mean scaling for 4d data:
% voxel values in every image are divided by the average global mean
% intensity of the whole session. This effectively removes any mean global
% differences in intensity between sessions.

% FSL User Guide:
% filtered_func_data will normally have been temporally high-pass filtered,
% it is not zero mean; the mean value for each voxel's time course has been
% added back in for various practical reasons.
% When FILM begins the linear modelling, it starts by removing this mean.

% input: 'sub-*/run-?.feat/filtered_func_data.nii.gz' of VIS, AO, AV
% -> should already grand mean scaled

%
Given that FSL adds back the mean value for each voxel's time course at the end
of the preprocessing, mean values were subtracted again, and each voxels's time
course was multiplied by 10000 [using a Python script].
% saved to 'sub-??\_task-*\_run-?\_bold\_filtered.nii.gz'


\paragraph{Masking}

\todo[inline]{explain, why we need a reduction of features/voxels; and how we do
it ``theory-driven'' based on anatomical data}

\todo[inline]{problem 1: grpPPA contains N=14 subject, not N-1 subjects}

\todo[inline]{problem 2: still, there are voxels outside of the PPA-mask;
probably, because of the warping procedures}

% masks-from-mni-to-bold3Tp2.py:
% - merges unilateral ROIs overlaps (already in MNI) to bilateral ROI
% - output: 'masks/in_mni/PPA_overlap_prob.nii.gz'
% - warps union of ROIs from MNI into each subjects space
% output: 'sub-*/masks/in_bold3Tp2/grp_PPA_bin.nii.gz' + audio_fov.nii.gz dilate
% the ROI masks by 1 voxel; output: 'grp_PPA_bin_dil.nii.gz'

% masks-from-mni-to-bold3Tp2.py:
% warp MNI masks into individual bold3Tp2 spaces

% masks-from-t1w-to-bold3Tp2.py:
% transforms 'inputs/tnt/sub-*/t1w/brain_seg*.nii.gz'
% into individual's bold3Tp2
% output: 'sub-*/masks/in_bold3Tp2/brain_seg*.nii.gz'

% mask-builder-voxel-counter.py:
% builds different individual masks by dilating, merging other masks
% creates a FoV of AO stimulus for every subject from 4d time-series of AO run
% output: sub-*/masks/in_bold3Tp2/audio_fov.nii.gz'
% counts the voxels
% long story short: we cannot used all gyri that contain PPA to some degree
% even if the mask by FoV of AO stimulus and individual gray matter mask

% data_mask_concat_runs.py: masks are not dilated and not masked with
% subject-specific gray matter mask

% union of PPA masks
For each paradigm and subject, the corresponding 4d time series of each run /
segment were masked with the union of individual PPA masks
\citep[s.][]{haeusler2022processing} that was warped from MNI space into each
subjects' space.
% AO FoV
Each subjects' data were further masked with the subject-specific \ac{fov} of
the audio-description.
%
The number of remaining voxels for each subject can be seen in Table
\ref{tab:ppamaskvoxels}.


\begin{table*}[btp]
    \caption{Number of remaining voxels after each subject's brain was masked
    with the union of individual \acp{ppa} that was warped from MNI space into
    each individual's subjects-space and a subject's-specific FoV from AO study.}

\label{tab:ppamaskvoxels}
\begin{tabular}{ll}
\toprule
\textbf{Subject} & \textbf{no. of voxels} \\
\midrule
sub-01 & 1665 \tabularnewline
sub-02 & 1732 \tabularnewline
sub-03 & 1400 \tabularnewline
sub-04 & 1575 \tabularnewline
sub-05 & 1664 \tabularnewline
sub-06 & 1951 \tabularnewline
sub-14 & 1376 \tabularnewline
sub-09 & 1383 \tabularnewline
sub-15 & 1683 \tabularnewline
sub-16 & 1887 \tabularnewline
sub-17 & 1441 \tabularnewline
sub-18 & 1729 \tabularnewline
sub-19 & 1369 \tabularnewline
sub-20 & 1437 \tabularnewline
\bottomrule
\end{tabular}
\end{table*}

% scaling
Data of every run were independently normalized ($z$-scored) to a mean of zero
and a standard deviation of one ($\mu=0$, $\sigma=1$).
% output: 'sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy; and:
% 'sub-*_task_visloc_run-1-4_bold-filtered.npy'
%
The last 75 TRs of the audio-description were missing in subject-04 due to an
image reconstruction problem.
%
The \ac{srm} allows the number of voxels to be different across subjects but the
number of samples must be the same.
%
Hence, we removed the last 75 TRs of the audio-description from the other
subjects' time series.
% it's only credits anyway; AO + AV has 7123 TRs; not 7198 volumes anymore;
% visual localizer has 156 TRs per run
As a result, the data to fit the \ac{srm} in the following step comprised 3599
TRs of the movie, 3524 TRs of audio-description, and 624 TRs of the visual
localizer experiment.

\todo[inline]{yeah, well, probably I should have cut the last run of AO first,
and then z-scored the last segment but anyway...; does not make much difference}


\paragraph{Fitting of shared response model}

\todo[inline]{We assessed estimation performance using leave-one-subject-out
cross-validation: the model was trained on all-but-one subject and the held-out
subject was aligned to the fixed model to obtain his/her transformation matrix}.

\todo[inline]{In the special case of SRM—which allows for calculating an
alignment from all provided subjects in a single decomposition—we with- held the
left-out subject from the shared response estimation step to avoid data leakage.
The projection of the left-out subject is then learnt from previously estimated
shared space. Finally, the learnt projections are applied to the decoding data,
and decoding is performed on the projected data'' \citep{bazeille2021empirical}}

\todo[inline]{mention that just 'ao \& av' as input were also tested?}

We used the probabilistic \ac{srm} algorithm implemented in BrainIAK v.11 (Brain
Imaging Analysis Kit, \citet{kumar2020brainiak, kumar2020brainiaktutorial},
http://brainiak.org) that spatial components to be orthonormal and $k\ll v$ (v
= voxel, t=time-points) to fit the \ac{srm}.
%
This implementation approximates the \ac{srm} using the Expectation Maximization
(EM) algorithm proposed by \citep{chen2015reduced} optimized by
\citet{anderson2016enabling}.
%
We followed a leave-one-subject-out folding scheme [correct term?]:
%
for every subject $n$, we used the other subjects' ($N-1$) concatenated fMRI
responses to the movie, audio-description, and functional localizer as training
set to compute the shared feature space.
%
Concatenated time series data were z-scored across experimental
paradigms.\todo{a second z-scoring}
% features
Considering the size of the ROIs the spatial resolution of the fMRI data, we
chose a value of $k=10$ for the number of features (i.e. the shared responses)
to be computed.
% n_iter
The number of iterations for the algorithm to minimize the error was set to 30
($n_{iter=10}$).

\todo[inline]{cf. \citep{haxby2011common} who ran PCA over ventral temporal
cortex and chose 35 dimensions; Figure 3; ``We show that the cortical
topographies associated with well-known category selectivities are preserved in
the 35-dimensional common model space}

``In this paper, $k=10$ since low-rank SVD with 20 dimensions captures 90\% of
the variance of the original fMRI matrices \citep{chen2015reduced}. We also
experimented with using $k=5,20,30,40,50$, but the results barely varied from
using $k=10$ dimensions.  Note that, for testing, the learned $W_{i}$ allow us
to project unseen fMRI data into the shared space via $W_{i}^{T}X_{i}^{test}$
since $W_{i}$ has orthogonal columns'' \citep{vodrahalli2018mapping}.

%
During model fitting, the algorithm computes the shared feature space $S$ ($k$
features by $t$ time points) and calculates subject-specific, orthogonal [?]
transformation matrices $W_{n}$ ($v$ voxel by $k$ features).
%
These transformation matrices, or weight matrices, reflect the loadings of
voxels onto features [i.e. subject-specific functional topographies], and allow
to project responses of voxels within each subjects' \ac{roi}(s) from anatomical
space into the $k$-dimensional shared feature space, and thus functionally align
subjects.

\todo[inline]{negative control: I need to check the code; unfortunately, it is
still pretty early in the processing pipeline}

% shuffled data
As negative control, we shuffled the order of runs of audio-description and
movie (but not visual localizer) before fitting a shared response model to the
time series.


\subsection{Alignment of left-out subjects}

% AO: 0-451, 0-892, 0-1330, 0-1818, 0-2280, 0-2719, 0-3261, 0-3524
% AV: 3524-3975, 3524-4416, 3524-4854, 3524-5342, 3524-5804, 3524-6243,
%     3524-6785, 3524-7123
% AO+AV: 0-7123

\todo[inline]{partial alignment: well, explanation makes sense in my head}
%
\todo[inline]{perform alignment with TRs of localizer for
cross-subject-prediction = model variation, ceiling ...}
%
\todo[inline]{s. get\_wmatrix\_for\_left-out.py: srm.transform\_subject calls
np.linalg.svd()}

``The weight matrices that we created from SRM data are not only useful for
projecting new data into a shared space but also for the reverse —
reconstructing data in subject space''.

%
We then aligned each left-out subject to the shared space $S$ that was computed
from the other subjects' data.
%
The algorithm learns a mapping $W_{n}$ of the left-out subject's anatomical
space into the shared space $S$ that is kept fixed.
%
Critically, in order evaluate the relationship between the length of the
stimulus that is used to align an ``unknown'' subject to a preexisting shared
space and prediction performance, we performed a \textit{partial alignment}:
%
We used an increasing number of segments (1 up to 8) of the naturalistic stimuli
per subject to let the algorithm learn the orthogonal mapping to the TRs of the
corresponding segment(s) in the shared space.
%
As a result, we obtained 8 transformation matrices per subject and per
naturalistic stimulus.
%
Each matrix has a size of $v$ voxels by $k$ features but is based on a different
quantity of data used to calculate the mapping [i.e. subject-specific functional
topographies].


\subsection{Estimation of scene-selective topography}

\todo[inline]{in both following cases the mean was computed in subject-specific
anatomical space; I also tested creating a template by computing the mean in
shared feature space; leads to pretty similar; but I did not create a template
by taking the mean in MNI space; estimation using anatomy has always been mean
of z-maps in subject-space in both cases}


%
To predict the results of the functional localizer (statistical $Z$-maps
representing functional topographies) of a left-out subject, we projected all
other participants $Z$-maps from their subject-specific anatomy into the shared
feature space, and from the shared feature space into the left-out subject's
anatomy.
%
As a benchmark to evaluate the performance of the estimation using functional
alignment, we similarly estimated each left-out subject's $Z$-map using
volume-based anatomical alignment.


\subsubsection{Prediction/estimation using \ac{cms}}

\todo[inline]{terminology: it is NOT the "inverse" but "transpose" matrix}

%
First, individual results from the visual localizer [and auditory naturalistic
stimulus] were masked (in each individuals' subject-space) with the union of PPA
masks group PPA.
%
Then, data were transformed from subject-specific, anatomical spaces into the
shared response space [by calling srm.transform(masked\_zmaps)'].
%
Using the transpose of the left-out subject's transformation matrix, the
$Z$-maps were projected from shared response space into the anatomical space of
the left-out subject.
%
The mean of the 13 projected $Z$-maps served as an estimation of the left-out
subject empirical $Z$-map (s. \citep{sengupta2016extension})


\subsubsection{Prediction using anatomical alignment}

\todo[inline]{tell more about how transformation matrices were created;
potential reviewer's question: your anatomical alignment might have been shitty,
linear etc. ?}

\todo[inline]{as baseline against which we compare the functional alignment}

\todo[inline]{check TNT github/gin repo; what is the corresponding paper?}

%
We also estimated each left-out subject's $Z$-map using volume-based anatomical
alignment.
% into MNI
First, we warped the results of the visual localizer from each subjects'
space(bold3Tp2) into MNI space (grpbold3Tp2) using precomputed transformation
matrices that are part of the studyforrest dataset [LINK; cf. method section in
PPA-Paper].
% into left-out subject
Then, results of all subjects were warped from MNI space in to the left-out
subject's space
%
The transformed $Z$-maps were masked with mask of unions of PPAs and the
individual's \ac{fov}.
%
Finally, the mean of these $Z$-maps served as a prediction of the corresponding
left-out subject's empirical $Z$-map.


\subsection{Quantifying the performance}


\subsubsection{Correlation stuff}

%
To quantify the performance of our estimations based on different number of runs
of the movie and audio-description, we calculated the Pearson's correlation
coefficients between the empirical $Z$-maps from the localizer experiment [and
audio-description] and estimated $Z$-map based on other subject's data.

%
Further we compared these correlations to the reliability [internal
consistency?] of visual localizer using Cronbach's Alpha based on the
variability across runs [as did
\citep{jiahui2020predicting}].

\subsubsection{Cronbach's Alpha of localizer}

\todo[inline]{s. 'studyforrest-srm-movies/test/statistics\_cronbachs.py'}

\todo[inline]{script is apparently written; tbh, did not take a close look at
the result or just cannot remember; it will be mention 'pro forma' anyway}


\subsection{backup: alternative template creation}
%
lastly, we currently apply another method to get the „z-map template“ on which
the prediction is based ('test/data\_denoise-vis.py';
'test/data\_srm-vis-to-ind.py')
%
From \citep{jiahui2020predicting} (an approach different from ours): ``We
estimated a participant's map from other participants' data by first projecting
all other participants' localizer data into that participant's cortical anatomy
and calculating the GLM univariate contrast map of faces versus all other
categories for each run in each other participant then averaging t-values across
the maps (56 maps for StudyForrest, 14 subjects x 4 runs for each; 80 maps for
Grand Budapest, 20 subjects x 4 runs each)'' \citep{jiahui2020predicting}.
%
``An added benefit is that SRM helps address the data starvation problem above:
because the SRM space is by definition shared across individuals, data from
multiple participants can be combined prior to MVPA or other analyses''
\citep{cohen2017computational}.

\paragraph{denoising}
%
``De-noised individual-subject data can be reconstructed by
projecting data from the reduced-dimension shared space back into any given
subject’s brain space'' \citep{kumar2020brainiak}.

%
``Transformations estimated from one subset of data can be used to project
unseen data into the shared space. Projecting data into shared space increases
both temporal and spatial ISC (by design), and in many cases improves
between-subject model performance to the level of within-subject performance''
\citep{kumar2020brainiak}.


\section{Results}

Unthresholded $Z$-maps [in each subject's voxel space] can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}.

\todo[inline]{lots of phrasing is taken from \citep{jiahui2020predicting}; check
Fig. 2 in \citep{jiahui2020predicting}; any good?}

\todo[inline]{also check \citep{haxby2011common, weiner2018defining} and the
other, new studies that try to predict stuff}


\subsection{Model-related}

\subsubsection{plot\_srm.py}

\todo[inline]{imo, plots are not very informative}

e.g. time series plot of features [first 800 TRs of SRM]; distance matrix of
time-points in shared space (all features combined)


\subsubsection{plot\_corr-of-glm-and-srm.py}

\todo[inline]{imo, plots are not very informative, too}

plots correlation matrix of shared features; correlation matrix of
AO-regressors (s. PPA-Paper) and shared responses (sliced to TRs of AO
stimulus); correlation matrix of AV-regressors (s. PPA-Paper) and shared
responses (sliced to TRs of AV stimulus)


\todo[inline]{possible plots:
correlate AO regressors (PPA-paper) with AO TRs;
correlate AV regressors (PPA-paper) with AV TRs;
do the same with the model based on shuffled runs}


\begin{figure*}[tbp]
\centering
    \includegraphics[width=\linewidth]{figures/corr_ao-regressors-vs-shared-resp}
    \caption{Pearson correlation coefficients of regressors used in the analysis
    of audio-description to model responses correlating with nouns spoken by the
    narrator and features of the \ac{srm} (i.e. shared responses).
    \texttt{geo\&groom} \texttt{geo\&groom\&furn} are combination of regressors
    (as used on the positive side of contrasts). The
    time series of the \ac{srm} were sliced to match the TRs of the
    audio-description.
      }
\label{fig:reg-corr}
\end{figure*}


\subsection{Prediction-related}

\todo[inline]{predict\_ppa.py: outputs also the Pearson correlations}

``We estimated each participant's face-selectivity map based on that
participant’s localizer data [anatomical alignment], based on other
participants' localizer data projected into that participant's cortical anatomy
using hyperalignment and anatomical surface alignment (see Fig. 1)
\citep{jiahui2020predicting}''.

``We correlated the whole-cortex contrast map (faces-vs-all) based on a
participant’s own data with the maps estimated from other participants’ data
separately for the StudyForrest and Grand Budapest datasets. After 1-step
hyperalignment, the mean Pearson correlation values across participants were
0.58 (StudyForrest, N = 15, S.D. = 0.08)'' \citep{jiahui2020predicting}.

``Hyperalignment greatly improved the prediction performance compared with
anatomical surface alignment. With surface alignment, the average Pearson
correlation values across participants were 0.40 (N = 15, S.D. = 0.08) and 0.50
(N = 21, S.D. = 0.06) in the StudyForrest and Grand Budapest datasets,
respectively (Fig. 3). The difference between the hyperaligned and the
surface-aligned mean correlation values was highly significant (StudyForrest:
t(14) = 17.39, p < 0.001)'' \citep{jiahui2020predicting}.


\subsubsection{plot\_stripplot.py}

%
Pearson correlation coefficients between empirical $Z$-maps (results of
localizer; y-axis) and estimated $Z$-maps.
%
Green dots: A left-out subject's $Z$-map was estimated by projecting all other
subjects ($N = 13$) $Z$-maps through the MNI152 space into the left-out subject
space and averaging values across subject.
%
Blue dots: transformation matrices computed based on an increasing number of
segments of the audio-description.
%
Orange dots: transformation matrices computed based on an increasing number of
segments of the audio-description.


For every subject, we see the correlation of z-maps that tell us the, quote
``real'' unquote, PPA and predicted PPA
%
In green, we see the correlations between empirical values from the localizer \&
the predicted values using anatomical alignment
%
In orange, we see the correlations between empirical values \& the predicted
values using parts of the movie
%
In blue, we see the correlations between empirical values \& the predicted
values using parts of the audio-description


\subsubsection{statistics\_t-test-correlations.py}

Tests whether differences between prediction using anatomy and prediction using
\ac{srm} is significant.


\subsubsection{show plots of brain slices?}

\todo[inline]{talk during seminar showed (quick screenshot in FSL) from sub-04
as an example}

%% We have these nice blurry EPI-images and all z-maps are threshold at a value of bigger than 2.3.

% always in red, we can see the z-map from the localizer experiment across the whole brain,

% the region of interest that we used is white and the predicted values, are blue

% The prediction using anatomical alignment and the prediction using 15 minutes of movie data show a correlation of about .7

% the prediction using 15 minutes of the audio-description correlates about 0 with the empirical z-map

% The last one is the extreme case, but it can give you an idea of how the z-maps look in a slice of the brain

\todo[inline]{I guess, I could adjust script that plots the brain slices for the
PPA-paper (without losing my mind?); but script expects standard brain in MNI
space; transformation of prediction into MNI (again?); and it is just about the
ROI not about the whole brain; standard space could be substituted with (nice
blurry) subject-specific functional template}


\subsubsection{plot\_bland-altman.py}

\todo[inline]{I hate that script}

% corrstats.py; not necessary anymore; calculates the statistical significant
% differences between two dependent or independent correlation coefficients


\subsubsection{Estimation of localizer from localizer TRs}

just "cross-subject" but not "cross-experiment-cross-subject" prediction"


\subsubsection{Cronbach's Alpha of localizer}


all from \citep{jiahui2020predicting}: ``We compared the correlations between
maps estimated from a participant's own data and maps estimated from other
participants' data to the reliability of the localizer. We computed the
reliability of the contrast maps with Cronbach's Alpha based on variability
across the four localizer runs for each set. The mean Cronbach's Alpha between
the four localizer runs was 0.60 (N = 15, S.D. = 0.14) in the StudyForrest
dataset [...](Fig. 2)''.
%
``These results mean that if we scan each participant for another 4 localizer
runs, and compute the correlation between the two maps (4 runs vs. 4 runs), the
correlation would be 0.60 on average in the StudyForrest''.
%
``Cronbach's alpha indicates that the predicted contrast map based on
hyperalignment is close to or as good as the real contrast map based on four
localizer runs (StudyForrest: t(14) = 0.61, p = 0.55; Grand Budapest: t(20) =
3.02, p = 0.007)''.
%
``In the StudyForrest dataset, the predicted contrast map based on
hyperalignment was better than the contrast map based on data from three out of
four localizer runs in other participants (t(14) = 2.36, p = 0.03) and in Grand
Budapest, the predicted contrast map was comparable to the contrast map based on
three localizer runs in other participants (t(20) = 0.48, p = 0.63)''.
%
``A scatterplot of the individual correlation values with hyperalignment and
with surface alignment (Fig 3), shows that predicted maps based on
hyperalignment were more accurate than those based on surface alignment in every
participant''.


\section{Discussion}


\subsection{Short Summary}

\subsubsection{Aim}

\subsubsection{Methods}

\subsubsection{Results}


\subsection{Discussion of current results}


We have cross-subject, cross-experiment, and cross-scanner prediction


\subsubsection{Model Space}
%
Results indicate that we are able to use multiple subjects to learn a
10-dimensional shared space for the fMRI data that increases performance on our
experiments.

%
Asymptotic ``performance curve'' might be different for another brain region.
%
What about rentinotopic mapping?
%
What about ``higher'' cognition?
%
What about executive functions?


\subsubsection{Partial Alignment}
%
15 min of movie watching used for functional alignment outperform prediction
using anatomical alignment
%
30 minutes of movie watching outperform 15 minutes of movie watching
%
more than 30 minutes do not lead to a significantly improved prediction
performance.


\paragraph{what partial alignment might be good for}
%
Reduced costs.
%
There is a benefit of shared parts of naturalistic stimuli across datasets;
shared stimulus part is easier than shared subjects (e.g.
\citep{zhang2018transfer}).


\subsubsection{Audio PPA}

%
Response in PPA during AO might be different.
%
There are some subjects which do not have a AO PPA 'cause they just do not give
a shit about spatial information in AO? Might be an issue of missing task?


\subsection{Self-critique \& short comings}


\subsubsection{localizer is ``ground truth''}

%
``The dynamic localizer (in Grand Budapest data set) was significantly more
reliable than the static localizer (in StudyForrest data set) (t(34) = 3.76, p <
0.001) despite its shorter length (four 234s runs versus four 312s runs,
respectively)'' \citep{jiahui2020predicting}.

%
From \citep{weiner2018defining}: ``the identification of the PPA is complicated
by (at least) four methodological considerations. First, the PPA definition may
depend on the type of experiment, task, and stimuli used. Second, the boundaries
of the PPA may depend on the statistical threshold used. Third, the spatial
extent and localization of the PPA may vary if defined within the native brain
space of an individual or based on a group analysis. Fourth, the size of the PPA
may depend on data acquisition choices (e.g. large vs. small voxels) and data
analysis choices (e.g. liberal smoothing vs. no spatial smoothing). The present
study aims to identify and to predict the most probable location of
place-selective voxels within medial VTC of an individual brain that is
impervious to these methodological decisions'' \citep{weiner2018defining}.

%
\citet{lilienfeld2015fifty} on ``gold standard'': ``In the domains of
psychological and psychiatric assessment, there are precious few, if any,
genuine ``gold standards''. Essentially all measures, even those with high
levels of validity for their intended purposes, are necessarily fallible
indicators of their respective constructs [Cronbach and Meehl, 1955; Faraone and
Tsuang, 1994]. As a consequence, the widespread practice referring to even
well-validated measures of personality or psychopathology, such as Hare’s
(1991/2003) Psychopathy Checklist-Revised, as ``gold standards'' for their
respective constructs [Ermer et al., 2012] is misleading [see Skeem and Cooke,
2010]. If authors intend to refer to measures as ``extensively validated'', they
should simply do so'' \citep{lilienfeld2015fifty}

%
\citet{scheinost2019ten} in context of phenotypic measures: ``Predictive models
based on neuroimaging data will only ever account for a fraction of the
variance. Neuroimaging studies are limited by how much information the signal
can capture about the measure of interest. At the same time, these studies are
also limited by the chosen phenotypic measure used.  While the success of a
model is evaluated by how well it predicts a phenotypic measure (and these
phenotypic measures have to be treated as gold standards), it is well known that
such measures are not always the ground truth but themselves suffer from
confounds and noise.  When studying brain-behavior associations, one must keep
in mind how extraordinary it is that neuroimaging data can be distilled to
approximate phenotypic measures that reflect a simplification of multiple
complex features. Thus, even modest results are reasonable and remarkable. For a
discussion on the reliability of phenotypic measures in the context of
predictive modeling, we point the interested reader to: [Dubois et al., 2018a,
2018b; Gignac and Bates, 2017]'' \citep{scheinost2019ten}.


\subsubsection{ROI creation procedure}

\todo[inline]{check paper regarding ``probabilistic functional atlases''}


\subsubsection{Volume-based registration vs. surface-based normalization}

%
Sengupta did the published analyses $Z$-maps in volume space
%
I could have run the analyses/contrasts on the surface but opportunity costs.
%
\todo[inline]{Volumen-based anatomical alignment was compared to volume-based
functional alignment}

Volume vs. surface: e.g. \citep{desai2005volumetric}

\citep{weiner2018defining} compares cortex-based alignment
\citep{fischl1999high} with volume-based (Talairach) alignment: ``we repeated
our leave-one-out cross-validation procedure across all 24 participants with an
affine volume-based registration to the Talairach brain and compared this
performance to the same procedure implemented with CBA in FreeSurfer
\citep{weiner2018defining}''




\subsection{Future questions}

\subsubsection{Predict other t-contrasts of higher-visual area localizer}


\paragraph{Predict other functional areas / other localizers}


e.g. retinotopy, language areas

%
Again, data from the studyforrest project will be used to perform assessments
analog to WP1e for a range of additional individually determined regions based
on established localizer paradigms (Sengupta, et al., 2016).
%
These regions include: the \ac{ffa}, \ac{ppa}, and \ac{eba}  which are
associated with face perception \citep{kanwisher1997ffa,
pitcher2011occipitalfacearea}, scene perception \citep{epstein1998ppa}, and the
perception of human bodies \citep{downing2001bodyarea}, respectively.

``First, our brains directly process exogenous information about the external
environment by transducing physical phenomena (e.g., changes in energy,
molecular concentrations, etc.) into sensory perceptions that allow us to
generate and maintain a sense of what is happening around us (1, 2). Mental
representations that are directly driven by the external world are likely to be
highly similar across individuals who share the same sensory experience. Second,
our brains also process endogenous information that reflects our current
internal homeostatic states, past experiences, and future goals (3). The
integration of exogenous and endogenous informa- tion allows us to meaningfully
interpret our surroundings, prioritize information that is relevant to our
goals, and develop action plans (4). Given the same input information,
individuals may have unique interpretations, feelings, and plans, often leading
endogenous rep- resentations to be idiosyncratic across individuals''
\citep{chang2021endogenous}.

``Human brains have much in common with one another. Similarities exist not only
at the anatomical level, but also in terms of functional organization. Given the
same stimulus—an expanding ring, for example—regions of the brain that process
sensory (visual) stimuli will respond in a highly predictable and similar manner
across differ- ent individuals. This predictability is not limited to sensory
systems: shared activity across people has also been observed in higher-order
brain regions (for example, the default mode network6, or DMN) dur- ing the
processing of semantically complex real-life stimuli such as movies and
stories7–13. Notably, shared responses in these high-order areas seem to be
associated with narrative content and not with the physical form used to convey
it11,14,15. It is unknown, at any level of the cortical hierarchy, to what
extent the similarity of human brains during shared perception is recapitulated
during shared recollection. This prospect is made especially challenging when
recall is spontane- ous and spoken, and the selection of details is left up to
the remem- berer (rather than the experimenter), as is often the case in real
life'' \citep{chen2017shared}.



\subsubsection{Create CMS from other study}

\todo[inline]{a.k.a. ``more subjects''}
%
Create a CMS from another experiment's data,
using another scanner and hopefully more subjects
%
In case of an alignment of time series,
that experiment needs, at least, a part of Forrest Gump as an intersection


\subsubsection{Other functional alignment algorithms}

\todo[inline]{cf. \citep{bazeille2021empirical}}

\todo[inline]{Multimodal Surface Matching (Robinson et al. 2014)?}

``As we use inter-subject decoding to compare functional alignment methods, we
only consider methods that meet the following two criteria. First, the alignment
transformations should be learned on activations evoked during temporally
synchronized (i.e., co-occuring) task data, or on contrasts matched across
individuals. Second, the learned transformations must be invertible or almost
invertible linear mappings and applicable as-is on unseen data with a different
task structure. These two criteria exclude several methods currently used in the
literature such as regularized canonical correlation analysis (rCCA; Bilenko and
Gallant, 2016), gradient hyperalignment (Xu et al., 2018), connectivity
hyperalignment (Guntupalli et al., 2018), and methods based on Laplacian
embeddings (Langs et al., 2014)'' \citep{bazeille2021empirical}.

``Nonetheless, it remains unclear how researchers should choose among the
available functional alignment methods for a given research application''
\citep{bazeille2021empirical}.


\subsubsection{ROI vs. searchlight}

\todo[inline]{searchlight SRM \citep{zhang2016searchlight}}

\todo[inline]{Hyperalignment searchlight paper?}

%
Our union of PPA is XX voxels big.
%
Influence of voxel count on performance; take searchlight (but how big should it
be?

% Guntupalli's searchlight paper
Later, \citet{guntupalli2016model} showed that hyperalignment can be extended to
predict functional organization across large proportions of the cortical
surface, for example to predict the represented visual field coordinate in
visual cortex based on retinotopic mapping scans of other individuals.

``Applying SRM to a large swath of the brain means that all voxels within the
region contribute to the final derived metric. This can conflict with the goal
of associating spatially local activity with specific cognitive functions. To
address such issues, SRM can be applied in small overlapping searchlights to
obtain localized metrics of shared information [71,99]
\citep{cohen2017computational}''.

``As piecewise alignment is learned within a parcellation, an important question
is: which brain atlas should be used for piecewise alignment? In Supplementary
Figure S2 we compare results from the Schaefer et al. (2018) atlases to those
from parcellations derived directly on the alignment data. By default, the
results presented piecewise alignment is learned within a parcellation, an
important question is: which brain atlas should be used for piecewise alignment?
In Supplementary Figure S2 we compare results from the Schaefer et al. (2018)
atlases to those from parcellations derived directly on the alignment data. By
default, the results presented below are derived with the 300 ROI parcellation
of the Schaefer atlas unless noted otherwise. In the case of searchlight
Procrustes, we selected searchlight parameters to match those used in Guntupalli
et al. (2016); that is, each searchlight had 5 voxel radius, with a 3 voxel
distance between search- light centers. All searchlight analyses were
implemented using PyMVPA [Hanke et al., 2009].ted below are derived with the 300
ROI parcellation of the Schaefer atlas unless noted otherwise. In the case of
searchlight Procrustes, we selected searchlight parameters to match those used
in Guntupalli et al. (2016); that is, each searchlight had 5 voxel radius, with
a 3 voxel distance between search- light centers. All searchlight analyses were
implemented using PyMVPA (Hanke et al., 2009)'' \citep{bazeille2021empirical}.

``An alternative scheme, piecewise alignment (Bazeille et al., 2019), uses
non-overlapping neighborhoods either learnt from the data using a parcellation
method—such as k-means—or derived from an a priori functional or anatomical
atlas. Local transforms are derived in each neighborhood and concatenated to
yield a single large-scale transformation. Unlike searchlight, this returns a
transformation matrix with the desired regularities. This framework might induce
staircase effects or other functionally-irrelevant discontinuities in the final
transformation due to the underlying boundaries'' \citep{bazeille2021empirical}.

``To align the entire cortex across subjects, two main frameworks have been
proposed: searchlight and piecewise aggregation schemes. Each of these
frameworks use functional alignment methods to learn local transformations and
aggregate them into a single large-scale alignment; however, search- light and
piecewise differ in how they aggregate transforms, as illustrated in Figure 2.
The searchlight scheme [Kriegeskorte et al., 2006], popular in brain imaging
[Guntupalli et al., 2018; 2016], has been used as a way to divide the cortex
into small overlapping spheres of a fied radius. This method allows researchers
to remain agnostic as to the lo- cation of functional or anatomical boundaries,
such as those suggested by parcellation-based approaches. A local transform can
then be learnt in each sphere and the full alignment is obtained by aggregating
[e.g. summing as in Guntupalli et al., 2016 or averaging] across overlapping
transforms. Importantly, the aggregated transformation produced is no longer
guaranteed to bear the type of regularity (e.g orthogonality, isometry, or
diffeomorphicity enforced during the local neighborhood fit''
\citep{bazeille2021empirical}.


\subsubsection{Time series vs connectivity-based}

\todo[inline]{kind of a killer cause you do not need intersection of
time series; but s. Guntupalli's paper: time series hyperalignment outperforms
connectivity-based hyperalignment (?)}

% from project proposal
From project proposal: ``In his doctoral thesis, recently submitted to the
Faculty of Natural Sciences in Magdeburg, Falko Kaule showed that congruent
time-locked BOLD responses across subjects (i.e. all subjects watching the exact
same full-length movie) as used by Haxby and colleagues are not required to
derive a valid alignment of individuals with a common representational space
\citep{kaule2017examination}.
%
Comparable prediction performance can be achieved by using \textbf{functional
connectivity patterns} (correlation of a voxel's time series with reference
regions in the same brain)'' (s. dissertation project proposal).
%
It should be noted that the number of voxels that can be considered
simultaneously for functional BOLD response time series alignment is limited by
the number of timepoints in the calibration scan (about 300-400 voxels for a
15min scan with a 2s TR, corresponding to a local cortical neighborhood of about
1cm in diameter for a standard resolution).
%
This limitation does not exist in this form for a functional alignment that is
based on connectivity vectors.
%
The length of these connectivity vectors is determined by the number of
reference (or seed) regions in the brain.

\todo[inline]{wtf did \citep{nastase2019leveraging} do?}

% Nastase's ugly mofo paper
``Finally, estimating the SRM from functional connectivity
data rather than response time series circumvents the need for a single shared
stimulus across subjects; connectivity SRM allows us to derive a single shared
response space across different stimuli with a shared connectivity profile
\citep{nastase2019leveraging}'' \citep{kumar2020brainiak}.


\subsubsection{Individual residuals?}

\todo[inline]{We have transformation matrices for individual mapping; but just
onto shared responses}

%
``Furthermore, in cases where each subject's unique response is of more interest
than the shared signal, SRM can be used to factor out the shared component
thereby isolating the idiosyncratic response for each subject [13]''
\citep{kumar2020brainiak}.

From \citep{cohen2017computational}: ``The flip side of focusing on shared
responses is to focus on responses that are idiosyncratic to individuals.
Although these responses are excluded in SRM, they are not necessarily noise and
may in fact be highly reliable within participants.  SRM can be used to isolate
participant-unique responses by examining the residuals after removing shared
group responses, or it can be applied hierarchically to the residuals to
identify subgroups \citep{chen2017shared} \citep{cohen2017computational} [this
other ``Chen'' is fucking up the aethestics of references].  More generally,
there is a growing trend toward investigating individual differences as another
source of meaningful variance in fMRI [73].  Recognizing that signal exists
beyond the average or shared response of a group, such studies exploit
idiosyncratic but stable responses to account for previously unexplained
variance in brain function, behavioral performance and clinical measures
[70,74]'' \citep{cohen2017computational}.

\todo[inline]{but extension of SRM: ``Capturing Shared and Individual
Information in fMRI Data'' \citep{turek2018capturing}}


\subsection{Vision}

\todo[inline]{mih: das Kapitel braucht aus meiner Sicht keinen eigenen
"Outlook"; kommt eh direkt danach für die ganze Arbeit \&  ist da viel
interessanter}

\todo[inline]{IBC nennen?}


\subsubsection{Multiple localizer}

``Calibration scan'' to align to \ac{cms} and atlas of reference group.
%
From project proposal: ``Once a valid alignment is established, known functional
properties of a (normative) reference, derived from extensive scans and analysis
of other subjects, can then be projected into the respective individual voxel
space (s.  Fig. 1 in \citep{nishimoto2016lining})''.

%
``Movies engage multiple brain systems in parallel. From a single movie dataset
multiple functional topographies can be estimated [Guntupalli et al., 2016],
whereas different localizers are typically required to map different functional
topographies, making a thorough mapping of selective topographies time-consuming
and inefficient'' \citep{jiahui2020predicting}.


\subsubsection{Functional atlas}

\todo[inline]{see also \citep{bazeille2019local}; who is quoting him?}

%
From project proposal: ``Moreover, beyond the scope of this project the targeted
homogenization of acquisition procedures will further the goal of large scale
data collection for the purpose of producing a normative reference of brain
function as measured by fMRI''.

%
From project proposal: ``The availability of such a reference would enable
quantitative and qualitative description of an individual's brain function with
respect to such a norm, and consequently progress the field towards neuroimaging
studies of individual differences that more closely resemble their psychological
counterparts''.

%
From talk at INM-7 seminar: ``Imagine you scan a new, unknown subject for just
15 minutes more, and you additionally get results from a whole variety of other
paradigms mapped onto that brain: results from localizers of low-level
perceptual processes, but also higher-level cognitive processes like language,
memory, emotions and so on. And kinda ``adventurous'': If you have different
common model spaces for different subgroups, you can investigate which alignment
onto which ``subgroup common model space'' results in less error, that lets you
classify to which subgroup your new subject might belong''.

%
From \citep{jiahui2020predicting}: ``results lay a foundation for building a
computational tool with a database that could allow others to map multiple
functional topographies in new subjects using only data collected during movie
viewing. Functional localizers are inefficient because they only estimate one or
a few topographies for each localizer. Movies, by contrast, engage in parallel
multiple neural systems for vision, audition, language, person perception,
social cognition, and other functions. Consequently, movies have the potential
to estimate selective topographies in all of these domains. Such a tool would
require a database of data for movies and a range of functional localizers in a
normative group of subjects. A new subject's functional topographies could be
estimated based only on that subject's movie data and other subjects’ localizer
data from the normative database that could be projected into that subject’s
cortical anatomy using hyperalignment transformation matrices derived from movie
data. Such a resource would be more efficient and replace tedious functional
localizers with an engaging movie and could enable mapping of multiple
functional topographies with data from a single fMRI using a naturalistic
stimulus'' \citep{jiahui2020predicting}.

``Characterizing this functional variability, particularly when considering the
genetic level, ideally requires acquiring functional imaging data from hundreds
of sub- jects and organizing these data into a large-scale database, together
with genetic, behavioral and biomorphological data. Databasing and analysis of
structural magnetic reso- nance images has already resulted in probabilistic
ana- tomical atlases [Toga, 2001, Probabilistic approaches; Van Essen, 2002,
Windows on the brain]. However, a similar large scale description of
functional networks is still lacking'' \citep{pinel2007fast}.


\subsection{Conclusion}


\section{Data Availability}

\todo[inline]{all from PPA-Paper but with new GIN link leading to an empty repo}

% \href{https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-analysis}}

% new; PPA analysis
All fMRI data and results are available as Datalad \citep{halchenko2021datalad}
datasets, published to or linked from the \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).
% original
Raw data of the audio-description, movie and visual localizer were originally
published on the \emph{OpenfMRI} portal
(\url{https://legacy.openfmri.org/dataset/ds000113}; \citep{Hanke2014ds000113},
\space \url{https://legacy.openfmri.org/dataset/ds000113d};
\citep{hanke2016ds000113d}).
% visual localizer
Results from the localization of higher visual areas are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-visualrois}{\url{github.com/psychoinformatics-de/studyforrest-data-visualrois}}).
% raw data
The realigned participant-specific time series that were used in the current
analyses were derived from the raw data releases and are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned}{\url{github.com/psychoinformatics-de/studyforrest-data-aligned}}).
% OpenNeuro
The same data are available in a modified and merged form on OpenNeuro at
\url{https://openneuro.org/datasets/ds000113}.
% NeuroVault for z-maps of SRM
Unthresholded $Z$-maps of all contrasts can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}.


\section*{Code Availability}

Scripts to generate the results as Datalad \citep{halchenko2021datalad} datasets
are available in a \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).
