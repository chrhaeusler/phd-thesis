\section{Abstract}
% intro
In order to map perceptual or cognitive functions onto the brain anatomy of
study participants, researchers usually conduct dedicated experiments
\textit{functional localizer} often accompanied with a task.
% problem
Nevertheless, the approach ``one paradigm, one brain function'' becomes
unfeasible if one wants to map a variety of functions in a time-efficient
manner.
% therefore
We explored a way to project brain mapping data (statistical $Z$-maps) from a
reference group onto individual brains of study participants after performing a
functional aligning of participants with a common model space .

%
Data were obtained from conducting a functional localizer paradigm and two
paradigms using naturalistic stimulation during functional magnetic resonance
imaging (fMRI):
%
participants took part in a task-based, block-design visual localizer, and
participants were watching an audio-visual movie and listening to the movie's
audio-description, both paradigms free of any task.
%
Based on these data, we created a common model space employing a shared response
model \citep{chen2015reduced}.
%
On the one hand, the common model space allows denoising data from individuals
that were used to create the common model space.
%
On the other hand, data from left-out subjects can be aligned with the common
model space by using a (preferably short) segment of a naturalistic stimulus as
a ``diagnostic run'', a process that provides a subject-specific transformation
matrices.
%
The inverse of the acquired transformation matrices can then be used to project
data from other paradigms aligned in common space onto the brain of the left-out
subjects.
%
The general goal of the project is to assess the required length of the
diagnostic run, and to compare empirical $Z$-maps with $Z$-maps that were
predicted from other participants' data.

%
Results suggest...

%
Discussion / Conclusion...


\section{Introduction}

\todo[inline]{given this is not a separate publication, I guess it is okay to
write the introduction (which will be similar to the corresponding parts in the
introduction) shorter than it would be written in a stand-alone paper}

\todo[inline]{still: imo the text here should probably be more detailed here
than in the general introduction}

\todo[inline]{to do: extend here, shorten in general introduction / overview}

\todo[inline]{define PPA in intro shortly (maybe \ac{ffa}, too)}

% brain mapping
Topographic brain mapping maps brain functions, perceptual or cognitive
processes, onto brain areas.
% localizer
Usually, this is done by letting study participants perform a task during a
\textit{functional localizer}.
% problem: one localizer for one domain
Functional localizers are designed to maximize detection power and thus usually
map just on domain of brain functions (e.g. different object categories;
\citet{kanwisher1997ffa}) or cognitive processes (e.g. theory of mind;
\citet{spunt2014validating}).
% which gets messy
Consequently, if a researcher or practitioner wants to map a variety of domains,
the approach ``one paradigm, one brain function'' gets time-consuming and
inefficient.
% localizer batteries: intro
Researchers have tried to tackle this issue by creating time-efficient
multi-functional \textit{localizer batteries} \citep{pinel2007fast,
pinho2018individual, pinho2020individual}.\todo{others? H.B.Project?}
% localizer batteries: example
For example, \citet{pinel2007fast} employs a range of dedicated stimuli and
specific tasks participants have to perform in a 5-minute routine.
\todo[inline]{which domains? what tasks?}
% task based = shit
Nevertheless, the diagnostic quality of localizer batteries relies heavily on
participants' comprehension of the task instruction and their compliance, a
criterion that can be difficult to meet in clinical or pediatric populations.
% validity?
Additionally, localizer batteries also rely on carefully chosen and
tightly-controlled, simplified stimuli that are usually presented in blocks do
not resemble how we perceive the real-world during every-day life leading to
questionable external and ecologically validity.\todo{add references}

% interim summary
In summary, we have two issues: a) validity and compliance, and b) efficiency.
% foreshadowing to next sections & transition to naturalistic stimuli
This dissertation will explore how we can both increase validity by using
\textit{naturalistic stimulus} paradigms as well as increase efficiency by
predicting individual functional topography from data collected in a reference
group.


\subsection{Anatomical alignment}

\todo[inline]{es gibt auch prediction from anatomy (i.e. like really from
anatomy (e.g. cortical folding); not using anatomical template to project data
into it and than into th left-out subject)

\todo[inline]{shortly explain volume alignment (and surface-based alignment?)}

% anatomical alignment
In general, a individual's diagnostic could be based on a prediction based on
the anatomical location of the functional area in the reference group (e.g.
\citet{weiner2018defining}.
%
anatomical alignment aligns vertices in 2-dimensional space [is it really
two-dimensional?] or voxels in three-dimensional anatomical space.
%
We transform the shape of an individual brain into the shape of an average,
standard brain
%
''After standard alignment, fMRI data can be aggregated at the group level by
averaging the values at each voxel across participants. Although this reduces
intersubject noise, variation in the anatomical locations of functional signals
across participants blurs estimates of their shared responses''
\citep{cohen2017computational}.




\citep{weiner2018defining} compares cortex-based alignment
\citep{fischl1999high} with volume-based (Talairach) alignment: ``we repeated
our leave-one-out cross-validation procedure across all 24 participants with an
affine volume-based registration to the Talairach brain and compared this
performance to the same procedure implemented with CBA in FreeSurfer
\citep{weiner2018defining}''

% current approach
One approach is based on anatomical alignment in a three-dimensional, anatomical
reference space (hence, \textit{anatomical alignment}).
%
Anatomical alignment aligns functional data of an individual to an anatomical
brain template (e.g. the MNI152 template brain).
%
In order to predict, the functional data in the subject-specific functional
space, the data of N-1 subjects are mapped from the anatomical template space
into the functional space of the left-out / new / unknown subject.
%
Last step: e.g. mean of projected $Z$-maps.



\paragraph{Functional-anatomical correspondence}

\todo[inline]{check \citep{feilong2018reliable}}

Definition

% problem
But problematic is the shitty \textit{functional-anatomical correspondence}
% definition of functional-anatomical correspondence
<definition of functional-anatomical correspondence here>
% illustration
Hence, even assuming two persons had a identically shaped cortex anatomy, still
location, shape size of functional areas would possibly/probably be different.
% shitty
The approach relying on merely anatomical alignment is shitty because of modest
functional-anatomical correspondence.
% explain functional-anatomical correspondence
A modest functional-anatomical correspondence means that even if one would
assume two persons had an identically shaped brain anatomy the location, size,
and shape of a functional area would still not be identical.
%
Figure XY depicts sum of voxels of participants aligned to an anatomical brain
template that are located within the \ac{PPA} \ac{roi} mask of 14 subjects
\citep{sengupta2016extension}.
%
the brighter the voxel the more subjects have their PPA in that location.
%
location is roughly similar across persons, but still, the plot shows that there
is individual variation.
%
``These probabilistic ROIsere generated using cortex-based alignment (CBA) tools
in FreeSurfer (Fischl et al., 1999; http://surfer.nmr.mgh.harvard.edu/). Brieﬂy,
the cortical surface of each of our 12 participants was aligned to the
FreeSurfer average surface (from 39 healthy adults not used in our study) using
a high-dimensional nonlinear registration algorithm. Using this alignment, we
transformed functional ROIs (fROIs) from each subject to the FreeSurfer (FS)
average brain. Probabilistic maps (Fig. 2) were then generated by summing the 12
fROIs at each point along the cortical surface of the FS average brain and
dividing by the number of participants. Each vertex within the map reﬂects the
proportion of participants exhibiting place selectivity at that location on the
cortical surface'' \citep{weiner2018defining}.


\paragraph{individual differences}

\todo[inline]{don't know if it makes sense to mention it in context of my
thesis}
%
do you find differences in individual brain patterns or do differences stem from
differences in functional-anatomical correspondence?
%
vice versa: we might find no differences because of poor functional-anatomical
correspondence


\subsection{Functional alignment}

``Another promising alignment approach attempts to remap regions across
participants based on common temporal response profiles to the same
time-locked external stimulation. Functional alignment algorithms such as
Hyperalignment (Haxby, 2011; Guntupalli, 2016) and the Shared Response Model
(SRM) (Chen, 2016) can be highly effective at improving spatial alignment across
people based on common func- tional responses while still maintaining individual
differences (Nastase, 2019, Reliable...) \citep{chang2021endogenous}.
%
Given that we do not want to predict brain anatomy but predict an accurate
mapping of brain functions onto the brain anatomy:
%
wouldn't it make more sense to not perform an anatomical alignment but a
functional alignment?
%
Functional alignment aligns cortical patterns (time-series or connectivity
profiles) in a multi-dimensional function space.
% reference
The reference is not an average brain anatomy but a \textit{common model space}
(CMS).

``Fine-grained patterns of activity and connectivity can be studied with
functional magnetic reso- nance imaging (fMRI), but the topographies of these
patterns are idiosyncratic, impeding study of their role in the neural basis of
individual differences in cognitive ability. We used hyperalignment (Feilong et
al., 2018; Guntupalli et al., 2018; Guntupalli et al., 2016; Haxby et al., 2020;
Haxby et al., 2011) to resolve the interindividual variation of fine-grained
topographies of functional connectivityi \citep{feilong2021neural}.

%
There are two algorithms dominating the field: hyperalignment and \ac{srm}

``Shared information content is represented across brains in idiosyncratic
functional topographies. Hyperalignment addresses these idiosyncrasies by using
neural responses to project individuals’ brain data into a common model space
while maintaining the geometric relationships between distinct patterns of
activity or connectivity. The dimensions of this common model capture functional
proﬁles that are shared across individuals such as cortical response proﬁles
collected during a common time-locked stimulus presentation (e.g. movie viewing)
or functional connectivity proﬁles'' \citep{busch2021hybrid}.

``Hyperalignment models shared information that is embedded in id- iosyncratic
cortical patterns across brains. Modeling shared informa- tion makes it possible
to compare functional anatomy across brains at a ﬁne spatial scale.
Hyperalignment projects cortical pattern vec- tors into a common,
high-dimensional information space (Haxby et al., 2020). Derivation of this
common space can be based on either neural response proﬁles (e.g. data collected
during tasks, such as movie viewing (Haxby et al., 2011)) or functional
connectivity profiles ﬁles (Guntupalli et al., 2018)'' \citep{busch2021hybrid}.

``A major objective of the hyperalignment algorithm is to map the shared
information originally found in idiosyncratic cortical topogra- phies into a
common space in which this information is better aligned across participants''
\citep{busch2021hybrid}.

``We performed functional alignment on study 1 using the SRM (Chen, 2015). The
deterministic SRM algorithm performs a joint Principal Com- ponents Analysis
(PCA) and can identify a reduced set of functional response components that
maximally align across participants. We chose an arbitrarily selected
1000-dimensional embedding for each ROI. This algorithm learns a separate
transformation matrix that projects each participant into this common space
(Chen, 2015). Estimating a lower-dimensional common model space can potentially
aid in filtering out measurement noise that is assumed to be independent across
individuals. To minimize bias and ensure that ISC analyses were estimated
independent of computing the SRM (58), we learned transformation matrices using
data from episode 2 and subse- quently applied these transformations to data
from episode 1. This allowed us to project each ROI’s voxel responses into the
common 1000-­dimensional space. To identify regions where spatial ISC sig-
nificantly improved as a result of functional alignment, we performed a
paired-sample sign permutation test on the dynamic ISC value using 5000 samples
and used a threshold of false discovery rate (FDR) q < 0.05 to correct for
multiple comparisons'' \citep{chang2021endogenous}'s method section.


\subsubsection{Hyperalignment}

\todo[inline]{part on hyperalignment will probably still to long}
%
Hyperalignment, an algorithm that was pioneered by \citet{haxby2011common}, uses
\ac{bold} response patterns to derive a \ac{cms} using a variant of Procrustes
analysis and computes invertible (orthonormal) transformations from each
individual brain's voxel-space into the \ac{cms}.
%
Importantly, the study also showed that an individual's \ac{ffa} or the
\ac{ppa}, can be localized precisely based on data from a reference group.


\subsubsection{Shared response model}

\todo[inline]{why did we chose SRM? -> nice performance (s. Chen, 2015;
Bazeille, 2021); nicely implemented in BrainIAK; probably faster than
hyperalignment}

\todo[inline]{following is blatantly copied (but little shortened/rephrased)
from \citep{cohen2017computational}}

``The \ac{srm} \citep{chen2015reduced} projects \ac{fmri} responses from
each participant into a low-dimensional space that captures temporal variance
shared across participants [Box 1; Fig. 4–5]'' \citep{cohen2017computational}.
%
``If participants are given the same stimulus or task sequence (for example, a
movie), which leads their brains through a series of cognitive states (for
example, visual, auditory, semantic), then identifying shared variance has the
effect of highlighting variance related to these states''
\citep{cohen2017computational}.


\paragraph{SRM procedure}

%
``SRM offers jointly factoring each participant's data into a shared set of
feature time series and subject-specific topographies for each feature (Fig.
4)''\citep{cohen2017computational}.
%
``Figure 4: fMRI data are collected from each of m participants experiencing the
same stimulus and then organized into a matrix X (voxels by time). Each matrix X
is then factored using a probabilistic latent-factor model into the product of a
subject-specific matrix W of k brain maps (an orthogonal basis) and a shared
temporal response matrix S of size k by time. That is, for each participant: X =
W S + R, where X, W, and the residuals, R (not shown), are subject-specific, and
S is shared across participant's''\citep{cohen2017computational}.

% input data
``Naturalistic stimuli such as movies and stories are often used to generate
such training data, though any study design in which participants perform the
same sequence of trials—or for which a common sequence can be spliced together
from the same set of trials—could be used (for example, a battery of cognitive
tasks)'' \citep{cohen2017computational}
%
``As a rule of thumb, SRM will improve sensitivity for detecting a cognitive
process of interest in the test data if the training stimuli or trials strongly
and variably engage that process in a way that is reliable across participants.
'' \citep{cohen2017computational}.

\paragraph{applications}
%
The simplest use of SRM is for extracting a shared response across an anatomical
ROI. Used this way, SRM and related methods can yield significant gains in
sensitivity for group-level inference [\citep{chen2015reduced,haxby2011common}]
\citep{cohen2017computational}.
%
For example, which short segment of a movie is being watched can be classified
with many times greater accuracy from fMRI data after functional versus
anatomical alignment [65,71]\citep{cohen2017computational}.


%
Problem: ``One of the main obstacles in leveraging brain activity across
subjects is the considerable heterogeneity of functional topographies from
individual to individual. Variability in functional–anatomical correspondence
across individuals means that even high-performing anatomical alignment does not
ensure fine-grained functional alignment [e.g., 36]. As an example, multi-voxel
pattern analysis models that perform well within subjects often degrade in
performance when evaluated across subjects [e.g., 37, 38]''
\citep{kumar2020brainiak}.

``SRM [13], alongside other methods of hyperalignment [39–41], aims to resolve
this alignment problem by aligning based on functional data. SRM estimation is
driven by the commonality in functional responses induced by a shared stimulus
(e.g. watching a movie). Unlike ISC analysis, which presupposes (often very
coarse) functional correspondence, SRM isolates the shared response while
accommodating misalignment across subjects. SRM decomposes multi-subject fMRI
data into a lower-dimensional shared space and subject-specific transformation
matrices for projecting from each subject’s idiosyncratic voxel space into the
shared space (Figure 2). Each of these topographic transformations effectively
rotates and reduces each subject’s voxel space to find a subspace of shared
features where the multivariate trajectory of responses to the stimulus is best
aligned. These shared features do not correspond to individual voxels; rather,
they are distributed across the full voxel space of each subject; each shared
feature can be understood as a weighted sum of many voxels''
\citep{kumar2020brainiak}.

``Transformations estimated from one subset of data can be used to project
unseen data into the shared space. Projecting data into shared space increases
both temporal and spatial ISC (by design), and in many cases improves
between-subject model performance to the level of within-subject performance.
Between-subject models with SRM can, in some cases, exceed the performance of
within-subject models because (a) the reduced-dimension shared space can
highlight stimulus- related variance by filtering out noisy or
non-stimulus-related features, and (b) the between-subject model can effectively
leverage a larger volume of data after functional alignment than is available
for any single subject. De-noised individual-subject data can be reconstructed
by projecting data from the reduced-dimension shared space back into any given
subject’s brain space.  Furthermore, in cases where each subject’s unique
response is of more interest than the shared signal, SRM can be used to factor
out the shared component thereby isolating the idiosyncratic response for each
subject [13]'' \citep{kumar2020brainiak}.

``Building on the initial probabilistic SRM formulation [13, 42], several
variants of SRM have been developed to address related challenges. For example,
a fast SRM implementation has been introduced for rapidly analyzing large
datasets with reduced memory demands [43]. The robust SRM algorithm tolerates
subject specific outlying response elements [44], and the semi-supervised SRM
capitalizes on categorical stimulus labels when available [45]. Finally,
estimating the SRM from functional connectivity data rather than response time
series circumvents the need for a single shared stimulus across subjects;
connectivity SRM allows us to derive a single shared response space across
different stimuli with a shared connectivity profile [46]''
\citep{kumar2020brainiak}.

``Figure 2: Schematic of the shared response model (SRM). (a) Data are typically
split into a training set (light gray) used to estimate the SRM and a test set
(dark gray) used for evaluation. The SRM is estimated from response time series
from the training set for multiple subjects (top left; transposed here for
visualization). The multi-subject response time series are decomposed into a set
of subject- specific orthogonal topographic transformation matrices and a
reduced-dimension shared response space. The learned subject-specific
topographic bases can be used to project test data (bottom left) into the shared
space. This projection functionally aligns the test data''
\citep{kumar2020brainiak}.

\citep{chen2015reduced}


\subsection{What we want to know}
% main idea
We want to predict location, size, and shape of a functional area in
an individual person based on location, size, and shape of the same functional
area in persons of a reference group.

%
We want to know if functional alignment based on a \ac{srm} improve the
prediction compared to a prediction based on merely anatomical alignment?
%
We want to know how the parameter ``data points'' influences the results.
%
How much data do we need to align an individual subject to a/the \ac{cms} and
outperform a prediction based on anatomical alignment?
%
Is a short diagnostic run ``sufficient''?

\subsection{What we have to do}

We will predict the location of the functional area by performing an
anatomical alignment as a benchmark, and explore possibly improved prediction
performance using a functional alignment

Steps for prediction using anatomical alignment to a brain template (anatomical
standard space):
%
conduct the localizer experiment to get the brain responses in the reference
group
%
align the brains in that reference group anatomically to the standard brain
%
also align your left-out subject to the standard brain, which gives you a
transformation matrix
%
then you use the inverse of that transformation matrix to project the data
%
from the reference group into the anatomy of the left-out subject, which is
essentially the prediction

Steps for prediction using functional alignment to a \ac{cms} (functional
standard space):

%
We let participants watch the movie and audio-description of Forrest Gump,
assuming that the naturalistic stimuli trigger, among others, brain responses
that a similar to those triggered by the functional localizer
%
create CMS from brain patterns (during natural stimulation) in reference group

From that stimuli, you get the time-series per voxel and per subject, and what
the algorithm essentially does is: it learns responses that are shared across
participants (which is the common model space)
%
algorithm also learns individual transformation matrices.
%
project localizer data of reference group into CMS
%
align left-out subject with CMS using a varying amount of data (1-8
%
runs je per stim.) data (parts) need to be of the same stimulation used to
create the CMS (cause we align time-series not connectivity profiles) result:
transformation matrix
%
project localizer data from CMS into left-out subject  using the inverse matrix
from step 3
%
compare predicted localizer results vs. empirical results
%
compare steps 1-5 to a similar procedure using anatomical alignment

%
Additionally for anatomical alignment: ``Partial alignment''.
%
e do not use the same time-series that was used to
create the \ac{cms} but only a part of it.
%
align left-out subject to model space using varying amount of data of
movie or audiobook - transform localizer results into left-out subject
%
Test which amount of data is needed to do an alignment to the common
model space that provides transformation matrices that outperform a prediction
using just an anatomical alignment.

%
\todo[inline]{model validation: needs to be done but not mentioned here: model
validation: predict localizer results using localizer TRs for alignment; across
subjects?}


\subsection{Hypotheses}
%
functional alignment outperforms anatomical alignment.
%
The more data for alignment the better.
%
for discussion: asymptotical curve might be different for another brain region
(we look at perception; what about ``higher'' cognition or even executive
functions?)


\section{Methods}

\todo[inline]{taken from naturalistic PPA paper}

% intro
We used components of the publicly available
\href{http://www.studyforrest.org}{studyforrest.org} dataset that has
been repeatedly used by other research groups in independent studies
(\citep[e.g.,][]{ben2018hippocampal, jiahui2020predicting, hu2017decoding,
lettieri2019emotionotopy, nguyen2016integration}).
% used studies
The same participants were
% AD
a) listening to the audio-description \citep{hanke2014audiomovie} of
the movie ``Forrest Gump'',
% AV
b) watching the audio-visual movie \citep{hanke2016simultaneous}, and
% VIS
c) participating in a dedicated six-category block-design visual localizer
\citep{sengupta2016extension}.
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications. Following is a summary of the most important aspects.


\subsection{Participants}

\todo[inline]{taken from naturalistic PPA paper}

% we get the data from the naturalistic PPA paper (its subdataset)
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in\_bold3Tp2/sub-??\_task-a?movie\_run-?\_bold*.*


% AD study
In the audio-description study \citep{hanke2014audiomovie}, 20 German native
speakers (all right-handed, age 21–38 years, mean age 26.6 years, 12 male)
listened to the German audio-description \citep{ForrestGumpGermanAD} of the
movie ``Forrest Gump'' \citep{ForrestGumpMovie}.
% AV study
In the movie study \citep{hanke2016simultaneous}, 15 participants (21–39 years,
mean age 29.4, six female), a subgroup of the prior audio-description study,
watched the audio-visual movie with dubbed German audio track
\citep{ForrestGumpDVD}.
% VIS study
In the block-design localizer study \citep{sengupta2016extension}, the same 15
participants took part in a six-category block-design visual localizer.
% participants' health
All participants reported to have normal hearing, normal or corrected-to-normal
vision, and no known history of neurological disorders.
% compensation, consent and shit
In all studies, participants received monetary compensation and gave written
informed consent for their participation and for public sharing of obtained data
in anonymized form. The studies had prior approval by the Ethics Committee of
Otto-von-Guericke University of Magdeburg, Germany.


\subsection{Stimuli and Procedure}

\todo[inline]{taken from naturalistic PPA paper}

% AD & AV stimulus name & references
The German DVD release \citep{ForrestGumpDVD} of the movie ``Forrest Gump''
\citep{ForrestGumpMovie} and its temporally aligned audio-description
\citep{ForrestGumpGermanAD} served as naturalistic stimuli, with an approximate
duration of two hours, split into eight consecutive segments of
\unit[$\approx$15]{minutes}.
% AD: additional narrator
The audio-description adds another male narrator to the voice-over narration of
the main character Forrest Gump. This additional narration describes essential
aspects of the visual scenery when there is no off-screen voice, dialog, or
other relevant auditory content.
% task
For all sessions with naturalistic stimuli, participants were instructed to
inhibit physical movements except for eye-movements, and otherwise to simply
``enjoy the presentation''.
%
For details on stimulus creation and presentation see
\citet{hanke2014audiomovie, hanke2016simultaneous}.

% VIS study picture categories
Stimuli for the block-design localizer study were 24 unique grayscale images of
faces, bodies, objects, houses, outdoor scenes and scrambled images, matched in
luminance and size, that were previously used in other studies
(\citep[e.g.,][]{haxby2011common}).
% procedure: presentation & instructions
Participants performed a one-back image matching task for four block-design
runs, with two \unit[16]{s} blocks per stimulus category in each run.
%
For details on stimulus creation and presentation see
\citet{sengupta2016extension}.


\subsection{Stimulation setup}

\todo[inline]{taken from naturalistic PPA paper}

% AD
In the audio-description study, visual instructions were presented on a
rear-projection screen inside the scanner bore. During the functional scans, the
projector presented a medium gray screen with the primary purpose to illuminate
a participant's visual field in order to prevent premature fatigue.
% AV & VIS
In the movie and block-design localizer study, visual instructions and stimuli
were presented on a rear-projection screen
% screen size \unit[23.75 $\times$ 10.25]{cm}
at a viewing distance of \unit[63]{cm}, with a movie frame projection size of
approximately \unit[21.3]$^{\circ}$ $\times$ \unit[9.3]$^{\circ}$.
% angle of view: VIS
In the block-design localizer study, stimulus images were displayed at a size of
approximately \unit[10]$^{\circ}$ $\times$ \unit[10]$^{\circ}$ of visual angle.
% AD & AV: auditory stimulation
Auditory stimulation was implemented using custom in-ear (audio-description), or
over-the-ear headphones (movie), which reduced the scanner noise by at least
\unit[20–30]{dB}.


\subsection{fMRI data acquisition}

\todo[inline]{taken from naturalistic PPA paper}

Gradient-echo fMRI data for the audio-description study were acquired using a
\unit[7]{Tesla} Siemens MAGNETOM magnetic resonance scanner equipped with a 32
channel brain receive coil at \unit[2]{s} repetition time (TR) with 36 axial
slices (thickness \unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane
resolution, \unit[224]{mm} field-of-view, anterior-to-posterior phase encoding
direction) and a \unit[10]{\%} inter-slice gap, recorded in ascending order.
% slice orientation
Slices were oriented to include the ventral portions of frontal and occipital
cortex while minimizing intersection with the eyeballs.
% FOV
The field of view was centered on the approximate location of Heschl's gyrus.
% motion correction
EPI images were online-corrected for motion and geometric distortions.

% AV & VIS
In the movie and block-design localizer study, a \unit[3]{Tesla} Philips Achieva
dStream MRI scanner with a 32 channel head coil acquired gradient-echo fMRI data
at \unit[2]{s} repetition time with
% slices
35 axial slices (thickness \unit[3.0]{mm}, \unit[10]{\%} inter-slice gap) with
\unit[80 $\times$ 80]{voxels} (\unit[3.0 $\times$ 3.0]{mm} of in-plane
resolution, \unit[240]{mm} field-of-view) and an anterior-to-posterior phase
encoding direction, recorded in ascending order.
% no. of volumes
A total of 3599 volumes were recorded for each participant in each of the
naturalistic stimulus paradigms (audio-description and movie).

% visual localizer: 4 x 156 TR
A total of 624 volumes were recored for each participant across the four runs of
the visual localizer experiment

\todo[inline]{taken from Ben-Yakov 2018}
%
Imaging was performed on a 3 T Achieva
scanner (Philips Medical Systems) using a 32-channel head coil.  High-resolution
T1- weighted structural images (Hanke et al., 2014) were acquired using a 3D
turbo field echo sequence [acquisition voxel size of 0.7 mm with a 384 x 384
in-plane reconstruction matrix (0.67 mm isotropic resolution), TR = 2500 ms, TE
= 5.7 ms, TI = 900 ms, flip angle = 8°, FOV = 191.8 x 256 x 256 mm, bandwidth
144.4 Hz/px, sense reduction AP 1.2, RL 2.0]. Functional images (Hanke et al.,
2016) were acquired using a gradient-echo, T2*-weighted EPI sequence (TR = 2000
ms, TE = 30 ms,flip angle = 90°, 35 axial slices with 3.0 mm thickness and a 10%
gap, FOV = 240 x 240 mm, voxel size = 3 x 3 x 3 mm). Slices were auto- matically
positioned in AC–PC orientation using SmartExam (Philips) such that the topmost
slice was at the superior edge of the brain.


\subsection{Preprocessing}

\todo[inline]{taken from naturalistic PPA paper}

% data sources
The current analyses were carried out on the same preprocessed fMRI data
\citep{hanke2016aligned} that were used for the technical validation analysis
presented in \citet{hanke2016simultaneous}.
% exclusion of VP 10
Of those 15 participants in the studyforrest dataset that took part in all three
experiments, data of one participant were dropped due to invalid distortion
correction during scanning of the audio-description stimulus.
% preprocessing of pre-aligned data
Data were corrected for motion, aligned with and re-sliced onto a
participant-specific BOLD template image \citep{sengupta2016extension} (uniform
spatial resolution of \unit[2.5$\times$2.5$\times$2.5]{mm} for both
audio-description and movie data).
% preprocessing intro
Preprocessing was performed by FEAT v6.00 (FMRI Expert Analysis Tool
\citep{woolrich2001autocorr}) as shipped with FSL v5.0.9
(\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software Library}
\citep{smith2004fsl}) on a computer-cluster running
\href{http://neuro.debian.net}{NeuroDebian} \citep{halchenko2012open}.



\subsubsection{Naturalistic stimuli}

\todo[inline]{following procedure in study 2 might not apply to study 3}


\paragraph{first-level analysis}

% install naturalistic ppa analysis as subdataset
% comprises  "aligned-data", templates etc. as subdatasets
% datalad install -d . -s https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis

% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in_bold3Tp2/sub-??_task-a?movie_run-?_bold*.*
The same input data used in study2 were downloaded as subdataset in
\citep{haeusler2021ppadata} for the creation of the \ac{cms}.

% get motion correction parameters for AO data
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/phase1
% datalad get inputs/studyforrest-ppa-analysis/inputs/phase1/sub???/BOLD/task001\_run00?/bold\_dico\_moco.txt
%
In order to use the data of the naturalistic stimuli for the creation of the
\ac{cms}, we preprocessed them in the same way as in
\citep{haeusler2022processing} by rerunning the first-level analysis:

\todo[inline]{taken from naturalistic PPA paper}

% temporal filtering
For the present analysis, the following additional preprocessing was performed.
High-pass temporal filtering was applied to every stimulus segment using a
Gaussian-weighted least-squares straight line with a cutoff period of
\unit[150]{s} (sigma=\unit[75.0]{s}) to remove low-frequency confounds.
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
Data were spatially smoothed applying a Gaussian kernel with full width at half
maximum (FWHM) of \unit[4.0]{mm}.
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% pre-whithening
Correction for local autocorrelation in the time series (prewhitening) was
applied using FILM (FMRIB's Improved Linear Model \citep{woolrich2001autocorr})
to improve estimation efficiency.


\paragraph{second-level analysis}

%% templates and transforms
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/sub-*/bold3Tp2/;
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/templates/*

% rerun second-level analysis
The second-level analysis was rerun to obtain the (un)thresholded $z$-maps.
\todo[inline]{was this even necessary???}


\subsubsection{Visual localizer}
% install VIS dataset as subdataset
% datalad install -d . -s https://github.com/psychoinformatics-de/studyforrest-data-visualrois.git inputs/studyforrest-data-visualroi
% datalad get inputs/studyforrest-data-visualrois/src/aligned/sub-*/in\_bold3Tp2/sub-*\_task-objectcategories\_run-*\_bold.nii.gz
% datalad get inputs/studyforrest-data-visualrois/sub-*/onsets/run-*/*.txt
In order to use the data of the visual localizer for the creation of the
\ac{cms}, we preprocessed them in the same way as in the original study
\citep{sengupta2016extension} by rerunning the first-level analysis:

% condor\_submit code/despike.submit
We despiked the ``raw'' data
(inputs/studyforrest-data-visualrois/code/despike.submit).
% rerun the analysis
% ./code/generate\_1st\_level\_design.sh
% condor\_submit code/compute\_1stlvl\_glm.submit
\todo[inline]{get the parameters from the design-files; cf. with VIS paper}


\subsection{Creation of common model space via Shared response model}

% introductory stuff
\citep{chen2015reduced}: let the algorithm learn the shared responses which is
the common model space, and it also learns the transformation matrices.
%

\todo[inline]{grand\_mean\_for\_4d.py (formerly: data\_normalize\_4d.py)}

\todo[inline]{actually, not necessary anymore 'cause FSL seems to have applied
grand mean scaling to 'filtered\_func\_data.nii.gz)'}

\todo[inline]{data\_mask\_concat\_runs.py will apply scipy's
    preprocessing.StandardScaler() before concatenation anyway}

% grand mean scaling for 4d data:
% voxel values in every image are divided by the average global mean
% intensity of the whole session. This effectively removes any mean global
% differences in intensity between sessions.

% FSL User Guide:
% filtered_func_data will normally have been temporally high-pass filtered,
% it is not zero mean; the mean value for each voxel's time course has been
% added back in for various practical reasons.
% When FILM begins the linear modelling, it starts by removing this mean.

% input: 'sub-*/run-?.feat/filtered_func_data.nii.gz' of VIS, AO, AV
% -> should already grand mean scaled

%
The time series of every experiment, subject, and run that has been filtered
by FSL ('filtered\_func\_data.nii.gz') were used.
%
Given that FSL adds back the mean value for each voxel's time course at the
and of the preprocessing it was subtracted again.
%
The each voxel was multiplied by 10000%
and saved to 'sub-??\_task-*\_run-?\_bold\_filtered.nii.gz'

\subsection{ROI creation}

\todo[inline]{problem 1: grpPPA contains n=14 subject, not n-1 subjects}

\todo[inline]{problem 2: still, there are voxels outside of the PPA-mask;
probably, because of the warping procedures}

% masks-from-mni-to-bold3Tp2.py:
% - merges unilateral ROIs overlaps (already in MNI) to bilateral ROI
% - output: 'masks/in_mni/PPA_overlap_prob.nii.gz'
% - warps union of ROIs from MNI into each subjects space
% output: 'sub-*/masks/in_bold3Tp2/grp_PPA_bin.nii.gz' + audio_fov.nii.gz dilate
% the ROI masks by 1 voxel; output: 'grp_PPA_bin_dil.nii.gz'

% masks-from-mni-to-bold3Tp2.py:
% warp MNI masks into individual bold3Tp2 spaces

% masks-from-t1w-to-bold3Tp2.py:
% transforms 'inputs/tnt/sub-*/t1w/brain_seg*.nii.gz'
% into individual's bold3Tp2
% output: 'sub-*/masks/in_bold3Tp2/brain_seg*.nii.gz'

% mask-builder-voxel-counter.py:
% builds different individual masks by dilating, merging other masks
% creates a FoV of AO stimulus for every subject from 4d time-series of AO run
% output: sub-*/masks/in_bold3Tp2/audio_fov.nii.gz'
% counts the voxels
% long story short: we cannot used all gyri that contain PPA to some degree
% even if the mask by FoV of AO stimulus and individual gray matter mask

% data_mask_concat_runs.py:
For each subject, and experiment, the corresponding 4d time-series of each run
were masked with the 'grp PPA' (but undilated and not masked with individual
gray matter mask) multiplied by the individual's \ac{fov} of the
audio-description.

%
Number of voxel within the mask can be seen in \ref{tab:ppamaskvoxels}.

\begin{table*}[btp]
\caption{Number of voxels within the union of individual PPAs projected back
    into individuals' subject-space.}
\label{tab:ppamaskvoxels}
\begin{tabular}{ll}
\toprule
\textbf{Subject} & \textbf{no. of voxels} \\
\midrule
sub-01 & 1665 \tabularnewline
sub-02 & 1732 \tabularnewline
sub-03 & 1400 \tabularnewline
sub-04 & 1575 \tabularnewline
sub-05 & 1664 \tabularnewline
sub-06 & 1951 \tabularnewline
sub-14 & 1376 \tabularnewline
sub-09 & 1383 \tabularnewline
sub-15 & 1683 \tabularnewline
sub-16 & 1887 \tabularnewline
sub-17 & 1441 \tabularnewline
sub-18 & 1729 \tabularnewline
sub-19 & 1369 \tabularnewline
sub-20 & 1437 \tabularnewline
\bottomrule
\end{tabular}
\end{table*}

% scaling
Then, data of every run were scaled using scipy's preprocessing.StandardScaler()
% output: 'sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy; and:
% 'sub-*_task_visloc_run-1-4_bold-filtered.npy'
Given that the last 75TR of the last run of the audio-description were missing
in subject 04 due to WHAT?, we also dropped the last 75 TR from the other
participants data since it these TRs consisted just of the credits of the movie
anyway.
% AO + AV has 7123 TRs; not 7198 volumes anymore
As a result the data to create the CMS consisted of 3524 TRs of
audio-description, 3599 TRs of the movie and 4x156 = 624 TRs of the visual
localizer experiment.

\subsubsection{Fitting of shared response model}

\todo[inline]{check brainIAK tutorial; how to cite brainIAK?}

\todo[inline]{check die benutzen Befehle in BrainIAK docs}

% data_srm_fitting.py
% output: 'test/sub-??/srm-ao-av-vis\_feat10-iter30.npz

\todo[inline]{2 kinds of flavours: 'ao \& av \& vis' vs. only 'ao \& av'}

leave-one-subject-out folding scheme [?], creating von CMS for every (left-out)
subject from other subjects' data

Fore each subject, we created a \ac{cms} based on other subject's data.
%
The runs of the audio-description, movie, and visual localizer were concatenated
to one continuous and z-scored using scipy.stats.zscore().

% BrainIAK

BrainIAK v0.11 (Brain Imaging Analysis Kit) \citep{kumar2020brainiak, kumar2020brainiaktutorial}
% features
Model fitting was performed using 10 features [WHY?] and 30 iterations; calling
the fit method if brainIAK's srm object (brainiak.funcalign.srm.SRM)

% negative control; output: 'test/sub-01/srm-ao-av-shuffled\_feat10-iter30.npz'
As negative control, we also fitted a shared response model to after shuffling
the order of runs of audio-description and movie (but not visual localizer) of
every subject before fitting the model.


\paragraph{plot\_srm.py}

\todo[inline]{imo not very informative}
%
plots SRM (features*time points) created for a left-out subject; plots
time-course of top 3 responses (currently, TRs 0-800 = quarter of AO); plots
distance matrix of time points in shared space


\paragraph{plot\_corr-of-glm-and-srm.py}
%
\begin{itemize}
    \item plot correlation matrix between
    \item regressors of the naturalistic stimuli (and some combinations, e.g.
        geo\&groom, geo\&groom\&furn
    \item and shared responses
    \item at the moment, it correlates only with AO TRs (and wrongly uses
        0:3599 TRs)
    \item ToDo: at the moment in\_dir with hardcoded model 'srm-ao-av'
    ->  means: no vis, not shuffled; correlate AO regressors with correct AO
        TRs; correlate AO regressors with AV TRs; correlate AV regressors with
        AV TRs; correlate AV regressors with AO TRs; correlate AO and AV
        regressors with all TRs (+VIS?)
\end{itemize}


\subsubsection{Partial alignment}

\todo[inline]{matrices have been computed CMS created from AO+AV+VIS, but also
just from AO+AV}

\todo[inline]{perform alignment wit TRs of localizer for
cross-subject-prediction; we do cross-subject-cross-experiment-prediction}

In order to derive transformation matrices based on response patterns.

\paragraph{get\_wmatrix\_for\_left-out.py}
% AO: 0-451, 0-892, 0-1330, 0-1818, 0-2280, 0-2719, 0-3261, 0-3524
% AV: 3524-3975, 3524-4416, 3524-4854, 3524-5342, 3524-5804, 3524-6243,
%     3524-6785, 3524-7123
% AO+AV: 0-7123

% input 'sub-*/sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy'
We used an increasing number of runs of the filtered audio-description and movie
data to align each left-out subject to the corresponding \ac{cms}, and obtain
the subject's transformation matrix.
%
The SRM object was sliced according to the used runs and the weight matrix was
obtained calling the SRM method srm\_sliced.transform\_subject(runsdata)
% output: f'wmatrix_{model}_feat{n_feat}_{start}-{end}.npy'

imo, SRM uses procrustes transformation, too, hence: ``the searchlight response
hyperalignment algorithm, which utilizes Pro- crustes transformations to
calculate a transformation matrix for each participant that maps their AA data
into a shared high-dimensional in- formation space shared across participants
(Guntupalli et al., 2016)'' \citep{busch2021hybrid}.

\paragraph{predict\_ppa.py}

\todo[inline]{mention that anatomical transformation matrices were precomputed
and part of the studyforrest dataset}
\todo[inline]{probably tell a little more how the transformation matrices were
created; reviewer's question: your anatomical alignment might have been shitty}
\todo[inline]{check TNT github/gin repo; what is the corresponding paper?}

% prediction from anatomy
We first created the template/prediction from the anatomy of other subjects.
%
We warped/mapped [what kind of transformation?] the results of the visual
localizer (and results of the auditory naturalistic stimulus) from each
individual's subject-space (bold3Tp2) into MNI space (grpbold3Tp2).
% output 1: 'test/masks/in_mni/sub-*_VIS-PPA.nii.gz' output 2:
% 'test/masks/in_mni/sub-*_AO-PPA.nii.gz'
The results of all subjects were warped from MNI space in to the left-out
subject's space
% output 1: 'test/sub-*/masks/in_bold3Tp2/sub-*_VIS-PPA.nii.gz' output 2:
% 'test/sub-*/masks/in_bold3Tp2/sub-*_AO-PPA.nii.gz'
The transformed z-maps were masked with mask of unions of PPAs and the
individual's \ac{fov}.
%
The mean of these z-maps served as the prediction for the left-out subject.
% output1: 'test/sub-*/predicted-VIS-PPA_from_anatomy.nii.gz' output2:
% 'test/sub-*/predicted-AO-PPA_from_anatomy.nii.gz'

% prediction from CMS
We then predicted the $Z$-maps of the PPA from the visual localizer experiment
and the auditory naturistic stimulus.
%
The transformation matrices that were created using an increasing number of runs
of each naturalistic stimulus were used to transform a $Z$-map templated created
in the \ac{cms} into the left-out subject's space
% aligned zmaps to shared space: (k features x t time-points); 1 time-point
% because it's a zmap no time-series) using

First, individual results from the visual localizer and auditory naturalistic
stimulus were masked with the group PPA mask in each individuals subject space
and then transformed from each individual's space into the \ac{cms} [by calling
'zmaps\_in\_cms = srm.transform(masked\_zmaps)'].
% zmap from CMS into subject space: compute dot product of left-out subject
% weight-matrix with zmap in cms
Using the inverse of the left-out subject's transformation matrix, the zmaps
from the other subjects that were projected into \ac{cms} were then projected
into the anatomy of the left-out subject.
%
Here again, the mean of these z-maps served as the prediction for the left-out
subject.

\todo[inline]{i.e.: in both cases the mean was computed in anatomical space;
this is more 'similar' than computing the mean in anatomy for prediction from
anatomy, and computing the mean in CMS for prediction from CMS; but: creating
the template in CMS does not lead to different results anyway}

% correlation between empirical z-map & predicted z-maps
Finally, we calculated the Pearson correlation coefficients between the
empirical z-maps of each subject (visual localizer and auditory PPA) and the
values that were predicted from other subjects' anatomy.
%
Further, we calculated the Pearson correlation coefficients between the
empirical z-maps of each subject (visual localizer and auditory PPA) and the
values that were predicted from using an increasing number of runs of the
naturalsitic stimuli to obtain the transformation matrices.


\subsubsection{alternative template creation}
%
lastly, we currently apply another method to get the „z-map template“ on which
the prediction is based ('test/data\_denoise-vis.py';
'test/data\_srm-vis-to-ind.py')

%
``An added benefit is that SRM helps address the data starvation problem above:
because the SRM space is by definition shared across individuals, data from
multiple participants can be combined prior to MVPA or other analyses''
\citep{cohen2017computational}.


\section{Results}

\todo[inline]{Neurovault, \citep{gorgolewski2015neurovault}}

%
For every subject, we see the correlation of z-maps that tell us the, quote
``real'' unquote, PPA and predicted PPA
%
In green, we see the correlations between empirical values from the localizer \&
the predicted values using anatomical alignment
%
In orange, we see the correlations between empirical values \& the predicted
values using parts of the movie
%
In blue, we see the correlations between empirical values \& the predicted
values using parts of the audio-description
%
I marked subject 4, because I want to show you how results look like in a
horizontal slice of the brain of subject 4
%
We have these nice blurry EPI-images and all z-maps are threshold at a value of
bigger than 2.3.
%
always in red, we can see the z-map from the localizer experiment across the
whole brain,
%
the region of interest that we used is white and the predicted values, are blue
%
The prediction using anatomical alignment and the prediction using 15 minutes of
movie data show a correlation of about .7
%
the prediction using 15 minutes of the audio-description correlates about 0 with
the empirical z-map
%
The last one is the extreme case, but it can give you an idea of how the z-maps
look in a slice of the brain


\subsection{predict\_ppa.py: outputs also the Pearson correlations}


\subsection{plot\_stripplot.py}

the lovely stripplots of correlations

\subsection{statistics\_t-test-correlations.py}

Compute if differences between kinds of prediction is significant


\subsubsection{test/statistics\_cronbachs.py}

\subsubsection{plot\_bland-altman.py}

I hate that script

% corrstats.py; not necessary anymore; calculates the statistical significant
% differences between two dependent or independent correlation coefficients


\section{Results}

\todo[inline]{plot of the union of PPA mask}



\section{Discussion}


\subsection{Short Summary}

What we did. What the results are

\subsection{Discussion of current results}

%
15 min of movie watching used for functional alignment outperform prediction
using anatomical alignment

%
30 minutes of movie watching outperform 15 minutes of movie watching

%
more than 30 minutes do not lead to a significantly improved prediction
performance.
%

\subsection{self-critique; short comings}


\subsubsection{localizer is ``ground truth''}

\citet{lilienfeld2015fifty} on ``gold standard'': ``In the domains of
psychological and psychiatric assessment, there are precious few, if any,
genuine ``gold standards''. Essentially all measures, even those with high
levels of validity for their intended purposes, are necessarily fallible
indicators of their respective constructs (Cronbach and Meehl, 1955; Faraone and
Tsuang, 1994). As a consequence, the widespread practice referring to even
well-validated measures of personality or psychopathology, such as Hare’s
(1991/2003) Psychopathy Checklist-Revised, as ``gold standards'' for their
respective constructs (Ermer et al., 2012) is misleading (see Skeem and Cooke,
2010). If authors intend to refer to measures as ``extensively validated'', they
should simply do so'' \citep{lilienfeld2015fifty}

\citet{scheinost2019ten} in context of phenotypic measures: ``Predictive models
based on neuroimaging data will only ever account for a fraction of the
variance. Neuroimaging studies are limited by how much information the signal
can capture about the measure of interest. At the same time, these studies are
also limited by the chosen phenotypic measure used.  While the success of a
model is evaluated by how well it predicts a phenotypic measure (and these
phenotypic measures have to be treated as gold standards), it is well known that
such measures are not always the ground truth but themselves suffer from
confounds and noise.  When studying brain-behavior associations, one must keep
in mind how extraordinary it is that neuroimaging data can be distilled to
approximate phenotypic measures that reflect a simplification of multiple
complex features. Thus, even modest results are reasonable and remarkable. For
a discussion on the reliability of phenotypic measures in the context of
predictive modeling, we point the interested reader to: [Dubois et al., 2018a,
2018b; Gignac and Bates, 2017]'' \citep{scheinost2019ten}.

``the identiﬁcation of the PPA is complicated by (at least) four
methodological considerations. First, the PPA deﬁnition may depend on the type
of experiment, task, and stimuli used. Second, the boundaries of the PPA may
depend on the statistical threshold used. Third, the spatial extent and
localization of the PPA may vary if deﬁned within the native brain space of an
individual or based on a group analysis. Fourth, the size of the PPA may depend
on data acquisition choices (e.g. large vs. small voxels) and data analysis
choices (e.g. liberal smoothing vs. no spatial smoothing). The present study
aims to identify and to predict the most probable location of place-selective
voxels within medial VTC of an individual brain that is impervious to these
methodological decisions'' \citep{weiner2018defining}.


\subsubsection{SRM and (dropped) individual idiosyncrasies}

``The flip side of focusing on shared responses is to focus on responses that
are idiosyncratic to individuals.
%
Although these responses are excluded in SRM, they are not necessarily noise and
may in fact be highly reliable within participants.
%
Indeed, SRM can be used to isolate participant-unique responses by examining the
residuals after removing shared group responses, or it can be applied
hierarchically to the residuals to identify subgroups \citep{chen2017shared}
\citep{cohen2017computational}.
%
More generally, there is a growing trend toward investigating individual
differences as another source of meaningful variance in fMRI [73].
%
Recognizing that signal exists beyond the average or shared response of a group,
such studies exploit idiosyncratic but stable responses to account for
previously unexplained variance in brain function, behavioral performance and
clinical measures [70,74]'' \citep{cohen2017computational}.


\subsubsection{volume-based registration vs. surface-based normalization}
%
Volume vs. surface: e.g. \citep{desai2005volumetric}

Sengupta did analyses in volume space;
%
I could have run the analyses/contrasts on the surface
%
but: opportunity costs?
%
in a broader broader context as an issue of data sharing (if at all, a point for
general discussion); how well can you trust data that you did not collect
yourself (a.k.a. ``did you consider x?'', ``no, does not matter in our case'',
``but will matter for the person who will use/have used your data'')

Compare to hyperalignment or Multimodal Surface Matching (Robinson et al. 2014)


\subsection{Future questions}

\subsubsection{predict other t-contrasts of higher-visual area localizer}

\subsubsection{predict other localizers}
%
e.g. retinotopy, language areas


\subsubsection{create CMS from other study}
%
we currently have a cross-subject and cross-experiment prediction,
but we do not have a (real) cross-scanner prediction
%
create a CMS from another experiment’s data,
using another scanner and hopefully more subjects
%
In case of an alignment of time-series,
that experiment needs, at least, a part of Forrest Gump as an intersection


\subsubsection{ROI vs. whole-brain / searchlight}
%
Our union of PPA is XX voxels big.
%
Influence of voxel count on performance; take searchlight (but how big should it
be?

% Guntupalli's searchlight paper
Later, \citet{guntupalli2016model} showed that this approach can be extended to
predict functional organization across large proportions of the cortical
surface, for example to predict the represented visual field coordinate in
visual cortex based on retinotopic mapping scans of other individuals

``Applying SRM to a large swath of the brain means that all voxels within the
region contribute to the final derived metric. This can conflict with the goal
of associating spatially local activity with specific cognitive functions. To
address such issues, SRM can be applied in small overlapping searchlights to
obtain localized metrics of shared information [71,99]
\citep{cohen2017computational}''.


\subsubsection{connectivity-based}

\todo[inline]{kind of a killer cause you do not need intersection of
time-series; but s. Guntupalli's paper: time-series hyperalignment outperforms
connectivity-based hyperalignment (?)}

% from project proposal
``In his doctoral thesis, recently submitted to the Faculty of Natural Sciences
in Magdeburg, Falko Kaule showed that congruent time-locked BOLD responses
across subjects (i.e. all subjects watching the exact same full-length movie) as
used by Haxby and colleagues are not required to derive a valid alignment of
individuals with a common representational space \citep{kaule2017examination}.
%
Comparable prediction performance can be achieved by using \textbf{functional
connectivity patterns} (correlation of a voxel's time series with reference
regions in the same brain)'' (s. dissertation project proposal).


% Nastase's ugly mofo paper
\citep{nastase2019leveraging}


\subsection{Vision}

``Calibration scan'' to align to \ac{cms} and atlas of reference group.
%
Once a valid alignment is established, known functional properties of a
(normative) reference, derived from extensive scans and analysis of other
subjects, can then be projected into the respective individual voxel space (s.
Fig. 1 in \citep{nishimoto2016lining}).
%
Imagine you scan a new, unknown subject for just 15 minutes more, and you
additionally get results from a whole variety of other paradigms mapped onto
that brain
%
Results from localizers of low-level perceptual processes, but also higher-level
cognitive processes like language, memory, emotions and so on
%
Kinda ``adventurous'':If you have different common model spaces for different
subgroups, you can investigate which alignment onto which ``subgroup common
model space'' results in less error, that lets you classify to which subgroup
your new subject might belong.


\section{Conclusion}
