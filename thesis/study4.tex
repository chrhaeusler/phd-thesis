\section{Abstract}
% intro
\textit{Intro:} In order to map perceptual or cognitive functions onto the brain
anatomy of study participants, researchers usually conduct dedicated experiments
\textit{functional localizers} often accompanied with a task.

% problem
\textit{Current approach \& problem:} Nevertheless, the approach ``one paradigm
to map one domain of brain functions'' becomes impractical if a variety of
domains is supposed to be mapped in a time-efficient manner.

% therefore
\textit{Therefore:} In the current study, we explore a way and quantity of data
needed to (reliably) predict individual and idiosyncratic functional
topographies by projecting results of a localizer experiment (statistical
$Z$-maps) from a reference group into the brain anatomy of individual
participants.

\textit{Method:}
% data
During functional magnetic resonance imaging (fMRI), participants ($N=14$) took
part in a task-based, block-design visual localizer and two naturalistic stimuli
paradigms: an audio-visual movie and the movie's audio-description, both
paradigms free of any task.
% cms creation
Based on these response time-series, we first created a common model space
employing a shared response model \citep{chen2015reduced} following a $N-1$ fold
procedure [correct term??], a process that also computed transformation matrices
for the subjects that provided the data for the creation of the model space.
% alignment
Then, we aligned left-out subjects with the common [via Procusted
transformation?] to derive transformation matrices for the left-out-subjects by
also varying the quantity of functional response time-series to perform the
alignment.
% prediction
Lastly, the acquired transformation matrices were used to project the functional
topographies from the anatomy of the reference group into the common model
space, and from the common model space into the anatomy of the left-out
subjects.
% stimulus length?
[We assessed the relationship between length of naturalistic stimulation used
for a \textit{partial functional alignment} and the performance of predicting
empirical $Z$-maps.]
%
\textit{Results} suggest that ``a subject's idiosyncratic functional topography
can be estimated with high fidelity from that subject's fMRI data obtained while
watching a naturalistic movie using hyperalignment to project other subjects’
localizer data into that subject's idiosyncratic cortical anatomy''.
\citep{jiahui2020predicting}.

%
\textit{Discussion}: stimulus length: 15-30 min vs. 2h;  results of auditory stimulus to predict visual localizer are ``modest''.

%
\textit{Conclusion}: ``These findings lay the foundation for developing an
efficient tool for mapping functional topographies for a wide range of
perceptual and cognitive functions in new subjects based only on fMRI data
collected while watching an engaging, naturalistic stimulus and other subjects'
localizer data from a normative sample'' \citep{jiahui2020predicting}.


\section{Introduction}

\todo[inline]{mih: Dieses Kapitel sollte konzeptuell und inhaltlich deutlich als
"und jetzt alles zusammen" erscheinen}

\todo[inline]{define PPA in intro shortly (maybe \ac{ffa}, too)}
\todo[inline]{cf. PPA paper}

% brain mapping
Topographic brain mapping maps brain functions, perceptual or cognitive
processes, onto anatomical brain areas (\textit{functional areas}).\todo{better
def. of brain mapping functional areas}

%
``The topographies of category-selective areas are mostly distributed similarly
across individuals, but great individual variability exists in the locus, the
size, and the shape of the category-selective areas [Zhen et al., 2015, 2017]''
\citep{jiahui2020predicting}.

\todo[inline]{examples like PPA and stuff here}

% localizer
Location, size, and shape of functional areas are traditionally determined by
letting study participants perform a task during a \textit{functional localizer}
paradigm.

%
``To deal with the idiosyncratic topography of functional areas,
category-selective areas are identified separately in each individual using a
``functional localizer'' fMRI scan.
%
Functional localizers use a simple contrast between responses to different
categories of stimuli, such as responses to faces versus responses to objects,
to identify category-selective areas or to map a category-selectivity topography
[Saxe et al., 2006]'' \citep{jiahui2020predicting}.

% problem: one localizer for one domain
Functional localizers are designed to maximize detection power and thus
dedicated to map just one domain of brain functions (e.g. different object
categories; \citet{kanwisher1997ffa}) or cognitive processes (e.g. theory of
mind; \citet{spunt2014validating}) and
% which gets messy
Consequently, if a researcher or practitioner wants to map a variety of domains,
the approach ``one paradigm in order to map one domain of functions'' gets
time-consuming and inefficient.
% localizer batteries: intro
Researchers have tried to tackle this issue by creating time-efficient
multi-functional \textit{localizer batteries} \citep{pinel2007fast,
pinho2018individual, pinho2020individual}.\todo{others? H.B.Project?}
% localizer batteries: example
For example, \citet{pinel2007fast} employs a range of dedicated stimuli and
specific tasks in a 5-minute routine to map processes of ``auditory and visual
perception, motor actions, reading, language comprehension, and mental
calculation at an individual level'' \citet{pinel2007fast}.
% task based = shit
Nevertheless, the diagnostic quality of localizer batteries relies heavily on
the participant's comprehension of the task instruction and general compliance,
a criterion that can be difficult to meet in clinical or pediatric populations.

\todo[inline]{following: why predict results of unecological paradigm at all?}
\todo[inline]{better delete here; cf. discussion of ``ground truth''}

% localizer: validity
Further, localizer batteries rely on carefully chosen and tightly-controlled,
simplified stimuli.
%
Given that these simplified stimuli are presented in blocks of stimuli, often
accompanied with a task to keep participants alert and attentive, localizer
paradigms to not resemble how we perceive the real-world during every-day life
leading to questionable external and ecologically validity.\todo{add references}


\subsection{Solution: Predict}

An alternative approach to submit a person or patient to a \ac{fmri} scanning
procedure is to predict a person's functional topography based on data collected
in a reference group. Predictions can be performed based on \textit{anatomical
alignment} or \textit{functional alignment}


\subsubsection{Anatomical alignment}

\todo[inline]{there is ``real'' prediction from anatomy (e.g.  cortical folding)
not using anatomical template to project data into/out of}

% \citep{weiner2018defining} compares cortex-based alignment
% \citep{fischl1999high} with volume-based (Talairach) alignment: ``we repeated
% our leave-one-out cross-validation procedure across all 24 participants with
% an affine volume-based registration to the Talairach brain and compared this
% performance to the same procedure implemented with CBA in FreeSurfer
% \citep{weiner2018defining}''


% current approach
Anatomical alignment (AA) predicts functional topographies based on the
anatomical location, size, and shape of a functional area in the anatomy
reference group (e.g. \citet{weiner2018defining})
% volume based
Volume-based anatomical alignment \citep{evans19933d} aligns voxel-wise data of
individual subjects to a three-dimensional brain template [e.g. MNI152 template;
QUOTE].
% surface-based
Surface-based anatomical alignment \citep{fischl1999cortical}  aligns
vertex-wise data of individual subjects to a two-dimensional template (e.g.
FreeSurfer fsaverage template; \citep{fischl1999high}).
%
The most probable functional topography of a new subject is predicted by
projecting averaged functional data of the reference group through the brain
template into the new subject's subject-specific anatomy.

\paragraph{Functional-anatomical correspondence}

\todo[inline]{shorten and rephrase}
%
Nevertheless, one of ``one of the main obstacles in leveraging brain activity
across subjects is the considerable heterogeneity of functional topographies
from individual to individual. Variability in functional--anatomical
correspondence across individuals means that even high-performing anatomical
alignment does not ensure fine-grained functional alignment [e.g.,
\citet{frost2012measuring}]'' \citep{kumar2020brainiak}.
%
``Fine-grained functional organization of cortex is not well-conserved across
individuals. As a result, individual differences in cortical functional
architecture are confounded by topographic idiosyncrasies--i.e., differences in
functional–anatomical correspondence'' \citep{feilong2018reliable}.

``Functional architecture of the human brain is relatively consistent across
individuals at a coarse scale, but idiosyncrasies in functional topography
become increasingly apparent at finer scales. []. At the areal level,
category-selective regions can be localized to anatomical landmarks [Weiner et
al., 2018, 2014], though the locus can differ across individuals by millimeters
or centimeters, along with variability in size and shape [Zhen et al., 2017,
2015]. Furthermore, within a brain area, between-subject classification of
response patterns is typically considerably worse than within-subject
classification [e.g., Cox and Savoy, 2003; Haxby et al., 2011], indicating that
fine-grained functional architecture is not well-aligned macroanatomically. With
state-of-the-art cortical surface-based alignment [Fischl, 2012], the mismatch
between brain function and anatomy can be reduced but not eliminated [Duncan et
al., 2009; Frost and Goebel, 2012; Weiner et al., 2018]. Therefore, it is
problematic to assume that a given anatomical location or topographic
conformation will have the same functional role across brains''
\citep{feilong2018reliable}.


\subsection{Functional alignment (FA)}

\todo[inline]{shorten and rephrase}

% definition
Functional alignment is a recently developed alternative to anatomical alignment
and promises to preserve more fine-grained idiosyncrasies.

%
Functional alignment algorithms align cortical anatomy of different subjects
based on the maximization of the inter-subject similarity of \ac{bold} responses
\citep{haxby2011common, chen2015reduced, sabuncu2010function} correlating with a
time-locked external stimulation (e.g. movie or auditory narrative), or
connectivity profiles \citep{feilong2018reliable, guntupalli2018computational}.

%
``Functional alignment algorithms that are based on common temporal response
profiles such as Hyperalignment [Haxby, 2011; Guntupalli, 2016] and the Shared
Response Model (SRM) [Chen, 2016] ``attempt to remap regions across participants
and can be highly effective at improving spatial alignment across people based
on common functional responses while still maintaining individual differences
[\citep{feilong2018reliable}]'' \citep{chang2021endogenous}.

%
``Hyperalignment [Guntupalli et al., 2016, 2018; Haxby et al., 2011] is a family
of methods that can disentangle functional variability from anatomical
variability. Hyperalignment projects features (voxels or surface vertices) from
a brain into a common high-dimensional space through linear transformations. In
this common space, the same features from different individuals will share
similar functional properties instead of the same anatomical locations or
topographic conformations. Hyperalignment decomposes the original fMRI data of
each individual into two parts: a transformation matrix, which reflects
topographic properties of the individual's functional activations; and a new
data matrix in the common space, which reflects shared, stimulus-driven
responses. This hyperaligned data matrix provides an opportunity to study brain
functions without confounds from topographic variability''
\citep{feilong2018reliable}.

%
``The dimensions of this common model capture functional profiles that are
shared across individuals such as cortical response profiles collected during a
common time-locked stimulus presentation (e.g. movie viewing) or functional
connectivity profiles'' \citep{busch2021hybrid}.

%
``Hyperalignment derives individual transformation matrices to project
information encoded in idiosyncratic topographies into a common model
information space. These matrices are derived based either on responses to a
naturalistic stimulus, such as a movie, or on functional connectivity
\citep{guntupalli2018computational}'' \citep{jiahui2020predicting}.

%
``Hyperalignment models shared information that is embedded in idiosyncratic
cortical patterns across brains. Modeling shared information makes it possible
to compare functional anatomy across brains at a fine spatial scale.
Hyperalignment projects cortical pattern vectors into a common, high-dimensional
information space (Haxby et al., 2020). Derivation of this common space can be
based on either neural response profiles (e.g. data collected during tasks, such
as movie viewing (Haxby et al., 2011)) or functional connectivity profiles files
[Guntupalli et al., 2018]'' \citep{busch2021hybrid}.

%
``A major objective of the hyperalignment algorithm is to map the shared
information originally found in idiosyncratic cortical topographies into a
common space in which this information is better aligned across participants''
\citep{busch2021hybrid}.


\subsubsection{Prediction using hyperalignment}

%
For example, \citet{haxby2011common, guntupalli2016model} have shown that
``idiosyncratic topographies for category-selectivity and retinotopy can be
estimated in individual brains with high fidelity using hyperalignment to
project other subjects' functional localizer data into a target subject's
ventral temporal and occipital cortical anatomy'' \citep{jiahui2020predicting}.
%
``Our findings show that individually-tailored maps estimated from other
subjects’ data after hyperalignment correlate much more highly with maps
estimated from that subject's own localizer data than does a group average map
based on anatomical normalization'' \citep{jiahui2020predicting}.
%
Hyperalignment, an algorithm that was pioneered by \citet{haxby2011common}, uses
\ac{bold} response patterns to derive a \ac{cms} using a variant of Procrustes
analysis and computes invertible (orthonormal) transformations from each
individual brain's voxel-space into the \ac{cms}.
%
Importantly, the study also showed that an individual's \ac{ffa} or the
\ac{ppa}, can be localized precisely based on data from a reference group.


\subsubsection{Shared response model}

\paragraph{Definition of SRM}

``Another recent paper \citep{chen2015reduced} introduced the Shared Response
Model (SRM), an algorithm that stems from previous work on hyperalignment
\citep{haxby2011common}'' \citep{vodrahalli2018mapping}'s Intro
section.

``The \ac{srm} \citep{chen2015reduced} projects \ac{fmri} responses from
each participant into a low-dimensional space that captures temporal variance
shared across participants [Box 1; Fig. 4–5]'' \citep{cohen2017computational}.

``The Shared Response Model (SRM) \citep{chen2015reduced} is an unsupervised
probabilistic latent variable model for multi-subject fMRI data under a
time-synchronized stimulus. From each subject's fMRI view of the movie, SRM
learns projections to a shared space that captures semantic aspects of the
fMRI response'' \citep{vodrahalli2018mapping}'s method section.

%
``SRM offers jointly factoring each participant's data into a shared set of
feature time series and subject-specific topographies for each feature (Fig.
4)''\citep{cohen2017computational}.

%
``If participants are given the same stimulus or task sequence (for example, a
movie), which leads their brains through a series of cognitive states (for
example, visual, auditory, semantic), then identifying shared variance has the
effect of highlighting variance related to these states''
\citep{cohen2017computational}.

%
``The deterministic SRM algorithm performs a joint Principal Components Analysis
(PCA) and can identify a reduced set of functional response components that
maximally align across participants. This algorithm learns a separate
transformation matrix that projects each participant into this common space
\citep{chen2015reduced}. Estimating a lower-dimensional common model space can
potentially aid in filtering out measurement noise that is assumed to be
independent across individuals'' \citep{chang2021endogenous}'s method
section.

%
``Building on the initial probabilistic SRM formulation \citep{chen2015reduced,
anderson2016enabling}, several variants of SRM have been developed to address
related challenges. For example, a fast SRM implementation has been introduced
for rapidly analyzing large datasets with reduced memory demands
\citep{richard2019fast}. The robust SRM algorithm tolerates subject specific
outlying response elements \citep{turek2018capturing}, and the semi-supervised
SRM capitalizes on categorical stimulus labels when available
\citep{turek2017semi}. '' \citep{kumar2020brainiak}.

%
``SRM estimation is driven by the commonality in functional responses induced by
a shared stimulus . Unlike ISC analysis, which presupposes (often very coarse)
functional correspondence, SRM isolates the shared response while accommodating
misalignment across subjects'' \citep{kumar2020brainiak}.

%
``Building on the initial probabilistic SRM formulation \citep{chen2015reduced,
anderson2016enabling}, several variants of SRM have been developed to address
related challenges. For example, a fast SRM implementation has been introduced
for rapidly analyzing large datasets with reduced memory demands
\citep{richard2019fast}. The robust SRM algorithm tolerates subject specific
outlying response elements \citep{turek2018capturing}, and the semi-supervised
SRM capitalizes on categorical stimulus labels when available
\citep{turek2017semi}. '' \citep{kumar2020brainiak}.

\paragraph{What does SRM do}

``fMRI data are collected from each of m participants experiencing the same
stimulus and then organized into a matrix X (voxels by time). Each matrix X is
then factored using a probabilistic latent-factor model into the product of a
subject-specific matrix W of k brain maps (an orthogonal basis) and a shared
temporal response matrix S of size k by time. That is, for each participant: X =
W S + R, where X, W, and the residuals, R (not shown), are subject-specific, and
S is shared across participant's'' [text of Figure 4 in]
\citep{cohen2017computational}.

%
``SRM decomposes multi-subject fMRI data into a lower-dimensional shared space
and subject-specific transformation matrices for projecting from each subject's
idiosyncratic voxel space into the shared space (Figure 2). Each of these
topographic transformations effectively rotates and reduces each subject's voxel
space to find a subspace of shared features where the multivariate trajectory of
responses to the stimulus is best aligned. These shared features do not
correspond to individual voxels; rather, they are distributed across the full
voxel space of each subject; each shared feature can be understood as a weighted
sum of many voxels'' \citep{kumar2020brainiak}.

%
``The SRM in \citet{chen2015reduced} optimizes the objective
$\sum_{i=1}^{n}||X_{i}-W_{i}S||_{F}$ for a low-dimensional shared space S and
orthogonal-column subject specific maps $W_{i}$, and can be thought of as a
multi-subject extension of PCA.  Simultaneously reducing dimensionality across
subjects outperforms other averaging approaches at matching up specific
timepoints in a movie across subjects'' \citep{vodrahalli2018mapping}'s Intro
section.

``Specifically, SRM learns $N$ maps $W_{i}$ with orthogonal columns such that
$||X_{i}-W_{i}S||_{F}$ is minimized over $\left\{ W_{i}\right\} _{i=1}^{N},S$,
where $X_{i}\in\mathbb{R}^{v\times{T}}$ is the $i^{th}$ subject's fMRI response
($v$ voxels by $T$ repetition times) and $S\in\mathbb{R}^{k\times{T}}$ is a
feature time-series in a $k$-dimensional shared space. In this paper, $k=10$
since low-rank SVD [Singular Value Decomposition?] with 20 dimensions captures
90\% of the variance of the original fMRI matrices \citep{chen2015reduced}. We
also experimented with using $k=5,20,30,40,50$, but the results barely varied
from using $k=10$ dimensions.  Note that, for testing, the learned $W_{i}$ allow
us to project unseen fMRI data into the shared space via $W_{i}^{T}X_{i}^{test}$
since $W_{i}$ has orthogonal columns'' \citep{vodrahalli2018mapping}'s method
section.

%
``Figure 2: The SRM is estimated from response time series from the training set
for multiple subjects. The multi-subject response time series are decomposed
into a set of subject-specific orthogonal topographic transformation matrices
and a reduced-dimension shared response space. The learned subject-specific
topographic bases can be used to project test data into the shared space. This
projection functionally aligns the test data'' \citep{kumar2020brainiak}.

%
``Transformations estimated from one subset of data can be used to project
unseen data into the shared space. Projecting data into shared space increases
both temporal and spatial ISC (by design), and in many cases improves
between-subject model performance to the level of within-subject performance.
Between-subject models with SRM can, in some cases, exceed the performance of
within-subject models because (a) the reduced-dimension shared space can
highlight stimulus- related variance by filtering out noisy or
non-stimulus-related features, and (b) the between-subject model can effectively
leverage a larger volume of data after functional alignment than is available
for any single subject. De-noised individual-subject data can be reconstructed
by projecting data from the reduced-dimension shared space back into any given
subject’s brain space. Furthermore, in cases where each subject's unique
response is of more interest than the shared signal, SRM can be used to factor
out the shared component thereby isolating the idiosyncratic response for each
subject [13]'' \citep{kumar2020brainiak}.


\paragraph{why we chose SRM}

\todo[inline]{nice performance; s. \citep{chen2015reduced} but esp.
\citep{bazeille2021empirica}}

\todo[inline]{``nicely'' implemented in BrainIAK}

\todo[inline]{probably faster, computationally less demanding than
hyperalignment}


\subsubsection{works better with naturalistic stimuli}

``Naturalistic stimuli such as movies and stories are often used to generate
such training data, though any study design in which participants perform the
same sequence of trials --- or for which a common sequence can be spliced
together from the same set of trials --- could be used (for example, a battery
of cognitive tasks)'' \citep{cohen2017computational}
%
``As a rule of thumb, SRM will improve sensitivity for detecting a cognitive
process of interest in the test data if the training stimuli or trials strongly
and variably engage that process in a way that is reliable across participants.
'' \citep{cohen2017computational}.

%
``There are many potential advantages to using data collected during
movie-viewing for estimating category-selective topographies. Movies are more
engaging and result in better compliance \citep{vanderwal2015inscapes}. Movie
viewing can also be used in subject populations, such as children
\citep{richardson2018development} or patients, that may have trouble maintaining
attention during repetitions of a tedious localizer task''
\citep{jiahui2020predicting}.
%
``Movies engage multiple brain systems in parallel. From a single movie dataset
multiple functional topographies can be estimated [Guntupalli et al., 2016],
whereas different localizers are typically required to map different functional
topographies, making a thorough mapping of selective topographies time-consuming
and inefficient'' \citep{jiahui2020predicting}.
%
``Movies also simulate better the statistics of natural viewing and listening
and may provide more ecologically valid maps. Analogously, the introduction of
dynamic videos of faces and control categories to localize face-selective
topographies provides more reliable maps and better estimate the extent of
face-selective regions than do localizers with still image stimuli (Fox et al.,
2009; Pitcher et al., 2011)'' \citep{jiahui2020predicting}.
%
``Similarly, naturalistic stimuli may better sample the full range of responses
to faces and other stimuli that contribute to face-selective topographies''
\citep{jiahui2020predicting}.


\subsection{Here, we...}

From \citet{jiahui2020predicting}: ``Here, we show that precise mapping of
functional topographies in a new subject can be achieved using hyperalignment
and a database of movie and localizer data from other subjects.
%
We present a proof-of-concept analysis of two different data sets with different
movies and different face-selectivity localizers.
%
We used hyperalignment to projects all other subjects' data into a group common
information space and then projecting data from the common space into a new
subject's cortical anatomy'' \citep{jiahui2020predicting}.


\subsubsection{What how we do it}
% main idea
We want to predict location, size, and shape of a functional area in
an individual person based on location, size, and shape of the same functional
area in persons of a reference group.

\paragraph{data}
%
We let participants watch the movie and audio-description of Forrest Gump,
assuming that the naturalistic stimuli trigger, among others, brain responses
that a similar to those triggered by the functional localizer


\paragraph{SRM fitting}

%
From that stimuli, you get the time-series per voxel and per subject, and what
the algorithm essentially does is: it learns responses that are shared across
participants (which is the common model space)
%
algorithm also learns individual transformation matrices.
%
project localizer data of reference group into CMS


\paragraph{partial alignment \& prediction}

%
align left-out subject with CMS using a varying amount of data (1-8 runs per
naturalistic stimulus) data (parts) need to be of the same stimulation used to
create the CMS (cause we align time-series not connectivity profiles) result:
transformation matrix
%
``Partial alignment'': we do not use the same time-series that was used to
create the \ac{cms} but only a part of it.
%
align left-out subject to model space using varying amount of data of
movie or audiobook - transform localizer results into left-out subject
%
Test which amount of data is needed to do an alignment to the common
model space that provides transformation matrices that outperform a prediction
using just an anatomical alignment.

%
We want to know if functional alignment based on a \ac{srm} improve the
prediction compared to a prediction based on merely anatomical alignment?
%
We want to know how the parameter ``data points'' influences the results.
%
How much data do we need to align an individual subject to a/the \ac{cms} and
outperform a prediction based on anatomical alignment?
%
Is a short diagnostic run ``sufficient''?
%
project localizer data from CMS into left-out subject using the inverse matrix


\paragraph{anatomical alignment as comparison}

We will predict the location of the functional area by performing an
anatomical alignment as a benchmark, and explore possibly improved prediction
performance using a functional alignment


\paragraph{functional alignment}

\todo[inline]{model validation: needs to be done but not mentioned here: model
validation: predict localizer results using localizer TRs for alignment; not
cross-subject-cross-experiment-prediction, just cross-subject}


\subsection{Hypotheses}
%
functional alignment outperforms anatomical alignment.
%
The more data for alignment the better.
%
for discussion: asymptotic curve might be different for another brain region
(we look at perception; what about ``higher'' cognition or even executive
functions?)


\subsection{Summary of results}
%
``Results expands previous analysis from ventral temporal cortex to the whole
cortex, showing strong correlations of face-selectivity topographic maps derived
from a subject's own localizer data with maps derived from other subjects'
localizer data projected into that subject's cortical anatomy. Both the two-step
algorithm and the new one-step algorithm, which we introduce here, produce
high-fidelity, individualized topographic maps, but the new one-step algorithm
maps were superior'' \citep{jiahui2020predicting}.


\subsection{What it will be good for}

\todo[inline]{one or two sentences should be enough in the intro, talk about it
in discussion and general discussion}
%
``These results lay a foundation for building a computational tool with a
database that could allow others to map multiple functional topographies in new
subjects using only data collected during movie viewing. Functional localizers
are inefficient because they only estimate one or a few topographies for each
localizer. Movies, by contrast, engage in parallel multiple neural systems for
vision, audition, language, person perception, social cognition, and other
functions. Consequently, movies have the potential to estimate selective
topographies in all of these domains. Such a tool would require a database of
data for movies and a range of functional localizers in a normative group of
subjects. A new subject's functional topographies could be estimated based only
on that subject's movie data and other subjects’ localizer data from the
normative database that could be projected into that subject’s cortical anatomy
using hyperalignment transformation matrices derived from movie data. Such a
resource would be more efficient and replace tedious functional localizers with
an engaging movie and could enable mapping of multiple functional topographies
with data from a single fMRI using a naturalistic stimulus''
\citep{jiahui2020predicting}.


\section{Methods}

\todo[inline]{mih: braucht nicht wie Standalone-Paper geschrieben werden; statt
Kopien der "Participants" etc, einfach sagen, dass es die gleichen Daten sind
bzw. was abweicht; insbesondere keinen Overlap mit den anderen papers
herstellen}

\todo[inline]{mih: Eindruck vermeiden, dass das Kapitel aufgeblasen ist; wir
wollen, ist zu zeigen, wie es weiter geht und ging, nachdem Paper draussen
waren; optimaler Eindruck ist der eines Forschungsprogramms in voller Fahrt}

% we get the data from the naturalistic PPA paper (its subdataset)
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in\_bold3Tp2/sub-??\_task-a?movie\_run-?\_bold*.*

\todo[inline]{following phrasing is almost identical to PPA-Paper}


% input data
We spliced the data together: ``Naturalistic stimuli such as movies and stories
are often used to generate such training data, though any study design in which
participants perform the same sequence of trials—or for which a common sequence
can be spliced together from the same set of trials—could be used (for example,
a battery of cognitive tasks)'' \citep{cohen2017computational}

% reference to PPA-Paper
As in study 2, we
%
used components of the publicly available
\href{http://www.studyforrest.org}{studyforrest.org} dataset that has been
repeatedly used by other research groups in independent studies
(\citep[e.g.,][]{ben2018hippocampal, jiahui2020predicting, hu2017decoding,
lettieri2019emotionotopy, nguyen2016integration}).
% used studies
The same participants were
% VIS
a) participating in a dedicated six-category block-design visual localizer
\citep{sengupta2016extension}.
% AV
b) watching the audio-visual movie \citep{hanke2016simultaneous}, and
% AD
c) listening to the audio-description \citep{hanke2014audiomovie} of the movie
``Forrest Gump'',
% see corresponding papers for details
An exhaustive description of the participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications. Following is a summary of the most important aspects.


\subsection{Participants}

\todo[inline]{following phrasing is almost identical to study2}
% VIS study
In the block-design localizer study \citep{sengupta2016extension}, 15
participants took part in a six-category block-design visual localizer.
% AV study
In the movie study \citep{hanke2016simultaneous}, the same 15 participants
(21–39 years, mean age 29.4, six female), watched the audio-visual movie
``Forrest Gump'' \citep{ForrestGumpMovie} with dubbed German audio track
\citep{ForrestGumpDVD}.
% AD study
In the audio-description study \citep{hanke2014audiomovie}, 20 German native
speakers (all right-handed, age 21–38 years, mean age 26.6 years, 12 male), of
whom 15 participants also took part in the localizer and movie study, listened
to the German audio-description \citep{ForrestGumpGermanAD} of the movie.
% participants' health
All participants reported to have normal hearing, normal or corrected-to-normal
vision, and no known history of neurological disorders.
% compensation, consent and shit
In all studies, participants received monetary compensation and gave written
informed consent for their participation and for public sharing of obtained data
in anonymized form. The studies had prior approval by the Ethics Committee of
Otto-von-Guericke University of Magdeburg, Germany.


\subsection{Stimuli and procedure}

\todo[inline]{following phrasing is almost identical to PPA-Paper}

\subsubsection{functional localizer}

% VIS study picture categories
Stimuli for the block-design localizer study were 24 unique grayscale images of
faces, bodies, objects, houses, outdoor scenes and scrambled images, matched in
luminance and size, that were previously used in other studies
(\citep[e.g.,][]{haxby2011common}).
% procedure: presentation & instructions
Participants performed a one-back image matching task for four block-design
runs, with two \unit[16]{s} blocks per stimulus category in each run.
%
For details on stimulus creation and presentation see
\citet{sengupta2016extension}.


\subsubsection{Naturalistic stimuli}
% AD & AV stimulus name & references
The German DVD release \citep{ForrestGumpDVD} of the movie ``Forrest Gump''
\citep{ForrestGumpMovie} and its temporally aligned audio-description
\citep{ForrestGumpGermanAD} served as naturalistic stimuli, with an approximate
duration of two hours, split into eight consecutive segments of
\unit[$\approx$15]{minutes}.
% AD: additional narrator
The audio-description adds another male narrator to the voice-over narration of
the main character Forrest Gump. This additional narration describes essential
aspects of the visual scenery when there is no off-screen voice, dialog, or
other relevant auditory content.
% task
For all sessions with naturalistic stimuli, participants were instructed to
inhibit physical movements except for eye-movements, and otherwise to simply
``enjoy the presentation''.
%
For details on stimulus creation and presentation see
\citet{hanke2014audiomovie, hanke2016simultaneous}.


\subsection{Stimulation setup}

\todo[inline]{following phrasing is almost identical to PPA-Paper}

% instructions and distance; screen size \unit[23.75 $\times$ 10.25]{cm}
In the block-design localizer, movie study, and audio-description study, visual
instructions were presented on a rear-projection screen inside the scanner bore
at a viewing distance of \unit[63]{cm}.
%
Stimulus images of the localizer paradigm were displayed at a size of
approximately \unit[10]$^{\circ}$ $\times$ \unit[10]$^{\circ}$ of visual angle.
%
Movie frames of the movie paradigm were displayed at a size of approximately
\unit[21.3]$^{\circ}$ $\times$ \unit[9.3]$^{\circ}$.
% AD
During the functional scans of the audio-description study, the projector
presented a medium gray screen with the primary purpose to illuminate a
participant's visual field in order to prevent premature fatigue.
% AD & AV: auditory stimulation
Auditory stimulation was implemented using custom in-ear (audio-description), or
over-the-ear headphones (movie), which reduced the scanner noise by at least
\unit[20–30]{dB}.


\subsection{fMRI data acquisition}
%
\todo[inline]{following phrasing is almost identical to PPA-Paper}
% AV & VIS
In the block-design localizer and the movie study, a \unit[3]{Tesla} Philips
Achieva dStream MRI scanner with a 32 channel head coil acquired gradient-echo
fMRI data at \unit[2]{s} repetition time with
% slices
35 axial slices (thickness \unit[3.0]{mm}, \unit[10]{\%} inter-slice gap) with
\unit[80 $\times$ 80]{voxels} (\unit[3.0 $\times$ 3.0]{mm} of in-plane
resolution, \unit[240]{mm} field-of-view) and an anterior-to-posterior phase
encoding direction, recorded in ascending order.
% visual localizer: 4 x 156 TR
A total of 624 volumes were recorded for each participant across the four runs
of the visual localizer experiment.
%
A total 3599 volumes were recorded for each participant across the eight runs of
the movie study.

% AD
In the audio-description study gradient-echo fMRI data were acquired using a
\unit[7]{Tesla} Siemens MAGNETOM magnetic resonance scanner equipped with a 32
channel brain receive coil at \unit[2]{s} repetition time (TR) with 36 axial
slices (thickness \unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane
resolution, \unit[224]{mm} field-of-view, anterior-to-posterior phase encoding
direction) and a \unit[10]{\%} inter-slice gap, recorded in ascending order.
% slice orientation
Slices were oriented to include the ventral portions of frontal and occipital
cortex while minimizing intersection with the eyeballs.
% FOV
The field of view was centered on the approximate location of Heschl's gyrus.
% motion correction
EPI images were online-corrected for motion and geometric distortions.
%
An number of 3599 volumes, identical to the movies study, was recorded for each
participant in the audio-description study.

%
T1 from \citep{ben2018hippocampal}: ``High-resolution T1- weighted structural
images (Hanke et al., 2014) were acquired using a 3D turbo field echo sequence
[acquisition voxel size of 0.7 mm with a 384 x 384 in-plane reconstruction
matrix (0.67 mm isotropic resolution), TR = 2500 ms, TE = 5.7 ms, TI = 900 ms,
flip angle = 8°, FOV = 191.8 x 256 x 256 mm, bandwidth 144.4 Hz/px, sense
reduction AP 1.2, RL 2.0]''.


\subsection{Preprocessing}

\todo[inline]{following phrasing is almost identical to PPA-Paper}

% data sources
The current analyses were carried out on the same preprocessed fMRI data
\citep{hanke2016aligned} that were used for the technical validation analysis
presented in \citet{hanke2016simultaneous}, the localization of higher-visual
areas \citep{sengupta2016extension}, and investigation of responses of the PPA
correlating with naturalistic spatial information \citep{sengupta2016extension}.
% exclusion of VP 10
Of those 15 participants in the studyforrest dataset that took part in all three
experiments, data of one participant were dropped due to invalid distortion
correction during scanning of the audio-description stimulus.
% preprocessing of pre-aligned data
Data were corrected for motion, aligned with and re-sliced onto a
participant-specific BOLD template image \citep{sengupta2016extension} (uniform
spatial resolution of \unit[2.5$\times$2.5$\times$2.5]{mm} for both
audio-description and movie data).
% preprocessing intro
On these data, the further preprocessing steps were performed by FEAT v6.00
(FMRI Expert Analysis Tool \citep{woolrich2001autocorr}) as shipped with FSL
v5.0.9 (\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software Library}
\citep{smith2004fsl}) to create the input data for the subsequent shared
response modelling to create the \ac{cms}
%
The preprocessing steps closely resemble the processing that was previously
performed in \citet{sengupta2016extension} and in \citet{haeusler2022processing}
responsively.

%
\todo[inline]{make sure that all mentioned steps have actually been applied to
\\ 'filtered\_func\_data.nii.gz' serving as input for custom SRM-scripts}

\subsubsection{functional localizer}
% install VIS dataset as subdataset datalad install -d . -s
% https://github.com/psychoinformatics-de/studyforrest-data-visualrois.git
% inputs/studyforrest-data-visualroi datalad get
% inputs/studyforrest-data-visualrois/src/aligned/sub-*/in\_bold3Tp2/sub-*\_task-objectcategories\_run-*\_bold.nii.gz
% datalad get inputs/studyforrest-data-visualrois/sub-*/onsets/run-*/*.txt

\todo[inline]{cf. with VIS paper; see design-files}
% ...inputs/studyforrest-data-visualrois/code/despike.submit
Every run of the functional localizer paradigm was despiked using AFNI's
\citep{cox1996afni, cox1997software} '3dDespike' command, and
slice-time-corrected.
% temporal filtering
High-pass temporal filtering was applied to every run using a Gaussian-weighted
least-squares straight line with a cutoff period of \unit[100]{s} to remove
low-frequency confounds.
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
Data were spatially smoothed applying a Gaussian kernel with full width at half
maximum (FWHM) of \unit[4.0]{mm}
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% rerun the analysis: ./code/generate\_1st\_level\_design.sh condor\_submit
% .code/compute\_1stlvl\_glm.submit

\subsubsection{naturalistic stimuli}
%% install naturalistic ppa analysis as subdataset comprises  "aligned-data",
% templates etc. as subdatasets datalad install -d . -s
% https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis

% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in_bold3Tp2/sub-??_task-a?movie_run-?_bold*.*

% get motion correction parameters for AO data
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/phase1
% datalad get inputs/studyforrest-ppa-analysis/inputs/phase1/sub???/BOLD/task001\_run00?/bold\_dico\_moco.txt

\todo[inline]{following phrasing is almost identical to PPA-Paper}

% temporal filtering
To every segment of the movie and audio-description, high-pass temporal
filtering was applied using a Gaussian-weighted least-squares straight line with
a cutoff period of \unit[150]{s} (sigma=\unit[75.0]{s}).
% brain extraction
The brain was extracted from surrounding tissues using BET \citep{smith2002bet},
% spatial smoothing
and data were spatially smoothed (Gaussian kernel, \unit[4.0]{mm}, FWHM).
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.
% pre-whithening
Correction for local autocorrelation in the time series (prewhitening) was
applied using FILM (FMRIB's Improved Linear Model \citep{woolrich2001autocorr})
to improve estimation efficiency.

%% templates and transforms
% datalad get
% inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/sub-*/bold3Tp2/;
% datalad get
% inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-templatetransforms/templates/*

% rerun second-level analysis to obtain the (un)thresholded $z$-maps?
% was it even necessary?


%
Results from \citet{sengupta2016extension} and in
\citet{haeusler2022processing} were simply downloaded.
%
``We estimated the face-selectivity map for a participant from his or her own
localizer data by calculating the GLM univariate contrast map of faces vs. all
the other categories (e.g., body, place) for each run and averaging t-values
across the four maps'' \citep{jiahui2020predicting}.


\subsection{Shared response modelling for creation of common model spaces}

\todo[inline]{how to reliably (!) hyphenate links??}

Further analyses steps were performed via Python scripts that relied on
%
NiBabel v3.2.1 (https://nipy.org),
%
NumPy v1.20.2 (https://numpy.org),
%
Pandas v1.2.3 (https://pandas.pydata.org),
%
Scipy v1.6.2 (https://scipy.org),
%
scikit-learn v1.0 (https://scikit-learn.org),
%
BrainIAK v0.11 (https://brainiak.org),
%
Matplotlib v3.4.0 (https://matplotlib.org),
%
seaborn v0.11.2 (https://seaborn.pydata.org),
%
and calling command line functions of FSL.

\paragraph{fixing FSL output}

\todo[inline]{grand\_mean\_for\_4d.py (formerly: data\_normalize\_4d.py) not
necessary anymore 'cause FSL seems to have applied grand mean scaling to
'filtered\_func\_data.nii.gz)'}
\todo[inline]{the script now just adds the mean again and scales by factor
10000; s. below}

% grand mean scaling for 4d data:
% voxel values in every image are divided by the average global mean
% intensity of the whole session. This effectively removes any mean global
% differences in intensity between sessions.

% FSL User Guide:
% filtered_func_data will normally have been temporally high-pass filtered,
% it is not zero mean; the mean value for each voxel's time course has been
% added back in for various practical reasons.
% When FILM begins the linear modelling, it starts by removing this mean.

% input: 'sub-*/run-?.feat/filtered_func_data.nii.gz' of VIS, AO, AV
% -> should already grand mean scaled

%
Given that FSL adds back the mean value for each voxel's time course at the end
of the preprocessing, mean values were subtracted again, and each voxels's time
course was multiplied by 10000 [using a Python script].
% saved to 'sub-??\_task-*\_run-?\_bold\_filtered.nii.gz'


\paragraph{Masking}

\todo[inline]{problem 1: grpPPA contains N=14 subject, not N-1 subjects}

\todo[inline]{problem 2: still, there are voxels outside of the PPA-mask;
probably, because of the warping procedures}

% masks-from-mni-to-bold3Tp2.py:
% - merges unilateral ROIs overlaps (already in MNI) to bilateral ROI
% - output: 'masks/in_mni/PPA_overlap_prob.nii.gz'
% - warps union of ROIs from MNI into each subjects space
% output: 'sub-*/masks/in_bold3Tp2/grp_PPA_bin.nii.gz' + audio_fov.nii.gz dilate
% the ROI masks by 1 voxel; output: 'grp_PPA_bin_dil.nii.gz'

% masks-from-mni-to-bold3Tp2.py:
% warp MNI masks into individual bold3Tp2 spaces

% masks-from-t1w-to-bold3Tp2.py:
% transforms 'inputs/tnt/sub-*/t1w/brain_seg*.nii.gz'
% into individual's bold3Tp2
% output: 'sub-*/masks/in_bold3Tp2/brain_seg*.nii.gz'

% mask-builder-voxel-counter.py:
% builds different individual masks by dilating, merging other masks
% creates a FoV of AO stimulus for every subject from 4d time-series of AO run
% output: sub-*/masks/in_bold3Tp2/audio_fov.nii.gz'
% counts the voxels
% long story short: we cannot used all gyri that contain PPA to some degree
% even if the mask by FoV of AO stimulus and individual gray matter mask

% data_mask_concat_runs.py: masks are not dilated and not masked with
% subject-specific gray matter mask

\todo[inline]{explain, why we need a reduction of features; and how we do it
theory-driven based on anatomical data}

For each paradigm, subject, the corresponding 4d time-series of each run /
segment were masked with the union of individual PPA masks (s.
\citep{haeusler2022processing} that was warped from MNI space into each
subjects' space. Each subjects' data were further masked with the
subject-specific \ac{fov} of the audio-description.
%
The number of remaining voxels for each subject can be seen in
\ref{tab:ppamaskvoxels}.

\begin{table*}[btp]
    \caption{Number of voxels within the union of individual \acp{ppa} that was
    warped from MNI space into each individual's subjects-space.}
\label{tab:ppamaskvoxels}
\begin{tabular}{ll}
\toprule
\textbf{Subject} & \textbf{no. of voxels} \\
\midrule
sub-01 & 1665 \tabularnewline
sub-02 & 1732 \tabularnewline
sub-03 & 1400 \tabularnewline
sub-04 & 1575 \tabularnewline
sub-05 & 1664 \tabularnewline
sub-06 & 1951 \tabularnewline
sub-14 & 1376 \tabularnewline
sub-09 & 1383 \tabularnewline
sub-15 & 1683 \tabularnewline
sub-16 & 1887 \tabularnewline
sub-17 & 1441 \tabularnewline
sub-18 & 1729 \tabularnewline
sub-19 & 1369 \tabularnewline
sub-20 & 1437 \tabularnewline
\bottomrule
\end{tabular}
\end{table*}

% scaling
Then, data of every run were scaled independently using the Scipy function
``preprocessing.StandardScaler()''.
% output: 'sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy; and:
% 'sub-*_task_visloc_run-1-4_bold-filtered.npy'
Given that the last 75 TRs of the audio-description were missing in subject 04
due to [WHAT?], we also dropped the last 75 TR from the other participants'
data.
% it's only credits anyway; AO + AV has 7123 TRs; not 7198 volumes anymore;
% visual localizer has 156 TRs per run
As a result, the data to create the \ac{cms} in the following step comprised
3524 TRs of audio-description, 3599 TRs of the movie, 624 TRs of the visual
localizer experiment.



\paragraph{Fitting of shared response model}

\todo[inline]{check used functions in BrainIAK docs}

\todo[inline]{cite the publications referenced in the documentation of the
BrainIAK modules you use, e.g., SRM.}

We used BrainIAK v.11 (Brain Imaging Analysis Kit, \citet{kumar2020brainiak,
kumar2020brainiaktutorial}, http://brainiak.org).

%
``The SRM builds a common model information space where the concatenated fMRI
responses to the movie, audio-description, and functional localizer are aligned
across subjects'' \citep{jiahui2020predicting}.

``At the same time, individual transformation matrices are calculated to project
each participant's [that was used to build the common model space] cortical
space into the common information space'' \citep{jiahui2020predicting}.
%
``Transformation matrices are calculated for the ROI(s) in each brain [using an
iterative procedure and Procrustes alignmen]'' \citep{jiahui2020predicting}.

Transformation matrices allowed us to project each \ac{roi}'s voxel responses into
the common k-dimensional space.


% data_srm_fitting.py
% output: 'test/sub-??/srm-ao-av-vis\_feat10-iter30.npz

\todo[inline]{2 kinds of flavours: 'ao \& av \& vis' vs. only 'ao \& av'}

leave-one-subject-out folding scheme [?], creating von CMS for every (left-out)
subject from other subjects' data

Fore each subject, we created a \ac{cms} based on other subject's data.
%
The runs of the audio-description, movie, and visual localizer were concatenated
to one continuous and z-scored using scipy.stats.zscore().

% features
Model fitting was performed using 10 features [WHY?] and 30 iterations; calling
the fit method if brainIAK's srm object (brainiak.funcalign.srm.SRM)

% negative control; output: 'test/sub-01/srm-ao-av-shuffled\_feat10-iter30.npz'
As negative control, we also fitted a shared response model to after shuffling
the order of runs of audio-description and movie (but not visual localizer) of
every subject before fitting the model.

\paragraph{plot\_srm.py}

\todo[inline]{imo not very informative}
%
plots SRM (features*time points) created for a left-out subject; plots
time-course of top 3 responses (currently, TRs 0-800 = quarter of AO); plots
distance matrix of time points in shared space


\paragraph{plot\_corr-of-glm-and-srm.py}
%
\begin{itemize}
    \item plot correlation matrix between
    \item regressors of the naturalistic stimuli (and some combinations, e.g.
        geo\&groom, geo\&groom\&furn
    \item and shared responses
    \item at the moment, it correlates only with AO TRs (and wrongly uses
        0:3599 TRs)
    \item ToDo: at the moment in\_dir with hardcoded model 'srm-ao-av'
    ->  means: no vis, not shuffled; correlate AO regressors with correct AO
        TRs; correlate AO regressors with AV TRs; correlate AV regressors with
        AV TRs; correlate AV regressors with AO TRs; correlate AO and AV
        regressors with all TRs (+VIS?)
\end{itemize}


\subsection{Alignment of left-out subjects to \ac{cms}}

%
``We used the shared response model as a functional alignment tool to predict
functional topographies in new subjects. To estimate one participant's
face-selectivity contrast map, all the other participants' localizer data were
projected into participant A's cortical anatomy using transformation matrices
calculated based only on the movie-viewing data for all pairings of participant
A with each of the other 14 or 20 participants'' \citep{jiahui2020predicting}.


\paragraph{Partial alignment}

%
``New data can be projected into any individual brain space by first projecting
data from other brains into the common model space, and then projecting those
data from the common model space into that participant's cortical anatomy using
the transpose of his or her transformation matrix'' \citep{jiahui2020predicting}

\todo[inline]{matrices have been computed CMS created from AO+AV+VIS, but also
just from AO+AV}

\todo[inline]{perform alignment wit TRs of localizer for
cross-subject-prediction; we do cross-subject-cross-experiment-prediction}

In order to derive transformation matrices based on response patterns.

\paragraph{get\_wmatrix\_for\_left-out.py}
% AO: 0-451, 0-892, 0-1330, 0-1818, 0-2280, 0-2719, 0-3261, 0-3524
% AV: 3524-3975, 3524-4416, 3524-4854, 3524-5342, 3524-5804, 3524-6243,
%     3524-6785, 3524-7123
% AO+AV: 0-7123

% input 'sub-*/sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy'
We used an increasing number of runs of the filtered audio-description and movie
data to align each left-out subject to the corresponding \ac{cms}, and obtain
the subject's transformation matrix.
%
The SRM object was sliced according to the used runs and the weight matrix was
obtained calling the SRM method srm\_sliced.transform\_subject(runsdata)
% output: f'wmatrix_{model}_feat{n_feat}_{start}-{end}.npy'

imo, SRM uses procrustes transformation, too, hence: ``the searchlight response
hyperalignment algorithm, which utilizes Procrustes transformations to calculate
a transformation matrix for each participant that maps their AA data into a
shared high-dimensional in- formation space shared across participants
[Guntupalli et al., 2016]'' \citep{busch2021hybrid}.


\subsubsection{Prediction using \ac{cms}}

%
``We estimated a participant's map from other participants' data by first
projecting all other participants' localizer data into that participant's
cortical anatomy and calculating the GLM univariate contrast map of faces versus
all other categories for each run in each other participant then averaging
t-values across the maps (56 maps for StudyForrest, 14 subjects x 4 runs for
each; 80 maps for Grand Budapest, 20 subjects x 4 runs each)''
\citep{jiahui2020predicting}.\todo{an alternative approach to ours}
%
``Thus, the map estimated from other participants' localizer data was a map of
average t-values across 56 or 80 maps (four runs per participant) for the
contrast faces vs all'' \citep{jiahui2020predicting}.

``To estimate one participant's face-selectivity map using this algorithm, all
the other participants' localizer data were projected into the common model
space, and then mapped to participant A's space using the transpose of his or
her hyperalignment transformation matrix'' \citep{jiahui2020predicting}.


\paragraph{predict\_ppa.py}

\todo[inline]{mention that anatomical transformation matrices were precomputed
and part of the studyforrest dataset}

\todo[inline]{probably tell a little more how the transformation matrices were
created; reviewer's question: your anatomical alignment might have been shitty}
\todo[inline]{check TNT github/gin repo; what is the corresponding paper?}

% prediction from anatomy
We first created the template/prediction from the anatomy of other subjects.
%
We warped/mapped [what kind of transformation?] the results of the visual
localizer (and results of the auditory naturalistic stimulus) from each
individual's subject-space (bold3Tp2) into MNI space (grpbold3Tp2).
% output 1: 'test/masks/in_mni/sub-*_VIS-PPA.nii.gz' output 2:
% 'test/masks/in_mni/sub-*_AO-PPA.nii.gz'
The results of all subjects were warped from MNI space in to the left-out
subject's space
% output 1: 'test/sub-*/masks/in_bold3Tp2/sub-*_VIS-PPA.nii.gz' output 2:
% 'test/sub-*/masks/in_bold3Tp2/sub-*_AO-PPA.nii.gz'
The transformed $Z$-maps were masked with mask of unions of PPAs and the
individual's \ac{fov}.
%
The mean of these $Z$-maps served as the prediction for the left-out subject.
% output1: 'test/sub-*/predicted-VIS-PPA_from_anatomy.nii.gz' output2:
% 'test/sub-*/predicted-AO-PPA_from_anatomy.nii.gz'

% prediction from CMS
We then predicted the $Z$-maps of the PPA from the visual localizer experiment
and the auditory naturalistic stimulus.
%
The transformation matrices that were created using an increasing number of runs
of each naturalistic stimulus were used to transform a $Z$-map template created
in the \ac{cms} into the left-out subject's space
% aligned zmaps to shared space: (k features x t time-points); 1 time-point
% because it's a zmap no time-series) using
%
First, individual results from the visual localizer and auditory naturalistic
stimulus were masked with the group PPA mask in each individuals subject space
and then transformed from each individual's space into the \ac{cms} [by calling
'zmaps\_in\_cms = srm.transform(masked\_zmaps)'].
% zmap from CMS into subject space: compute dot product of left-out subject
% weight-matrix with zmap in cms
Using the inverse of the left-out subject's transformation matrix, the zmaps
from the other subjects that were projected into \ac{cms} were then projected
into the anatomy of the left-out subject.
%
Here again, the mean of these z-maps served as the prediction for the left-out
subject.

\todo[inline]{i.e.: in both cases the mean was computed in anatomical space;
this is more 'similar' than computing the mean in anatomy for prediction from
anatomy, and computing the mean in CMS for prediction from CMS; but: creating
the template in CMS does not lead to different results anyway}

% correlation between empirical z-map & predicted z-maps
Finally, we calculated the Pearson correlation coefficients between the
empirical z-maps of each subject (visual localizer and auditory PPA) and the
values that were predicted from other subjects' anatomy.
%
Further, we calculated the Pearson correlation coefficients between the
empirical z-maps of each subject (visual localizer and auditory PPA) and the
values that were predicted from using an increasing number of runs of the
naturalistic stimuli to obtain the transformation matrices.


\subsubsection{Prediction using anatomical alignment}
%
``We also estimated each participant's face-selectivity map based on other
participants' anatomically aligned localizer data by simply averaging across the
maps based on anatomically-normalized data for each N-1 set of 14 (StudyForrest)
or 20 (Grand Budapest) participants'' \citep{jiahui2020predicting}.


\subsubsection{Quantifying the performance}

``We then calculated correlations between the map based on a participant’s own
data and the maps estimated from other participants' data and compared these
correlations to the reliability of the participant's map, indexed with
Cronbach's Alpha'' \citep{jiahui2020predicting}.
%
We tested the quality of the maps estimated from other participants' data by
calculating the correlation of each with the map estimated from a participant's
own data. We also gauged the reliability of the estimates based on
participants' own data by calculating Cronbach's alpha based on variability
across runs'' \citep{jiahui2020predicting}.


\subsubsection{alternative template creation}
%
lastly, we currently apply another method to get the „z-map template“ on which
the prediction is based ('test/data\_denoise-vis.py';
'test/data\_srm-vis-to-ind.py')
%
``An added benefit is that SRM helps address the data starvation problem above:
because the SRM space is by definition shared across individuals, data from
multiple participants can be combined prior to MVPA or other analyses''
\citep{cohen2017computational}.


\section{Results}

\todo[inline]{use Jiahu as fucking template}

\todo[inline]{Neurovault, \citep{gorgolewski2015neurovault}}

\todo[inline]{SRM based on shuffled runs}

\todo[inline]{other number of features/responses used}

``We estimated each participant's face-selectivity map based on that
participant’s localizer data [anatomical alignment], based on other
participants' localizer data projected into that participant's cortical anatomy
using hyperalignment and anatomical surface alignment (see Fig. 1) \citep{jiahui2020predicting}''.

%
For every subject, we see the correlation of z-maps that tell us the, quote
``real'' unquote, PPA and predicted PPA
%
In green, we see the correlations between empirical values from the localizer \&
the predicted values using anatomical alignment
%
In orange, we see the correlations between empirical values \& the predicted
values using parts of the movie
%
In blue, we see the correlations between empirical values \& the predicted
values using parts of the audio-description
%
I marked subject 4, because I want to show you how results look like in a
horizontal slice of the brain of subject 4
%
We have these nice blurry EPI-images and all z-maps are threshold at a value of
bigger than 2.3.
%
always in red, we can see the z-map from the localizer experiment across the
whole brain,
%
the region of interest that we used is white and the predicted values, are blue
%
The prediction using anatomical alignment and the prediction using 15 minutes of
movie data show a correlation of about .7
%
the prediction using 15 minutes of the audio-description correlates about 0 with
the empirical z-map
%
The last one is the extreme case, but it can give you an idea of how the z-maps
look in a slice of the brain


\subsection{predict\_ppa.py: outputs also the Pearson correlations}


\section{Results}

\todo[inline]{plot of the union of PPA masks?}


\subsection{plot\_stripplot.py}

the lovely stripplots of correlations

\subsection{statistics\_t-test-correlations.py}

Compute if differences between kinds of prediction is significant


\subsubsection{test/statistics\_cronbachs.py}

\subsubsection{plot\_bland-altman.py}

I hate that script

% corrstats.py; not necessary anymore; calculates the statistical significant
% differences between two dependent or independent correlation coefficients


\section{Discussion}


\subsection{Short Summary}

What we did. What the results are

\subsection{Discussion of current results}

Results indicate that we are able to use multiple subjects to learn a
10-dimensional shared space for the fMRI data that increases performance on our
experiments.
%
15 min of movie watching used for functional alignment outperform prediction
using anatomical alignment

%
30 minutes of movie watching outperform 15 minutes of movie watching

%
more than 30 minutes do not lead to a significantly improved prediction
performance.
%

\subsection{self-critique; short comings}


\subsubsection{localizer is ``ground truth''}

\citet{lilienfeld2015fifty} on ``gold standard'': ``In the domains of
psychological and psychiatric assessment, there are precious few, if any,
genuine ``gold standards''. Essentially all measures, even those with high
levels of validity for their intended purposes, are necessarily fallible
indicators of their respective constructs [Cronbach and Meehl, 1955; Faraone and
Tsuang, 1994]. As a consequence, the widespread practice referring to even
well-validated measures of personality or psychopathology, such as Hare’s
(1991/2003) Psychopathy Checklist-Revised, as ``gold standards'' for their
respective constructs [Ermer et al., 2012] is misleading [see Skeem and Cooke,
2010]. If authors intend to refer to measures as ``extensively validated'', they
should simply do so'' \citep{lilienfeld2015fifty}

\citet{scheinost2019ten} in context of phenotypic measures: ``Predictive models
based on neuroimaging data will only ever account for a fraction of the
variance. Neuroimaging studies are limited by how much information the signal
can capture about the measure of interest. At the same time, these studies are
also limited by the chosen phenotypic measure used.  While the success of a
model is evaluated by how well it predicts a phenotypic measure (and these
phenotypic measures have to be treated as gold standards), it is well known that
such measures are not always the ground truth but themselves suffer from
confounds and noise.  When studying brain-behavior associations, one must keep
in mind how extraordinary it is that neuroimaging data can be distilled to
approximate phenotypic measures that reflect a simplification of multiple
complex features. Thus, even modest results are reasonable and remarkable. For
a discussion on the reliability of phenotypic measures in the context of
predictive modeling, we point the interested reader to: [Dubois et al., 2018a,
2018b; Gignac and Bates, 2017]'' \citep{scheinost2019ten}.

``the identification of the PPA is complicated by (at least) four
methodological considerations. First, the PPA definition may depend on the type
of experiment, task, and stimuli used. Second, the boundaries of the PPA may
depend on the statistical threshold used. Third, the spatial extent and
localization of the PPA may vary if defined within the native brain space of an
individual or based on a group analysis. Fourth, the size of the PPA may depend
on data acquisition choices (e.g. large vs. small voxels) and data analysis
choices (e.g. liberal smoothing vs. no spatial smoothing). The present study
aims to identify and to predict the most probable location of place-selective
voxels within medial VTC of an individual brain that is impervious to these
methodological decisions'' \citep{weiner2018defining}.


\subsubsection{SRM and (dropped) individual idiosyncrasies}

``The flip side of focusing on shared responses is to focus on responses that
are idiosyncratic to individuals.
%
Although these responses are excluded in SRM, they are not necessarily noise and
may in fact be highly reliable within participants.
%
SRM can be used to isolate participant-unique responses by examining the
residuals after removing shared group responses, or it can be applied
hierarchically to the residuals to identify subgroups \citep{chen2017shared}
\citep{cohen2017computational} [this other ``Chen'' is fucking up the aethestics
of references].
%
More generally, there is a growing trend toward investigating individual
differences as another source of meaningful variance in fMRI [73].
%
Recognizing that signal exists beyond the average or shared response of a group,
such studies exploit idiosyncratic but stable responses to account for
previously unexplained variance in brain function, behavioral performance and
clinical measures [70,74]'' \citep{cohen2017computational}.


\subsubsection{volume-based registration vs. surface-based normalization}
%
Volume vs. surface: e.g. \citep{desai2005volumetric}

Sengupta did analyses in volume space;
%
I could have run the analyses/contrasts on the surface
%
but: opportunity costs?
%
in a broader broader context as an issue of data sharing (if at all, a point for
general discussion); how well can you trust data that you did not collect
yourself (a.k.a. ``did you consider x?'', ``no, does not matter in our case'',
``but will matter for the person who will use/have used your data'')

Compare to hyperalignment or Multimodal Surface Matching (Robinson et al. 2014)


\subsection{Future questions}

\todo[inline]{mih: Das Ende von Kapitel drei braucht aus meiner Sicht keinen
eigenen "Outlook", der kommt eh direkt danach für die ganze Arbeit und ist da
viel Interessanter}

\subsubsection{predict other t-contrasts of higher-visual area localizer}

\subsubsection{predict other localizers}
%
e.g. retinotopy, language areas


\subsubsection{create CMS from other study}
%
we currently have a cross-subject and cross-experiment prediction,
but we do not have a (real) cross-scanner prediction
%
create a CMS from another experiment’s data,
using another scanner and hopefully more subjects
%
In case of an alignment of time-series,
that experiment needs, at least, a part of Forrest Gump as an intersection


\subsubsection{ROI vs. whole-brain / searchlight}
%
Our union of PPA is XX voxels big.
%
Influence of voxel count on performance; take searchlight (but how big should it
be?

% Guntupalli's searchlight paper
Later, \citet{guntupalli2016model} showed that this approach can be extended to
predict functional organization across large proportions of the cortical
surface, for example to predict the represented visual field coordinate in
visual cortex based on retinotopic mapping scans of other individuals

``Applying SRM to a large swath of the brain means that all voxels within the
region contribute to the final derived metric. This can conflict with the goal
of associating spatially local activity with specific cognitive functions. To
address such issues, SRM can be applied in small overlapping searchlights to
obtain localized metrics of shared information [71,99]
\citep{cohen2017computational}''.


\subsubsection{connectivity-based}

\todo[inline]{wtf did \citep{nastase2019leveraging} do?}

\todo[inline]{kind of a killer cause you do not need intersection of
time-series; but s. Guntupalli's paper: time-series hyperalignment outperforms
connectivity-based hyperalignment (?)}

% from project proposal
``In his doctoral thesis, recently submitted to the Faculty of Natural Sciences
in Magdeburg, Falko Kaule showed that congruent time-locked BOLD responses
across subjects (i.e. all subjects watching the exact same full-length movie) as
used by Haxby and colleagues are not required to derive a valid alignment of
individuals with a common representational space \citep{kaule2017examination}.
%
Comparable prediction performance can be achieved by using \textbf{functional
connectivity patterns} (correlation of a voxel's time series with reference
regions in the same brain)'' (s. dissertation project proposal).


% Nastase's ugly mofo paper
``Finally, estimating the SRM from functional connectivity
data rather than response time series circumvents the need for a single shared
stimulus across subjects; connectivity SRM allows us to derive a single shared
response space across different stimuli with a shared connectivity profile
\citep{nastase2019leveraging}'' \citep{kumar2020brainiak}.




\subsection{Vision}

``Calibration scan'' to align to \ac{cms} and atlas of reference group.
%
Once a valid alignment is established, known functional properties of a
(normative) reference, derived from extensive scans and analysis of other
subjects, can then be projected into the respective individual voxel space (s.
Fig. 1 in \citep{nishimoto2016lining}).
%
Imagine you scan a new, unknown subject for just 15 minutes more, and you
additionally get results from a whole variety of other paradigms mapped onto
that brain
%
Results from localizers of low-level perceptual processes, but also higher-level
cognitive processes like language, memory, emotions and so on
%
Kinda ``adventurous'':If you have different common model spaces for different
subgroups, you can investigate which alignment onto which ``subgroup common
model space'' results in less error, that lets you classify to which subgroup
your new subject might belong.


\section{Conclusion}
