\todo[inline]{I use "prediction" and "estimation" pretty much interchangeably}

\section{Introduction}

\todo[inline]{delete unnecessary sub(sub)sections / captions to turn them into
paragraphs}

\todo[inline]{points of the discussion are now primed in the intro; however, I
do not know where to put "what does the localizer actually do?"; could come up
for the first time in SRM the discussion; above all, it's a topic for the
general discussion (cf. reliability of [audio] naturalistic localizer;
reliability vs.  generalizability}


% higher visual areas higher visual areas
In the domain of higher-visual perception, functionally defined,
category-selective brain regions like the \ac{ppa} \citep{epstein1998ppa}, the
\ac{ffa} \citep{kanwisher1997ffa}, or \ac{eba} \citep{downing2001bodyarea}
exhibit significantly increased \ac{bold} activity correlated with a
``preferred'' \citep{debeck2008interpreting} stimulus class.
%
The topographies (i.e. the location, size and shape) of these category-selective
areas are similarly distributed across individuals but the exact topographies
vary interindividually \citep{rosenke2021probabilistic, zhen2017quantifying,
zhen2015quantifying, frost2012measuring}.


\subsection{Localizers}

% definition of localizer
In order to identify the topography of functional areas in individual persons,
block-design \textit{functional localizer} paradigms are traditionally used that
contrast modeled hemodynamic responses correlating with the corresponding
stimulus classes (i.e. landscapes, faces, or bodies).
% problem: one localizer for one domain
Functional localizers are designed to maximize detection power and thus
dedicated to map just one domain of brain functions like, for example,
retinotopic visual areas \citep{wang2015probabilistic}, category-selective
regions \citep{stigliani2015temporal}, theory of mind
\citep{spunt2014validating}, or semantic processes \citep{fedorenko2010new,
fernandez2001language}.
% which gets messy
However, if one wants to map a variety of domains, the approach ``one paradigm
for one domain of functions'' gets time-consuming and impractical.
% localizer batteries: intro
Researchers have tried to tackle that issue by creating time-efficient
multi-functional \textit{localizer batteries} \citep[e.g.,][]{barch2013function,
drobyshevsky2006rapid, pinel2007fast}.
% task based = shit
Nevertheless, the diagnostic quality of localizer paradigms relies heavily on a
participant's comprehension of the task instructions and general compliance, a
criterion that can be difficult to meet in clinical or pediatric populations
\citep{eickhoff2020towards, vanderwal2019movies}.


\subsection{Estimation from reference group}

% ppa via audio-description
% Results also suggest that a naturally engaging, purely auditory paradigm like
% an audio-description could, in principle, substitute a visual localizer as a
% diagnostic procedure to assess brain functions in visually impaired %
% individuals \citep{haeusler2022processing}.

% ppa via movie
In \citet{haeusler2022processing}, we have shown that a functionally defined
region, such as the \ac{ppa}, can be localized using a model-driven \ac{glm}
analysis that is based on the annotated temporal structure of a two-hour long
naturalistic stimulus.
% full feature film is too long
However, a two-hour long paradigm is unsuitable for a clinical application due
to practical and monetary reasons.
% hence, predict from reference
An approach to reduce time and costs is to identify a functional area in an
individual person's brain anatomy based on data collected from an independent
sample of different persons (i.e. data from a \textit{reference group}).



\subsubsection{Anatomical alignment}

% intro: estimation via common anatomical space
Previous studies estimated a functional area's most probable location from a
reference group by performing a volume-based
\citep[e.g.,][]{zhen2017quantifying, zhen2015quantifying} or surface-based
\citep[e.g.,][]{frost2012measuring, weiner2018defining,
rosenke2021probabilistic, wang2015probabilistic} \textit{anatomical alignment}.
%
First, in order to address the issue of anatomical variability across persons,
functional data of persons in the reference group are anatomically aligned to
(i.e.  projected into) a \textit{common anatomical space} (e.g., Montreal
Neurological Institute brain atlas; \citep[MNI152,][]{fonov2011unbiased}).
% project into test subject to estimate
Then, data are projected from the common anatomical space into the individual
person's brain anatomy serving as an estimation of a functional region's
location.

% volume-based alignment in one sentence
Volume-based anatomical alignment \citep[s.][for a review]{klein2009evaluation}
aligns voxels to a three-dimensional common anatomical space \citep[e.g., MNI152
atlas;][]{fonov2011unbiased}.
% surface-based alignment in one sentence
Surface-based anatomical alignment \citep{fischl1999cortical, yeo2009spherical}
aligns vertices to a two-dimensional common anatomical space \citep[e.g.,
FreeSurfer's fsaverage template;][]{fischl1999high}.
% difference in one sentence
Whereas volume-based alignment does not account for individual sulcal and gyral
folding patterns, surface-based alignment respects interindividual variability
of the cortical surface.
% surface-based estimation works better
Consequently, previous studies that compared  [linear / affine] volume-based and
[nonlinear] surface-based alignment to estimate the location of functional
regions have shown that surface-based alignment lead to reduced inter-subject
variability, and thus increased estimation performance
\citep{rosenke2021probabilistic, frost2012measuring, wang2015probabilistic,
weiner2018defining}.
% remaining variability after surface-based alignment
However, even after surface-based alignment the anatomical location of
functional regions varies anatomically across persons
\citep[e.g.,][]{coalson2018impact, benson2014correction, natu2021sulcal,
wang2015probabilistic, frost2012measuring, langers2014assessment, weiner2014mid,
rosenke2021probabilistic}.
% frost as an example
For example, \citet{frost2012measuring} localized 13 functional areas of the
high-level visual cortex and ``found a large variability in the degree to which
functional areas respect macro-anatomical boundaries''
\citep{frost2012measuring}.
% functional--anatomical correspondence
The remaining variability indicates that functional areas a not necessarily
bound to anatomical landmarks, and reflects the degree of
\textit{functional--anatomical correspondence} between a brain function and its
underlying anatomical location.


% case of PPA
% cf. also \citet{frost2012measuring, rosenke2021probabilistic}
% \citet{weiner2018defining} showed ``that cortical folding patterns and
% probabilistic predictions reliably identify place-selective voxels in medial
% VTC across individuals and experiments''.
%
% However, ``this structural-functional coupling is not always perfect and there
% is inter-subject variability as to how much the place-selective voxels extend
% within the parahippocampal gyrus, as well as the lingual gyrus and medial
% aspects of the fusiform gyrus.
%
% Despite this inter-subject variability, place-selective voxels are always
%located within the collateral sulcus across participants.''
% \citep{weiner2018defining}.


\subsubsection{Functional alignment}

\todo[inline]{I simply dropped "matrix" from "transformation matrix"; sound a
little vague [to me] now...}

%
Since anatomical alignment addresses the issue of anatomical variability but not
functional-anatomical variability across subjects, algorithms---like
\textit{hyperalignment} \citep{haxby2011common, guntupalli2016model} or the
\textit{shared response model} \citep{chen2015reduced, zhang2016searchlight}---
have been developed that perform a \textit{functional alignment}.
%
Whereas anatomical alignment aligns voxels (or vertices) that share the same
anatomical location to a common anatomical space, functional alignment aligns
voxels (or vertices) that share similar functional properties to a
\textit{common functional space} (CFS).
%
Functional alignment algorithms are usually used to calculate both a
high-dimensional, functional brain template (i.e. the \ac{cfs}) from study
participants' functional data as well as subject-specific transformations.
%
A subject-specific transformation allows a mapping of functional data from a
subject's three-dimensional voxel space into the \ac{cfs}, or to project data
from the \ac{cfs} into a subject's voxel space \citep{haxby2020hyperalignment,
kumar2020brainiak}.
%
The \ac{cfs} and transformations can be calculated (i.e. \textit{trained})
based on the maximization of the inter-subject similarity of \ac{bold} response
time series correlating with a time-locked external stimulation
\citep{haxby2011common, chen2015reduced, sabuncu2010function}, or based on the
inter-subject similarity of connectivity profiles \citep{feilong2018reliable,
guntupalli2018computational, nastase2019leveraging}.
%
Though functional alignment algorithms can be applied to \ac{fmri} time series
data from paradigms employing simplified stimuli, data from naturalistic stimuli
provide
%
increased generalizability of the \ac{cfs}
%
and transformation matrices
%
to novel stimuli or tasks, presumably because naturalistic stimuli sample a
broader range of brain states \citep{haxby2011common, guntupalli2016model}.


\todo[inline]{Keep in mind our case for the discussion: what makes the
prediction of the localizer based on localizer runs for alignment so "bad"? Is
it our model or the transformation matrices? In any case, our results are
different from \citep{haxby2011common}: \acf{cfs} \& transformation matrices
based on visual localizer that predict the localizer as good as / better than
than \ac{cfs} \& transformation matrices based on movie!}



\subsubsection{Estimation via functional alignment}

% \citep{guntupalli2016model}: s. Supplementary Figure S8.

\todo[inline]{Problem: \citet{feilong2022individualized} assess data quantity,
too; results suggest 30m are "good" in case of their model.}

\todo[inline]{\citet{jiahui2022cross} do connectivity-based 1-step
hyperalignment across different movie datasets; i.e no \ac{cfs} but one
transformation matrix for every pair of subjects (which is a shitty procedure to
scale, imo)}

\todo[inline]{Depends on the discussion: maybe, add findings from
\citet{haxby2011common} that compare (within- and across-paradigm) prediction
based on naturalistic \ac{cfs} (more TRs) to non-naturalistic \ac{cfs}
(less TRs); if this is mentioned, within- and across-paradigm prediction
should be defined here already and not below in "Here, we..."}

%
Hence, a more recent procedure \citep[e.g., ][]{jiahui2020predicting,
guntupalli2016model, haxby2011common} to estimate the most probable location of
a functional area in a person's anatomy from a reference group performs an
functional alignment.
% solve functional-anatomical variability
First, functional data from persons in the reference group are anatomical
aligned to a common anatomical space.
%
Second, in order to address the issue of functional-anatomical variability
across persons, functional data are functionally aligned (i.e. projected into) a
\ac{cfs}.
%
Then, data are projected from the \ac{cfs} into the individual person's brain
anatomy serving as an estimation of a functional region's location.

% Jiahui
For example, \citet{jiahui2020predicting} used surface-based hyperalignment to
calculated both a \ac{cfs} and transformations based on data from
%
a) the whole movie ``Grand Budapest Hotel'' ($\approx$\unit[50]{m};
\ac{tr}=\unit[1]{s}), and
%
b) the whole movie ``Forrest Gump'' ($\approx$\unit[120]{m};
\ac{tr}=\unit[2]{s})
%
in order to estimate $Z$-maps resulting from a visual localizer's
mass-univariate \ac{glm} $t$-contrast that aimed at localizing the \ac{ffa}.
% summary of results
\citet{jiahui2020predicting}'s results revealed that mass-univariate GLM
contrast maps correlate more highly with contrast maps that were estimated via
hyperalignment from the reference group than contrast maps that were estimated
via surface-based anatomical alignment \citep{jiahui2020predicting}.




\subsection{Here, we...}

\todo[inline]{mentioned here that number of voxels is constrained to a ROI?}

\todo[inline]{I "defined" the terms \textit{criterion} and \textit{predictors}
to make the discussion easier; however, it's not predictor / criterion in a
strict sense as usually used (which refer to single variables)}

% focus: ppa
Here again, we focus on the \ac{ppa} \citep[e.g.,][for
reviews]{epstein2014neural, aminoff2013role}, and explore whether we can
estimate the results of $t$-contrasts (i.e. $Z$-maps) that were created to
localize the \ac{ppa} using functional data of three different paradigms serving
as the to predicted \textit{criteria}:
%
a) a classic visual localizer \citep{sengupta2016extension} as the assumed
``gold standard'',
%
b) a movie \citep{haeusler2022processing}, and
%
c) an auditory narrative \citep{haeusler2022processing}.


\subsubsection{SRM}

\todo[inline]{check via code if $W_{n}^{T}W_{n}=I_{k}$ == True}

% math stuff from citep{vodrahalli2018mapping}
% ``SRM learns $N$ maps $W_{i}$ with orthogonal columns such that
% $||X_{i}-W_{i}S||_{F}$ is minimized over $\left\{ W_{i}\right\} _{i=1}^{N},S$,
% where $X_{i}\in\mathbb{R}^{v\times{T}}$ is the $i^{th}$ subject's fMRI
% response ($v$ voxels by $T$ repetition times) and
% $S\in\mathbb{R}^{k\times{T}}$ is a feature time-series in a $k$-dimensional
% shared space'' \citep{vodrahalli2018mapping}.

% Inverse vs. transpose of a matrix:
% for orthogonal transformations (like we should have here, i.e. only rotation,
% expansion) these two are one and the same thing:
% https://www.quora.com/When-is-the-inverse-of-a-matrix-equal-to-its-transpose

% why SRM
Our volume-based functional alignment approach utilizes the \ac{srm} algorithm
\citep{chen2015reduced, richard2019fast} as implemented in the open-source
software package BrainIAK \citep[Brain Imaging Analysis Kit;
\href{https://brainiak.org}{\url{brainiak.org}};][]{kumar2020brainiak,
kumar2020brainiaktutorial}.
% general overview of SRM
The \ac{srm} is an unsupervised probabilistic latent-factor model that
decomposes \ac{bold} \ac{fmri} response time series of participants experiencing
the same stimulus into a \ac{cfs} of \textit{shared features} \citep[also called
``\textit{shared feature space}'';][]{chen2015reduced} and subject-specific
linear transformations matrices.
% math stuff
More specifically, the \ac{srm} algorithm uses each $n^{th}$ subject's response
time series represented as matrix $X_{n}$ ({$v$} voxels by $t$ time points) to
calculate the \ac{cfs} $C$ ($k$ shared responses by $t$ time points) and
subject-specific, transformation matrices $W_{n}$ ($v$ voxels by $k$ shared
responses) with orthonormal columns ($W_{n}^{T}W_{n}=I_{k}$).
% iteratively fitted
The algorithm randomly initializes and fits the transformation matrices over
iterations to minimize the error in explaining the participants' data, while
also learning the time course of the shared responses (s.
\href{https://brainiak.org/tutorials/11-SRM/}{\url{brainiak.org/tutorials/11-SRM}}).
% number of dimensions
In contrast to hyperalignment, the number of dimensions of the \ac{cfs} is not
set by the number of voxels but is set by the researcher to a number lower than
the number of voxels, a procedure that also filters out noise and reduces
overfitting \citep{chen2015reduced}.
% phrase math in words
Hence, each shared feature can be understood as a weighted sum of many voxels
across subjects \citep{kumar2020brainiak}.
% result = alignment
A subject-specific transformation matrix can be understood as the weight of each
voxel in a subject's voxel space on each shared feature, and allows to
functionally align a subject's functional data to the \ac{cfs} by projecting
responses within the voxels into the $k$-dimensional \ac{cfs}.



\subsubsection{Three aspects under investigation}

\todo[inline]{it might be too "method-detailed" here but number of respective
\acp{tr} are now stated here \& in the plot's (as in the review of
\citet{jiahui2020predicting} above)}

% multi-paradigm model
Contrary to previous studies \citep[e.g.][]{jiahui2020predicting,
guntupalli2016model, haxby2011common} that created a \ac{cfs} based on data from
a single paradigm, we calculate a \textit{multi-paradigm \ac{cfs}}:
% cross-validation
following an exhaustive leave-one-subject-out cross-validation (N$=$14
subjects), we train a shared feature space (i.e. the \ac{cfs}) based on
concatenated response time series of
%
the movie ``Forrest Gump'' ($\approx$\unit[120]{m}; \ac{tr}=\unit[2]{s}),
%
the movie's audio-description  that was produced for a visually impaired
audience ($\approx$\unit[120]{m}; \ac{tr}=\unit[2]{s}), and
%
a visual localizer ($\approx$\unit[21]{m}; \ac{tr}=\unit[2]{s})
%
from $N-1$ \textit{training subjects} as seen in
Fig.~\ref{fig:multi-stimulus-cfs}.



\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/multi-stimulus-cfs.pdf}
\caption{
%
\textbf{Overview of the shared response model.
}
    %
    For each fold of the leave-one-subject-out cross-validation, each training
    subject's response time series of
    %
    a movie ($\approx$\unit[120]{m}; \ac{tr}=\unit[2]{s}),
    %
    the movie's audio-description ($\approx$\unit[120]{m}; \ac{tr}=\unit[2]{s}),
    %
    and the visual localizer ($\approx$\unit[21]{m}; \ac{tr}=\unit[2]{s})
    %
    were concatenated to serve as input for the \ac{srm} algorithm.
%
From these response time series represented as matrix $X_{n}$ ({$v$} voxels
by $t$ time points), the algorithm calculates the common functional
space (CFS) $C$ ($k$ shared features by $t$ time points) and
subject-specific, transformation matrices $W_{n}$
($v$ voxels by $k$ shared features) with orthonormal columns
($W_{n}^{T}W_{n}=I_{k}$).
} \label{fig:multi-stimulus-cfs} \end{figure*}

\todo[inline]{Problem: validity of \ac{cfs} is not really separable from
validity of transformations in the current approach (if at all separable):}

% three aspects to explore
In the present study, we aim to investigate three aspects:
% the criteria
first, in order to explore the validity and generalizability of our
multi-paradigm \ac{cfs}, we predict a left-out \textit{test subject}'s results
from the analysis of
%
a) the localizer \citep{sengupta2016extension} as the assumed ``gold standard'',
%
b) the movie \citep{haeusler2022processing}, and
%
c) the auditory narrative \citep{haeusler2022processing}
%
serving as the criteria.
% three predictors
Second, in order to explore the validity and generalizability of a test
subject's transformation matrices based on data from different paradigms, we use
a test subject's response time series from each of the three paradigms
independently serving as the \textit{predictors} in order to align the test
subject with the corresponding \acp{tr} within the \ac{cfs} (i.e. one
\textit{within-paradigm prediction}, and two \textit{cross-paradigm predictions}
per predictor).
% partial alignment
Third, since using a complete naturalistic stimulus to align a test subject is
impractical in a clinical setting, we also explore the relationship between the
estimation performance of the $t$-contrast's results and the quantity of data
from each of the three paradigms used to functionally align the subject with the
multi-paradigm \ac{cfs}.
% benchmark: anatomical alignment
Lastly, we compare the performance of your volume-based, functional alignment
procedures to the performance of a volume-based, anatomical alignment approach
that serves as a benchmark.



\textbf{Questions as discussed during our meeting}:
\begin{itemize}

\item What is causing the difference?

\item a) is it the quality of the alignment? or:

\item b) is the model doing a better prediction
    (i.e. is the criterion doing an individual overfit?)

\item SRM could just do a denoising, too?

\item How do I know if the "localizer PPA" is the "gold standard"?

\item "tacit assumption"; what does the localizer actually do?
    does it what we want it to do?

\end{itemize}


\subsection{Summary of results}

\todo[inline]{2-3 sentences here}

%
Our results provide evidence that transformation matrices calculated based
on data from naturalistic stimuli promise an increased validity of derived
transformation for functional alignment over transformation matrices based on
data (of the same!) paradigm based on simplified stimuli.



\subsection{Conclusion \& Vision}

\todo[inline]{2-3 sentences here; better write more in discussion}

Our results suggest that it is possible to ``scan once, estimate many''...



\section{Methods}

% we get the data from the naturalistic PPA paper (its subdataset)
% datalad get -n inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned
% datalad get inputs/studyforrest-ppa-analysis/inputs/studyforrest-data-aligned/sub-??/in\_bold3Tp2/sub-??\_task-a?movie\_run-?\_bold*.*

% reference to PPA-Paper
For the current study, we used the same subset of the studyforrest dataset that
was used in \citet{haeusler2022processing}:
%
the same subjects ($N=14$) were
% VIS
a) participating in a dedicated six-category block-design visual localizer
\citep{sengupta2016extension},
% AV
b) watching the audio-visual movie ``Forrest Gump''
\citep{hanke2016simultaneous}, and
% AD
c) listening to the movie's audio-description \citep{hanke2014audiomovie}.
% see corresponding papers for details
An exhaustive description of participants, stimulus creation, procedure,
stimulation setup, and fMRI acquisition can be found in the corresponding
publications, whereas a summary is provided in \citet{haeusler2022processing}.



\subsection{Preprocessing}

% data sources
The current analyses were carried out on the same preprocessed fMRI data (s.
\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned
}{\url{github.com/psychoinformatics-de/studyforrest-data-aligned}}) that were
used for
%
a) the technical validation of the dataset \citep{hanke2016simultaneous},
%
b) the localization of higher-visual areas \citep{sengupta2016extension}, and
%
c) the investigation of responses of the \ac{ppa} correlating with naturalistic
spatial information in study 2 \citep{haeusler2022processing}.

%
We reran the preprocessing and analyses steps performed in
\citet{sengupta2016extension} and \citet{haeusler2022processing} using FEAT
v6.00 \citep[FMRI Expert Analysis Tool;][]{woolrich2001autocorr} as shipped with
FSL v5.0.9 \citep[\href{https://www.fmrib.ox.ac.uk/fsl}{FMRIB's Software
Library;}][]{smith2004fsl} in order to reproduce both the time series that
served as final input for the statistical analyses in the two previous studies
as well as their results (i.e. the statistical $Z$-maps).
% temporal filtering
In summary, high-pass temporal filtering was applied using a Gaussian-weighted
least-squares straight line to every run of the visual localizer (cutoff period
of \unit[100]{s}; sigma= \unit[100]{s}/2)\todo{???}, and every segment of the
movie and audio-description (\unit[150]{s}; sigma=\unit[75.0]{s}).
% brain extraction
Brains were extracted from surrounding tissues using BET \citep{smith2002bet}.
% spatial smoothing
As in the previous studies, data from all three paradigms were spatially
smoothed (Gaussian kernel with full width at half maximum of \unit[4.0]{mm}).
% grand mean normalization
A grand-mean intensity normalization was applied to each run of the functional
localizer and each segment of the naturalistic stimuli.

%
Further analyses on theses reproduced times series were performed via Python
scripts that relied on
%
NiBabel v3.2.1 (\href{https://nipy.org}{\url{nipy.org}}),
%
NumPy v1.20.2 (\href{https://numpy.org}{\url{numpy.org}}),
%
Pandas v1.2.3 (\href{https://pandas.pydata.org}{\url{pandas.pydata.org}}),
%
Scipy v1.6.2 (\href{https://scipy.org}{\url{scipy.org}}),
%
scikit-learn v1.0 (\href{https://scikit-learn.org}{\url{scikit-learn.org}}),
%
BrainIAK v0.11
\citep[\href{https://brainiak.org}{\url{brainiak.org}}][]{kumar2020brainiak,
kumar2020brainiaktutorial},
%
Matplotlib v3.4.0 (\href{https://matplotlib.org}{\url{matplotlib.org}}),
%
seaborn v0.11.2 (\href{https://seaborn.pydata.org}{\url{seaborn.pydata.org}}),
%
and calling command line functions of FSL.

%\paragraph{Fixing FSL output}

% grand_mean_for_4d.py (formerly: data_normalize_4d.py):
% is not necessary anymore: FSL has applied grand mean scaling to
% 'filtered_func_data.nii.gz'

% input: 'sub-*/run-?.feat/filtered_func_data.nii.gz' (of VIS, AO & AV)
% output: saved to 'sub-??_task-*_run-?_bold_filtered.nii.gz'

% FSL adds back the mean value for each voxel's time course at the end of the
% preprocessing;
% hence, the script substracts that mean again but multiplies it by 10000
% (like FSL does it, too)

% definition of grand mean scaling for 4d data:
% voxel values in every image are divided by the average global mean
% intensity of the whole session. This effectively removes any mean global
% differences in intensity between sessions.

% FSL User Guide:
% filtered_func_data will normally have been temporally high-pass filtered,
% it is not zero mean; the mean value for each voxel's time course has been
% added back in for various practical reasons.
% When FILM begins the linear modeling, it starts by removing this mean.


\subsubsection{Region of Interest}

\todo[inline]{two minor issues: grpPPA contains N=14 subject, not N-1 subjects;
warped back into subject-space, some voxels of individual masks is outside of
the back-projected union of masks}

% masks-from-mni-to-bold3Tp2.py:
% - merges unilateral ROIs overlaps (already in MNI) to bilateral ROI
% - output: 'masks/in_mni/PPA_overlap_prob.nii.gz'
% - warps union of ROIs from MNI into each subjects space
% output: 'sub-*/masks/in_bold3Tp2/grp_PPA_bin.nii.gz' + audio_fov.nii.gz dilate
% the ROI masks by 1 voxel; output: 'grp_PPA_bin_dil.nii.gz'

% masks-from-mni-to-bold3Tp2.py:
% warp MNI masks into individual bold3Tp2 spaces

% masks-from-t1w-to-bold3Tp2.py:
% transforms 'inputs/tnt/sub-*/t1w/brain_seg*.nii.gz'
% into individual's bold3Tp2
% output: 'sub-*/masks/in_bold3Tp2/brain_seg*.nii.gz'

% mask-builder-voxel-counter.py:
% builds different individual masks by dilating, merging other masks
% creates a FoV of AO stimulus for every subject from 4d time-series of AO run
% output: sub-*/masks/in_bold3Tp2/audio_fov.nii.gz'
% counts the voxels
% long story short: we cannot used all gyri that contain PPA to some degree
% even if the mask by FoV of AO stimulus and individual gray matter mask

% data_mask_concat_runs.py:
% masks are not dilated and not masked with subject-specific gray matter mask
% outputs:
% 'sub-*_task_aomovie-avmovie_run-1-8_bold-filtered.npy
% 'sub-*_task_visloc_run-1-4_bold-filtered.npy'

% reason why we do it
The \ac{srm} requires that the number of samples (i.e. the number of \acp{tr})
exceed the number of features (the number of voxels).
%
In order to restrict the number of voxels, we created bilateral \acp{roi} for
each subject by warping the union of individual \acp{ppa}
\citep[s.][]{haeusler2022processing} from MNI space into each subjects' voxel
space using previously computed subject-specific, non-linear transformation
matrices
\citep[][\href{https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms
}{\url{github.com/psychoinformatics-de/studyforrest-data-templatetransforms}}]{hanke2014audiomovie}.
% applying masks
Each subject's time series data were then masked by the union of individual
\acp{ppa} and the subject-specific \ac{fov} of the audio-description.
% voxels = [1665, 1732, 1400, 1575, 1664, 1951, 1376, 1383, 1683, 1887, 1441,
% 1729, 1369, 1437]
% median = 1619.5
The number of remaining voxels per subject (range 1369--1951,
$\overline{X}=1592$, $SD=188$) can be seen in Fig.~\ref{fig:plot_voxel-counts}.


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/plot_voxel-counts.pdf}
\caption{
%
\textbf{Number of voxels in the bilateral regions of interest (ROIs)
of each subject.}
%
In order to reduce the number of voxels, we warped the union of
individual \acp{ppa} \citep[cf. Fig. 1 in][]{haeusler2022processing} from
MNI152 space into each subject's native voxel space.
%
The remaining voxels of each subject were further constrained to those
voxels that are included in the respective subject's \ac{fov} of the
audio-description \citep[cf.][]{hanke2014audiomovie}.
}
\label{fig:plot_voxel-counts}
\end{figure*}


\begin{comment}

The number of remaining voxels per subject can be seen in Table
\ref{tab:ppamaskvoxels} (range 1369--1951, $\overline{X}=1592$, $SD=188$).


\begin{table*}[btp]
\caption{
%
\textbf{Table heading.}
%
Number of remaining voxels after time series data of each paradigm
and subject were masked with the union of individual \acp{ppa} that was
warped from MNI space into each individual's subjects-space and the
subject-specific field of view of audio-description.}

\label{tab:ppamaskvoxels}
\begin{tabular}{ll}
\toprule
\textbf{Subject} & \textbf{no. of voxels} \\
\midrule
sub-01 & 1665 \tabularnewline
sub-02 & 1732 \tabularnewline
sub-03 & 1400 \tabularnewline
sub-04 & 1575 \tabularnewline
sub-05 & 1664 \tabularnewline
sub-06 & 1951 \tabularnewline
sub-14 & 1376 \tabularnewline
sub-09 & 1383 \tabularnewline
sub-15 & 1683 \tabularnewline
sub-16 & 1887 \tabularnewline
sub-17 & 1441 \tabularnewline
sub-18 & 1729 \tabularnewline
sub-19 & 1369 \tabularnewline
sub-20 & 1437 \tabularnewline
\bottomrule
\end{tabular}
\caption*{The legend text goes here.}
\end{table*}

\end{comment}



\subsubsection{Concatenation of time series}

\todo[inline]{this is actually the first step of the functional
alignment procedure; but: it kind of belongs to preprocessing}

\todo[inline]{I z-scored the runs runwise, concatenated them, and z-scored
again}

% normalization
Data of every run were independently normalized ($z$-scored) to a mean of zero
($\overline{X}=0$) and a standard deviation of one ($SD=1$).
%
The last 75 \acp{tr} of the audio-description were missing in subject 04 due to
an image reconstruction problem \citep[s.][]{hanke2014audiomovie}.
%
The \ac{srm} allows the number of voxels to be different across subjects but the
number of \acp{tr} must be the same.
%
Hence, we removed the last 75 \acp{tr} of the audio-description from the all
other subjects' time series.
% summary; AO + AV = 7123 TRs (not 7198 TRs anymore); localizer has 4 x 156 TRs
As a result, the data to fit the \ac{srm} in the following step comprised 3599
\acp{tr} of the movie, 3524 \acp{tr} of the audio-description, and 624 \acp{tr}
of the visual localizer experiment (7747 \acp{tr} in total).
%% concatenate and z-score
The time series of all three paradigms were concatenated and $z$-scored.



\subsection{Estimation via anatomical alignment}

\todo[inline]{strictly speaking data were not projected through the MNI152 atlas
but a study-specific brain template co-registered to the MNI152 atlas, wasn't
it?}

%
As a baseline, we used anatomical alignment procedure to estimate the results of
the three $t$-contrast (i.e. the \textit{empirical $Z$-maps}).
%
A test subject's empirical $Z$-map from the analysis of
%
the visual localizer \citep{sengupta2016extension},
%
the movie \citep{haeusler2022processing}, and
%
the audio-description \citep{haeusler2022processing}
%
were predicted based on the training subjects' results of the same paradigm
(hence, a within-paradigm prediction).
%
First, the empirical $Z$-maps of the training subjects' localizer contrasts were
masked with the same subject-specific masks as the time series data that were
used to generate the \ac{cfs}.
% anatomical alignment; into MNI
We used previously computed transformation matrices
\citep[][\href{https://github.com/psychoinformatics-de/studyforrest-data-templatetransforms}{\url{github.com/psychoinformatics-de/studyforrest-data-templatetransforms}}]{hanke2014audiomovie}
to project the respective $t$-contrast's masked $Z$-map via a non-linear
transformation from each training subject's voxel space into the MNI space.
% from MNI into subject
Then, we  used the transpose of the transformation matrix [or was it another
matrix?]\todo{check that} to project the data from MNI space into the test
subject's voxel space.
% take the mean
For each of the three $t$-contrast, the arithmetic mean of the respective
projected $Z$-maps served as an estimation (hence, a \textit{predicted $Z$-map})
of the test subject's empirical $Z$-map.



\subsection{Estimation via functional alignment}
%
Our procedure to estimate the $t$-contrasts' empirical $Z$-maps via functional
alignment is conducted in four steps:
% create CFS and training subjects' matrices
first, for every fold of a leave-one-out cross-validation (N$=$14 subjects), we
trained a \ac{srm} on $N-1$ training subjects' response time series
of the movie, the audio-description, and the visual localizer.
% results in...
This first step provides both a \ac{cfs} for every fold of the cross-validation
and an orthonormal transformation matrix for each training subject.
% align test subject
Second, we used time series data from the visual localizer, the movie, or the
audio-description to align the test subject to the corresponding \acp{tr} within
the \ac{cfs}.
%
Therefore, this second step provides different transformation matrices for the
test subject based on data from different paradigms.
% quantity vs. performance
Moreover, we wanted to investigate the relationship between the estimation
performance [of the $t$-contrast's results] and the quantity of data used to
acquire a transformation matrix.
% therefore
Therefore, we also varied the number of runs of the paradigms used to align the
test subject resulting in different transformation matrices based on different
amounts of \ac{tr}.
% project into CFS
In the third step, we used the training subjects' transformations matrices to
perform a mapping of the training subjects' empirical $Z$-maps from their
respective voxel space into the \ac{cfs}.
% project from CFS into test subject
In the fourth step, we used the transpose of the test subject's transformation
matrix to project the training subjects' $Z$-maps from the \ac{cfs} into the
test subject's voxel space.
% actual prediction
For each of the three $t$-contrasts, the arithmetic mean of the projected
$Z$-maps serves as a test subject's predicted $Z$-map.



\subsubsection{Fitting the shared response model (SRM)}

%
In order to acquire the \ac{cfs} and the training subjects' transformation
matrices, we used the probabilistic \ac{srm} algorithm that is implemented in
BrainIAK v.11 \citep[Brain Imaging Analysis Kit;][]{kumar2020brainiak,
kumar2020brainiaktutorial}, and approximates the \ac{srm} based on the
Expectation Maximization (EM) algorithm as proposed by \citet{chen2015reduced}
and optimized by \citet{anderson2016enabling}.


\paragraph{Number of dimensions}

\todo[inline]{is description of \citet{haxby2011common} too long?}

% ``The effect of number of PCs on BSC was similar for models that were based
% only on Princeton (n = 10) or Dartmouth (n = 11) data, suggesting that this
% estimate of dimensionality is robust across differences in scanning hardware
% and scanning parameters'' \citep{haxby2011common}.
%
% ``These dimensionality estimates are a function of the spatial and temporal
% resolution of fMRI and the number and variety of response vectors used to
% derive the common space'' \citep{guntupalli2016model}.
%
% ``The true dimensionality of representation in human cortex surely involves
% vastly more distinct tuning functions. Estimates of the dimensionality of
% cortical representation, therefore, will almost certainly be much higher as
% data with higher spatial and temporal resolution for larger and more varied
% samples of response vectors are used to build new common models''
% \citep{guntupalli2016model}.

% features
For the number of shared features (i.e. the number of the \ac{cfs}'s
dimensions), we chose a value of $k=10$ considering a) the temporal and spatial
resolution of our data (\ac{tr} = \unit[2]{s}; \unit[2.5 $\times$ 2.5 $\times$
2.5]{mm}), b) the average number of voxels per \acp{roi}, b) and findings from
\citet{haxby2011common}.
%
\citet{haxby2011common} first used hyperalignment to create a \ac{cfs} of 1,000
dimensions based of functional data (\ac{tr} = \unit[3]{s}) of voxels (of size
\unit[3 $\times$ 3 $\times$ 3]{mm}) located in the ventral temporal cortex.
%
Then, \citet{haxby2011common} reduced the dimensionality of the \ac{cfs} by
applying a \ac{pca} in order to determine the subspace that is sufficient to
capture the full range of response-pattern distinctions.
%
Results revealed that approximately 35 principal components (i.e. dimensions)
were sufficient to represent the information content of a one-hour movie from
which the \ac{cfs} was derived.
%
Results also showed that the cortical topographies of category-selective brain
regions was preserved in the 35-dimensional \ac{cfs} \citep{haxby2011common}.
%
In the current study, we also computed \acp{cfs} of $k=5, 20, 30, 40, 50$ but
[---judged by awesome eyeballing---]prediction performance based on these
\acp{cfs} barely varied from a 10-dimensional \ac{cfs}.
% iterations:
The number of iterations for the algorithm to minimize the error was set to 30.



\paragraph{Correlation of regressors used in our previous studies with shared
responses}

\todo[inline]{sounds better now but is still complex}

\todo[inline]{when plots are "perfect", put the model plot from Fig. 5.1 into
the upper triangles; it should be pretty easy to grasp visually anyway}

\todo[inline]{make colors of non-used TRs more transparent}

\todo[inline]{which could be added: plots of the distance matrix showing the
similarity of \acp{tr} in the model (across shared features); imo, reader does
not gain a lot of information it; it's more "for the sake of showing it"}

\todo[inline]{One sentence about what the plots "suggest"?}

% Intro
In order to visualize characteristics of the \ac{cfs}, we calculated the Pearson
correlation coefficients between the shared responses and the regressors that
were previously modeled \citep[cf.][]{sengupta2016extension,
haeusler2022processing} to investigate hemodynamic responses during the three
paradigms.
%
As an example, we chose the \ac{cfs} that was created in the first fold of the
cross-validation from $N-1$ subjects to estimate $Z$-maps of subject 01.
%
The time series of the shared features were trimmed to match the corresponding
\acp{tr} of the respective paradigms.
%
The correlations between regressors that were created to model hemodynamic
responses during the visual localizer and shared responses (trimmed to \acp{tr}
that match the \acp{tr} of the visual localizer) can be seen in
Fig.~\ref{fig:corr-vis-reg-srm}.
%
The correlations between regressors that were created to model hemodynamic
responses during the movie \citep[cf. Table 3 in][]{haeusler2022processing} and
shared responses can be seen in Fig.~\ref{fig:corr-av-reg-srm}.
%
The correlations between regressors that were created to model hemodynamic
responses during the audio-description \citep[cf. Table 3
in][]{haeusler2022processing} and shared responses can be seen in
Fig.~\ref{fig:corr-ao-reg-srm}.

\todo[inline]{To keep in mind for the discussion: the highest correlations
between individual regressors and shared responses can be seen in the visual
localizer => but the matrices based on localizer runs suck the most for
estimation; interpretation?}


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/corr_vis-regressors-vs-cfs_sub-01_srm-ao-av-vis_feat10-iter30_7123-7747.pdf}
\caption{
%
\textbf{Pearson correlation coefficients between regressors of the visual
localizer and shared features.}
%
The time series of the shared features within the multi-paradigm \ac{cfs}
%
(as calculated for subject 01 in the first fold of the cross-validation)
%
were trimmed to match the corresponding \acp{tr} of the visual localizer
paradigm \citep{sengupta2016extension}.
%
The six regressors of the visual localizer model hemodynamic responses to
the six categories of pictures that were presented in blocks.
}
\label{fig:corr-vis-reg-srm}
\end{figure*}


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/corr_av-regressors-vs-cfs_sub-01_srm-ao-av-vis_feat10-iter30_3524-7123.pdf}
\caption{
%
\textbf{Pearson correlation coefficients between regressors of the movie
and shared features}
%
The time series of the shared features within the multi-paradigm \ac{cfs}
%
(as calculated for subject 01 in the first fold of the cross-validation)
%
were trimmed to match the corresponding \acp{tr} of the movie
\citep{hanke2016simultaneous}.
%
The regressors \texttt{vse\_new} to \texttt{vno\_cut} are based on
annotations movie frames, whereas the regressors
\texttt{fg\_av\_ger\_lr} to \texttt{fg\_av\_ger\_ud} represent low-level
visual or auditory confounds
\citep[cf. Table 3 in][]{haeusler2022processing}.
}
\label{fig:corr-av-reg-srm}
\end{figure*}


\begin{figure*}[tbp]
\centering
\includegraphics[width=\linewidth]{figures/corr_ao-regressors-vs-cfs_sub-01_srm-ao-av-vis_feat10-iter30_0-3524.pdf}
\caption{
%
\textbf{Pearson correlation coefficients between regressors of the
audio-description and shared features.}
%
The time series of the shared features within the multi-paradigm \ac{cfs}
%
(as calculated for subject 01 in the first fold of the cross-validation)
%
were trimmed to match the corresponding \acp{tr} of the
audio-description \citep{hanke2014audiomovie}.
%
The regressors \texttt{body} to \texttt{sex\_m} are based on
annotations of nouns spoken by the audio-description's narrator,
whereas the regressors \texttt{fg\_ad\_ger\_lrdiff} and
\texttt{fg\_ad\_ger\_rms} represent low-level auditory confounds
\citep[cf. Table 3 in][]{haeusler2022processing}.
%
\texttt{geo\&groom} is a combination of
regressors as used on the positive side of the primary contrasts aimed to
localize the \ac{ppa} \citep[cf. Table 5 in][]{haeusler2022processing}.
}
\label{fig:corr-ao-reg-srm}
\end{figure*}


\paragraph{Negative control}

\todo[inline]{Add representation of the model to the plots (which are located in
the appendix)}

% shuffle runs
As a negative control, we randomly shuffled the order of runs of the visual
localizer and the segments of the naturalistic stimuli for each paradigm and
training subject independently before concatenating the time series, fitting the
\ac{srm}, and calculating the Pearson correlation coefficients.
%
We assumed that the \ac{srm} algorithm would not be able to fit "meaningful"
\todo{??} shared responses to randomly shuffled training data.
%
As hypothesized, results based on shuffled time series revealed no or minor
correlations between the shared responses and regressors as can be seen in
Fig.~\ref{fig:corr-vis-reg-srm-shuffled},
Fig.~\ref{fig:corr-av-reg-srm-shuffled}, and
Fig.~\ref{fig:corr-ao-reg-srm-shuffled}.

\todo[inline]{Results suggest that... ???, "no meaningful shared responses";
"random shared response"; "shared responses that do not represent hemodynamic
responses that have anything to do with XY"?}



\subsubsection{Alignment of the test subject}

% AO: 0-451, 0-892, 0-1330, 0-1818, 0-2280, 0-2719, 0-3261, 0-3524
% AV: 3524-3975, 3524-4416, 3524-4854, 3524-5342, 3524-5804, 3524-6243,
%     3524-6785, 3524-7123
% AO+AV: 0-7123

\todo[inline]{How is it done? in-code documentation says:
    \# Solve the Procrustes problem;
    A = subjectFMRIdata.dot(SharedResponses.T);
    U, \_, V = np.linalg.svd(A, full\_matrices = False);
    return U.dot(V)
    }

\todo[inline]{cf.
\href{https://github.com/brainiak/brainiak/blob/master/brainiak/funcalign/srm.py}{\url{https://github.com/brainiak/brainiak/blob/master/brainiak/funcalign/srm.py}}}

\todo[inline]{imo, the paragraph needs to rephrased starting at "we let the
algorithm learn an orthonormal mapping..."}

\todo[inline]{confirm that matrices are in fact orthonormal}

%
We then used the test subject's response time series of the visual localizer,
the movie, or the audio-description to align the test subject \textbf{via
singular value decomposition / via Procrustes transformation} of the test
subject's data matrix to the corresponding \acp{tr} within the \ac{cfs}.
%
For the time series of each paradigm independently, we let the algorithm learn
an orthonormal transformation matrix $W_{n}$ that performs a mapping of the
responses in the test subject's voxel space during the respective paradigm into
the \ac{cfs}.
%
In order to investigate the relationship between the estimation performance [of
the t-contrast's results] and the quantity of data used to acquire a
transformation matrix, we also varied the number of runs per paradigm.
%
In case of \acp{tr} of the visual localizer, we used one up to four runs (each
lasting \unit[5.2]{m}) to align the test subject to the corresponding \acp{tr}
within the \ac{cfs}.
%
In case of the naturalistic stimuli, we used one up to eight segments (each
lasting $\approx$\unit[15]{m}) to align the test subject to the corresponding
\acp{tr} within the \ac{cfs}.
%
Consequently, for every test subject, we obtained four matrices from data of the
visual localizer, and eight different matrices per naturalistic stimulus.
%
Each of transformation matrices has a size of $v$ voxels by $k$ shared responses
but is based on an increasing quantity of data used to calculate the mapping.


\subsubsection{Estimation of $t$-contrasts' results}

% overview
We then estimated the empirical $Z$-maps of the test subject by projecting all
training subjects' empirical $Z$-maps from their voxel space trough the \ac{cfs}
into the test subject's voxel space:
% functional alignment; into CFS (calling srm.transform(masked\_zmaps))
first, we used the training subjects' transformation matrices that were derived
during training of the \ac{cfs} to perform a mapping of the masked empirical
$Z$-maps from each training subject's voxel space into the \ac{cfs}.
% into subject
Then, we used the transpose of a transformation matrix that was acquired from
the alignment of the test subject in order to project the $Z$-maps from the
\ac{cfs} into the test subject's voxel space.
% take the mean
The arithmetic mean of $N-1$ the projected empirical $Z$-maps served as the
predicted $Z$-map that estimates the test subject's empirical $Z$-map.



\subsection{Cronbach's alpha}

\todo[inline]{I rephrased this paragraph a couple of times; still sounds meh...}

\todo[inline]{might be just a spontaneous brain fart: iirc, the "deviant"
participants in the audio-description are essentially the ones that have a poor
Cronbach's; so, they are less probably not "reliable outliers" being different
from the norm but more probably noisy asses; hence, SRM should probably do more
of a denoising as opposed to modelling (reliable) outliers incorrectly}

\todo[inline]{A negative Cronbach's alpha is really shitty; should be discussed
(in the general discussion on naturalistic stimuli + \ac{glm}, too)}

\todo[inline]{Similarly, for general discussion: does a naturalistic stimulus
provide enough events sufficiently balanced across segments when $t$-contrasts
for whatever functions are supposed to be modeled?  Naturalistic stimuli are not
the panacea...!}

\todo[inline]{I have no clue where \citet{jiahui2020predicting, jiahui2022cross}
have the statement about what Cronbach's expresses from...}

%
We calculated Cronbach's $\alpha$ for the empirical $Z$-maps of each paradigm
and subject as a measure of the empirical $Z$-maps reliability and amount of
measurement error \citep{cronbach1951coefficient, cortina1993coefficient}.
%
Cronbach's $\alpha$ expresses the expected correlation between the currently
used empirical $Z$-maps and an additional set of empirical $Z$-maps calculated
based on data of a hypothetical independent dataset collected from the same
paradigm and subjects \citep{jiahui2020predicting, jiahui2022cross}.
%
These expected correlations represented by Cronbach's $\alpha$ were calculated
based on values of the first-level \ac{glm} $Z$-maps (four in case of the visual
localizer; eight in case of the naturalistic stimuli) that were averaged in the
second-level \ac{glm} analyses of their respective study
\citep{sengupta2016extension, haeusler2022processing} yielding the to be
estimated empirical $Z$-maps of the present study.


\todo[inline]{Strictly, the following is about results, not methods}

\todo[inline]{Shift results stated here to actual result section in case a
t-test involving Cronbach's is calculated (which is not the case so far)}
%
Cronbach's $\alpha$ of empirical $Z$-maps for each subject and paradigm can be
seen in Fig.~\ref{fig:cronbachs}, descriptive statistics across subjects for
each paradigm can be seen in Table~\ref{fig:cronbachs}.

\todo[inline]{Provide (some of the) descriptive statistics in the text?}

Visual localizer:  mean=0.899990, std=0.087051, min=0.658523,
25\%=0.906643, 50\%=0.934019, 75\%=0.947906, max=0.963065.
%
Movie: mean=0.611332, std=0.137878, min=0.284440,
25\%=0.555529, 50\%=0.627240, 75\%=0.676353, max=0.800254.
%
Audio-description: mean=0.476194, std=0.358019, min=-0.526626,
25\%=0.428975, 50\%=0.627799, 75\%=0.679987, max=0.823584.


\begin{table*}[btp]
\centering
    \caption{
    %
    \textbf{Descriptive statistics of Cronbach's $\alpha$ across subjects.}
    %
    Imo, not super necessary to provide these numbers. Stripplot + boxplots
    could be sufficient. If table is supposed to be kept, round the numbers,
    write a good description for the table.}
\label{tab:cronbachs}
\begin{tabular}{llll}
    \toprule
    \textbf{statistic} & \textbf{localizer} & \textbf{movie} & \textbf{audio-description} \\
    \midrule
    mean & 0.89999 & 0.611332 & 0.476194 \tabularnewline
    std & 0.087051 & 0.137878 & 0.358019 \tabularnewline
    min & 0.658523 & 0.28444 & -0.526626 \tabularnewline
    25\% & 0.906643 & 0.555529 & 0.428975 \tabularnewline
    50\% & 0.934019 & 0.62724 & 0.627799 \tabularnewline
    75\% & 0.947906 & 0.676353 & 0.679987 \tabularnewline
    max & 0.963065 & 0.800254 & 0.823584 \tabularnewline
    \bottomrule
\end{tabular}
\caption*{The legend text goes here.}
\end{table*}


\begin{figure*}[tbp] \centering
    \includegraphics[width=\linewidth]{figures/plot_cronbachs.pdf}
    \caption{\textbf{Cronbach's $\alpha$ of the empirical $Z$-maps for each
    paradigm and subject.}
    %
    Cronbach's $\alpha$ was calculated based on the $Z$-maps yielded by the
    first-level \ac{glm} analyses of the visual localizer
    \citep{sengupta2016extension} (four runs) and naturalistic stimuli
    \citep{haeusler2022processing} (eight segments each) respectively.
    %
    The second-level \ac{glm} analyses across runs / segments yielded the
    empirical $Z$-maps that were estimated in the present study.
    }
    \label{fig:cronbachs}
\end{figure*}



\pagebreak



\section{Results}

\todo[inline]{I tested all samples of Fisher transformed correlations for
normality via Shapiro-Wilk test (imo, the most appropriate test here)}

\todo[inline]for all samples that were part of one of the eleven t-test, just on
case of one sample (4 runs of localizer in order to estimate localizer) accepted
H1 (sample is NOT drawn from normal distribution); s. below; compute Wilcoxon
signed-rank test (which is for dependent samples)?}}

\todo[inline]{assumption of the t-test of equal variances is probably often not
give but whatever}

\todo[inline]{So far, I did not test any difference(s) to Cronbach's; s.
comments below}


\subsection{Overview}

\todo[inline]{Since the order is "Intro, Methods, Results" and not "Intro,
Results, Discussion, Method", the following part should better fit into the
beginning of the discussion (where it is not written so far)}

Following a leave-one-subject-out cross-validation, we trained a shared response
model based on concatenated response time series of three different paradigms
(i.e. visual localizer, movie, audio-description).
%
For every fold of the cross-validation, we used the test subject's response time
series of the three paradigms independently as predictors by functionally
aligning the test subject to the shared feature space (i.e. \ac{cfs}) in order
to predict the results from previous analyses of the paradigms \citep[i.e. the
empirical $Z$-maps calculated in]{sengupta2016extension, haeusler2022processing}
serving a criteria.
%
The empirical $Z$-maps of each training subject were projected from their
respective voxel space through the \ac{cfs} into the test subject's voxel space
to yield the test subject's predicted $Z$-maps.
%
In case of the anatomical alignment procedure that served as a benchmark, the
training subjects' $Z$-maps were projected via nonlinear, volume-based
transformation through the MNI152 brain atlas \todo{kind of incorrect} into the
test subject's voxel space.
%
Unthresholded empirical and predicted $Z$-maps in their respective subject's
voxel space can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}\todo{still
empty}.
%
In order to quantify the estimation performance of the alignment procedures, we
correlated each individual's empirical $Z$-maps gained from previous analyses
\citep{sengupta2016extension, haeusler2022processing} with their respective
predicted $Z$-map (cf. Fig.~\ref{fig:stripplot}).


\subsection{Cronbach's alpha}

\todo[inline]{shift the Table and Plot of Cronbach's to here? Should
definitively be done when values are actually used for a t-test}

\todo[inline]{movie's outlier: sub-06 (0.28); but when movie PPA is predicted he
/ she is not the outlier}

\todo[inline]{audio-description's outlier: sub-05 (-0.5), sub-02 (0.0), sub-20
(0.27); when audio PPA is predicted (based on max of functional data per
paradigm), these three subjects are the outlier}

%
``Because the localizer task comprises several scanning runs, we calculated the
reliability of the localizer across runs with Cronbach's alpha to provide an
estimate of the noise ceiling for these correlations'' \citep{jiahui2022cross}.


\begin{figure*}[tbp] \centering
    \includegraphics[width=\linewidth]{figures/plot_corr-emp-vs-estimation.pdf}
    \caption{
    %
    \textbf{Correlations between empirical and predicted
    \textit{\textbf{Z}}-maps for each paradigm and subject.}
    %
    Functional alignment was performed based on an increasing amount of
    functional data used to align a test subject to the common functional space
    (CFS): runs of the visual localizer paradigm lasted \unit[5.2]{m}; segments
    of the naturalistic stimuli lasted $\approx$\unit[15]{m}.
    %
    Solid horizontal lines:
    %
    median of Cronbach's $\alpha$ across subjects for empirical $Z$-maps of the
    respectively estimated paradigm (cf. Fig.~\ref{fig:cronbachs}).
    %
    Dotted horizontal lines:
    %
    mean of Cronbach's $\alpha$ across subjects for empirical $Z$-maps of the
    respectively estimated paradigm (cf. Fig.~\ref{fig:cronbachs});
    %
    Grey dots:
    %
    correlations between empirical $Z$-maps and an estimation using anatomical
    alignment.
    %
    A left-out subject's $Z$-map was estimated by projecting the training
    subjects' $Z$-maps ($N=13$) from their respective voxel space through the
    MNI152 space into the test subject's voxel space, then averaging the values
    across projected $Z$-maps;
    %
    Green dots:
    %
    correlations between empirical $Z$-map and an estimation using functional
    alignment based an transformation matrices calculated from one up to four
    runs (each lasting \unit[5.2]{m}) of the visual localizer.
    %
    Red dots:
    %
    correlations between empirical $Z$-map and an estimation using functional
    alignment based an transformation matrices calculated from one up to eight
    segments (each lasting $\approx$\unit[15]{m}) of the movie.
    %
    Blue dots:
    %
    correlations between empirical $Z$-map and an estimation using functional
    alignment based an transformation matrices calculated from one up to eight
    segments (each lasting $\approx$\unit[15]{m}) of the audio-description.
    %
    \textbf{to do: keep both, median and mean of Cronbach's? Improve legend
    somehow without crowding figure / subplots?}
   }
    \label{fig:stripplot}
\end{figure*}


\subsection{General stuff}

\todo[inline]{Jeez! Phrasing!}

%
The [mean] Pearson correlation coefficients vary depending on the criterion to
be estimated (i.e. $Z$-maps of the visual localizer, movie, or
audio-description), the functional alignment procedure (anatomical vs.
functional alignment), and---in case of functional alignment---on the quantity
of a paradigm's data used to align a test subject to the \ac{cfs} as can be seen
in Fig.~\ref{fig:stripplot}.
%
However, the functional alignment procedure across criteria and predictors
reveals a monotonically increasing estimation performance the more functional
data are used to align the test subjects.


\subsection{The exemplary tests}

\todo[inline]{correct the alpha-level (as it is done now) or the p-values?}

\todo[inline]{Report descriptive statistics of the correlations?}

\todo[inline]{Round the values or just write $p$<.0001}

\todo[inline]{Give one-sentence interpretations / summary  la "Results
suggest..."? imo, that's not good}

\todo[inline]{Maybe an ordering according to the "three aspects under
investigation" given in the intro is somehow possible}

\todo[inline]{correlations based on 4 localizer runs fail the test for
normality; I ran \& report t-test here anyway}

%
In order to investigate the differences between some conditions [well, if you
know what I mean] more closely, we post-hoc performed eleven [final number?]
paired t-tests on Fisher z-transformed correlation values, and set the
$\alpha$-level to a Bonferroni corrected $\alpha$ of $0.05 / 11 =
0.00\overline{45}$.
%
\textbf{For example, in case of estimating the $Z$-maps of the visual
localizer}, the correlations between empirical $Z$-maps and $Z$-maps predicted
via the first movie segment were significantly higher than the correlations
between empirical $Z$-maps and $Z$-maps predicted via anatomical alignment
(Fisher z-transformed, t(14)= 6.3525802, $p$=0.0000253).
%
However, functional alignment based on data of the visual localizer
(within-paradigm prediction) and audio-description was lower than anatomical
alignment independent of the number of runs / segments used to align the test
subjects to the \ac{cfs} [not tested; clear from eyeballing].
%
Whereas the functional alignment via the first movie segment (451 \acp{tr})
yielded significantly higher correlations than a functional alignment via four
localizer runs lasting 624 \acp{tr} (Fisher z-transformed, t(14)=5.8905545,
$p$<0.0000532),
%
the functional alignment via the first segment of the audio-description yielded
significantly lower correlations than the functional alignment via four
localizer runs (Fisher z-transformed, t(14)=-2.3000009, $p$<0.0386588, will not
be significant when Bonferroni corrected).
%
The mean correlation between empirical $Z$-maps and $Z$-maps predicted via the
first movie segment were significantly lower than the correlations between
empirical $Z$-maps and $Z$-maps predicted via the first two movie segments
(Fisher z-transformed, t(14)= -5.4946197, $p$=0.0001031, Bonferroni corrected).
%
The difference between mean correlations based on two movie segments in
comparison the mean correlation based on three movie segments was not
significant (Fisher z-transformed, t(14)= -0.1293547, $p$=0.8990569).

\todo[inline]{difference to Cronbach's: maybe, run test that compares 8 movie
segments to Cronbach's alpha? will probably significantly lower}

\todo[inline]{Following phrasing is just copied from above; values are adjusted
accordingly}

\textbf{In case of estimating the $Z$-maps of movie}, the correlations between
empirical $Z$-maps and predicted $Z$-maps via the first movie segment were
significantly higher than the correlations between empirical $Z$-maps and
predicted $Z$-maps via anatomical alignment (Fisher z-transformed, t(14)=
5.7754451, $p$=0.0000643).
%
Whereas the functional alignment via the first movie segment (451 \acp{tr})
yielded significantly higher correlation than a functional alignment via four
localizer runs lasting 624 \acp{tr} (Fisher z-transformed, t(14)=6.8532349,
$p$<0.0000116),
%
the functional alignment via the first segment of the audio-description yielded
no significantly different correlations than the functional alignment via four
localizer runs (Fisher z-transformed, t(14)=-1.8674144, $p$<0.084551).
%
The mean correlation between empirical $Z$-maps and predicted $Z$-maps via the
first segment alone of the movie were significantly lower than the correlations
between empirical $Z$-maps and predicted $Z$-maps predicted $Z$-maps via the
first two movie segments (Fisher z-transformed, t(14)= -3.7454592,
$p$=0.0024485).
%
The prediction performance based on data of the audio-description
increases with more data. However, the "relevant" test compared "similar" amount
of \acp{tr}:
%
The difference between mean correlations based on two movie segments in
comparison the mean correlation based on three movie segments was [not]
different [when adjusted alpha-level used] (Fisher z-transformed, t(14)=
-2.5759899, $p$=0.02303].

\todo[inline]{difference to Cronbach's: run test(s) that compare x movie
segments for alignment and Cronbach's alpha of movie's empirical $Z$-map? will
sooner or later significantly higher than Cronbach }


\todo[inline]{The following part is the ugly one}

\todo[inline]{All samples will probably fail the test of normality}

\todo[inline]{The three outliers are always sub-02, sub-05, sub-20 (always=
anatomical alignment, 4 runs of localizer, 8 runs of movie, 8 runs of
audio-description)}


\textbf{In case of estimating the $Z$-maps of the audio-description} it's a
fucking mess, which is why we tested only one difference [so far]:
%
Correlations between empirical $Z$-maps and predicted $Z$-maps via the first
movie segment were significantly lower than the correlations between empirical
$Z$-maps and predicted $Z$-maps via anatomical alignment (Fisher z-transformed,
t(14)= -4.2004329, $p$=0.0010387, Bonferroni corrected).

\todo[inline]{any differences compared to Cronbach's alpha to test?
audio-description for alignment will sooner or later be significantly higher
than Cronbach's; might be a cue that SRM does an denoising}


\subsection{Template of reporting descriptive statistics of (untransformed
correlations)}

\todo[inline]{Report descriptive statistics of the correlations?}

\todo[inline]{imo, it gets pretty crowded, they can somewhat be inferred from
the stripplot; they get Fisher transformed anyway before testing; mean is
(often) not the best measure of central tendency; }

%
The mean Pearson correlation values [not yet Fisher transformed] between
empirical $Z$-maps and predicted $Z$-maps via anatomical alignment were 0.xx
($N=14$, $\overline{X}=0.xx$, $SD=0.xx$, range [?], median
[9], 25\%, 50\%, 75\%).
%


\subsection{Template of procedure in case normality tests are relevant to
perform and report}

\todo[inline]{Correlations (Z-transformed) of estimation of visual localizer via
4 runs of visual localizer are not normally distributed according to
Shapiro-Wilk test -> use non parametric test instead? Wilcoxon signed-rank
should be the correct one?}

%
In case a test failed to reject the H0 that assumes a normal distribution for
both samples, we perform a t-test [which assumes same variance, but whatever];
in case H1 (non-normality) is accepted for one of the two to be compared
samples, we use Wilcoxon signed-rank test.


\subsection{plot\_bland-altman.py}

\todo[inline]{I hate that script! Hence, it should not be included ;-).}

\todo[inline]{mih: but results?}

\subsection{Plots of brain slices?}

\todo[inline]{iirc, we agreed on that it's not necessary}





\section{Discussion}

\todo[inline]{maybe, biggest issue for discussion: how to separate validity /
generalizability of model from validity / generalizability of transformation
matrices}

\subsection{Short summary of aims \& hypotheses, method, results}

\todo[inline]{Maybe an ordering according to the "three aspects under
investigation" given in the intro is somehow possible}

\subsubsection{Methods}

The \ac{srm} algorithm can be applied to functional data from traditional
paradigms using controlled stimuli.
%
However, results of \citet{haxby2011common} that suggest that \acp{cfs} [and
transformation matrices???] derived from traditional paradigms are of diminished
validity and a diminished generalizability to other paradigms, presumably
because traditional paradigms sample a sparser range of brain states than
naturalistic stimuli.
%
Here, we created a \ac{cfs} (for every fold) based on time-series of three
stimuli (visual localizer, movie, movie's audio-description) that were
concatenated before model fitting.
%
For every paradigms $Z$-map (visual localizer, movie, movie's
audio-description), we use a test subject's response time series to the visual
localizer, movie, or the audio-description to align the test subject to the
respective \acp{tr} within the \ac{cfs} and acquire the respective
transformation matrices.


\subsubsection{Aims}

\todo[inline]{Revise that part in parallel with the text in introduction}

\todo[inline]{It's hard to separate validity and generalizability; I hope the
text below makes sense?}

%
We assess the validity of the \ac{cfs} by estimating $Z$-maps from the analysis
(i.e. $t$-contrast) of same paradigms used to align the test subject
(cross-subject-\textbf{within-paradigm} prediction).
%
We assess the generalizability of the transformation matrices by estimating
$Z$-maps from the analysis (i.e. $t$-contrast) of paradigms that were not used
to align the test subject (cross-subject-\textbf{cross-paradigm} prediction).
%
Prediction based on anatomical alignment served as a benchmark.


\subsubsection{Hypotheses}

\todo[inline]{cf. introduction}


\subsubsection{Results}

\todo[inline]{Write a very high-level summary! It's way too much anyway: 3
$Z$-maps $\times$ 3 paradigms for alignment ($\times$ y runs/segments)}



``Predicting face-selectivity map with lower volumes of movie data (cf. Figure
3C). Face-selectivity maps can be accurately predicted with 20 minutes of movie
data, but the prediction performance continues to grow with more data''
\citep{feilong2022individualized}.

``To summarize, the performance of our model continuously grows with more
training data, but for certain tasks (e.g., individual identification,
predicting category-selectivity and retinotopic maps), only a small amount of
movie data (e.g., 30 minutes) is needed to achieve satisfying performance''
\citep{feilong2022individualized}.

``Note that brain responses to movies contain richer information than
traditional experimental paradigm'' \citep{feilong2022individualized} and
audio-description (it's cross-modal; i.e. lacks visual stimulation; interesting
in case "executive functions" are supposed to be sampled/predicted; however
\citet{haxby2011common} removed \acp{tr} (+ following \acp{tr} from the movie
that contained, e.g. faces).

\todo[inline]{Maybe, restructure to match the "Three aspects under
investigation}

\subsection{Estimation of visual localizer}

We assess how good which stimulus is to perform the alignment to the
corresponding \acp{tr} in our model and subsequently estimate the $Z$-map.


\subsubsection{Assumption that localizer is "ground truth"}

We estimate the $Z$-map of a visual localizer which is the fucking established /
traditional method to identify the \ac{ppa}.
%
% localizer data
\citet{sengupta2016extension} successfully delineated the left-hemispheric
\ac{ppa} in 12 of 14 subjects and right-hemispheric \ac{ppa} in 14 of 14
subjects based on localizer data.


\subsubsection{visual localizer via localizer runs}

\todo[inline]{\citet{haxby2011common}'s non-naturalistic stimulus \acp{cfs} need
to mentioned already (best somewhere in the intro already, not just discussion)
in case the following is supposed to make sense}

Surprisingly \citep[cf.][]{haxby2011common}, the within-experiment prediction
(localizer \acp{tr} for alignment) is worse than anatomical alignment.
%
Independent from the number of runs used.
%
``We applied the hyperalignment parameters derived from the face and object data
to the movie data in the ten Princeton subjects and the hyperalignment
parameters derived from the animal species data to the movie data in the 11
Dartmouth subjects.
%
BSC of 18s movie time segments after hyperalignment based on category perception
experiment data was markedly worse than BSC after hyperalignment based on movie
data (17.6\%  1.3\% versus 65.8\%  2.7\% for Princeton subjects; 28.3\% 
2.8\% versus 74.9\%  4.1\% for Dartmouth subjects; p < 0.001 in both cases;
Figure 4).
%
Thus, hyperalignment of data using a set of stimuli that is less diverse than
the movie is effective, but the resultant common space has validity that is
limited to a small subspace of the representational space in VT cortex''
\citep{haxby2011common}.

How does that come? At least the correlation of regressors with shared responses
show the "clearest" correlations of a regressor with a shared response (which is
not the case in the \acp{tr} of the movie or audio-description).


\paragraph{Template from \citet{haxby2011common}}:
%
``We applied the hyperalignment parameters derived from the face and object data
to the movie data in the ten Princeton subjects and the hyperalignment
parameters derived from the animal species data to the movie data in the 11
Dartmouth subjects.
%
BSC of 18s movie time segments after hyperalignment based on category perception
experiment data was markedly worse than BSC after hyperalignment based on movie
data (17.6\%  1.3\% versus 65.8\%  2.7\% for Princeton subjects; 28.3\% 
2.8\% versus 74.9\%  4.1\% for Dartmouth subjects; p < 0.001 in both cases;
Figure 4).
%
Thus, hyperalignment of data using a set of stimuli that is less diverse than
the movie is effective, but the resultant common space has validity that is
limited to a small subspace of the representational space in VT cortex''
\citep{haxby2011common}.


\subsubsection{visual localizer via movie segments}

%
Results indicate that $\approx$15 minutes of movie watching used for functional
alignment outperform prediction using anatomical alignment.
%
Prediction performance further increases when $\approx$30 minutes of movie data
submitted to the algorithm to calculate the subject-specific transformation
matrices.
%
However, three segments ($\approx$45 minutes) do not lead to an significantly
increased estimation performance suggesting a decreasing benefit of longer
scanning time than 30 minutes during audio-visual naturalistic stimulation.


\subsubsection{visual localizer via audio-description segments}

Audio-description for alignment gets as good (i.e. no statistical difference) as
anatomical alignment but for similar performance >4-6 segments are needed.


\paragraph{interpretation}
%
What might be the cause?
%
Audio-description might not sample the response vector space of the visual
paradigm "well" enough?
%
Maybe, the shared responses in the model space are shitty?

Or the shared responses of the AO within the \ac{cfs} are shitty?
%
The shared responses might suffer from the outliers but "work" to estimate the
audio PPA though.



\subsection{Estimation of movie PPA}

\todo[inline]{How do I know if the "localizer PPA" is the "gold standard" "tacit
assumption"; what does localizer actually do? does it what we want it to do?}

% usually, visual PPA works pretty well
The visual \ac{ppa} can be reliably localized using a localizer, which is why we
considered it to be the "gold standard" or benchmark.
%
However, ``the PPA definition may depend on the type of experiment, task, and
stimuli used'' \citep{weiner2018defining}.

%
The primary movie contrast in \citet{haeusler2022processing} ``yielded bilateral
clusters in five participants, a unilateral right cluster in six participants
(of which one participant yielded a unilateral cluster in the visual localizer),
and a unilateral left cluster in one participant. We find bilateral clusters for
participant sub-20, whereas the block-design localizer yielded only one cluster
in the right hemisphere'' \citep{haeusler2022processing};
%
i.e. 5 + 1  left-hemispheric, 5 + 6 right-hemispheric.


\subsubsection{movie PPA via localizer runs}

Localizer, independent of number of runs, cannot beat anatomical alignment.


\subsubsection{movie PPA via movie segments}

One movie segment beats anatomical alignment.
%
One or two movie segments beat the reliability of the empirical $Z$-map [do the
statistical test].


\paragraph{Template for Cronbach's}

%
\citep{jiahui2022cross}: ``We compared the correlations between topographies
estimated from a participant's own localizer data and those from other
participants' data to the reliability of the localizer, calculated with
Cronbach's alpha.
%
Predictions made with hyperalignment were close to and sometimes even exceeded
the reliability values (Figure 1B).
%
This indicates that the predicted category-selective topographies from other
participants' data using hyperalignment were as precise and sometimes even
better than the topographies estimated with their own localizer data''
\citep{jiahui2022cross}.

%
\citep{feilong2022individualized}: ``For both datasets, the localizer-based and
model-predicted face-selectivity maps were highly correlated (Forrest: r = 0.618
 0.089 [mean  SD], Raiders: r = 0.716  0.074), and the correlations were
higher than our previous state-of-the-art model using the same dataset and
hyperalignment (Jiahui et al., 2020).
%
Across all participants, the average Cronbach's alpha was 0.606  0.126 for
Forrest, and 0.764  0.089 for Raiders.
%
For approximately a third of the participants (Forrest: 6 out of 15, 40\%;
Raiders: 6 out of 20, 30\%), the correlation exceeded the Cronbach's alpha of
localizer-based maps.
%
In other words, for these participants, the predicted map based on our model can
be more accurate than the map based on a typical localizer scanning session
comprising four runs'' \citep{feilong2022individualized}.

%
\citep{feilong2022individualized}: ``Note that for the Forrest dataset, the
similarity sometimes exceeded Cronbach's alpha, which means the model-predicted
map is more accurate than a map based on 4 localizer runs (21 minutes).
%
The quality of localizer-based maps increases with more localizer data, which
can be estimated using the SpearmanBrown prediction formula [Brown, 1910;
Spearman, 1910].
%
Based on the SpearmanBrown prediction formula, we can estimate how Cronbach's
alpha changes with the amount of data (i.e., the number of localizer runs), and
correspondingly, how much localizer data is needed to achieve the quality of the
model-predicted map
%
For the Forrest dataset, the maps predicted by 15, 30, 60, and 120 minutes of
movie data were as accurate as 17, 22, 26, and 30 minutes of localizer data,
respectively.
%
For the Raiders dataset, the maps predicted by 15, 28, and 56 minutes of movie
data were as accurate as 10, 11 and 12 minutes of localizer data, respectively''
\citep{feilong2022individualized}

\paragraph{Interpretation}
%
Our modeling of spatial responses during movie (probably) "moderate"?
%
When it comes to Cronbach's alpha, there are no real outliers but Cronbach's of
about 0.6 is moderate anyway.
%
The participants do not seem to be too heterogeneous in their responses (i.e.
no "prediction outliers" in the stripplot).



\subsubsection{movie PPA via audio segments}

\todo[inline]{the outlier is sub-02}

%
Eight audio-description segments cannot beat anatomical alignment.
%
From latest segments on, probably no difference to anatomical alignment (a.k.a.
as good as).


\subsubsection{Interim summary of movie PPA}

Movie is nice for alignment in order to estimate the visual paradigms.



\subsection{Estimation of audio PPA}

\todo[inline]{it's (almost) always the same 3 outliers: sub 2, 5, 20; those have
the lowest values of Cronbach's alpha, too}

%
Results of \citet{haeusler2022processing} suggest that selected events embedded
in an auditory stimulation is correlated with increased hemodynamic activity in
the anterior parts of the \ac{ppa} as it was defined by the localizer.
%
The primary audio-description contrast in \citet{haeusler2022processing}
``yielded bilateral clusters in nine participants that are within or overlapping
with the block-design localizer results. In participant sub-04, two bilateral
clusters are apparent, whereas block-design localizer, and movie stimulus
yielded only one cluster in the right hemisphere.  For another participant
(sub-09) the analysis yielded one cluster in the left-hemispheric PPA''
\citep{haeusler2022processing};
i.e. 9 + 1 left-hemispheric, 9 right-hemispheric.
%
Hence, we should have sampled response vector(s) that carry the spatial response
information (if you know what I mean).



\subsubsection{audio PPA via anatomical alignment}

If just one run (or segment) is used for functional alignment, then anatomical
alignment performs (probably) better (and is as good as Cronbach's).


\subsubsection{audio PPA via localizer runs}

\todo[inline]{For 1-2 runs, the outliers are sub 2, 4, 5; for 3-4 runs the
outliers are 2, 5, 20 (as usual)}

%
The localizer sucks as usual for estimation.


\subsubsection{audio PPA via movie segments}

\todo[inline]{The outliers are always sub 2, 5, 20}

The more segments are used, the better (and quickly) it gets; gets as good as
anatomical alignment but not better


\subsubsection{audio PPA via audio segments}

\todo[inline]{The outliers are always sub 2, 5, 20}

The more data we use, the better it gets; eventually, outperforming anatomical
alignment and Cronbach's alpha.


\subsubsection{Interim summary of audio PPA}


\todo[inline]{Following is true for movie PPA, too}

%
Results of \citet{haeusler2022processing} should be influenced by paradigm and
methodological choices.
%
In particular, we might have modeled the time course of "spatial responses"
"insufficiently" in \citet{haeusler2022processing}.
%
However, our current results suggest that ``the training stimuli or trials
strongly and variably engage that process in a way that is reliable across
participants'' in such a way that the ``SRM will improve sensitivity for
detecting a cognitive process of interest in the test data''
\citep{cohen2017computational}.

%
Present results add evidence that the responses to auditory spatial information
lead to different activity patterns than visual stimulation [cf. interpretation
in \citet{haeusler2022processing}] and results in \citet{haeusler2022processing}
are not based on the modeling choices?

\todo[inline]{However, my biggest issue: what to do with the three outliers?}
%
Some subject seem to just not have cared about auditory spatial information.


\paragraph{Individual differences}

%
Responses in \ac{ppa} during auditory stimulation might (reliably) be
different (and not just a result that depends on the paradigm and the hilarious
modeling approach in \citet{haeusler2022processing}).
%
If the response pattern is reliably not occurring in these subjects then
individual differences in brain patterns might reflect differences in behavior.
%
Do the subjects simply do not give a shit about auditory spatial information?
%
Are they simply not attending to it? Or are they simply incapable to process it?




\subsection{Discussion across "everything"}

\subsubsection{We built a "multi-paradigm" model}

\todo[inline]{cf. "Templates from \citet{haxby2011common} on validity \&
generalizability" at the very and of this chapter}

\todo[inline]{is there ANY paper except \citet{haxby2011common} that derives a
\ac{cfs} from a non-naturalistic stimulus?}

The multi-paradigm model is kind of valid;
%
However, I have no clue which stimulus is the crucial one...


\subsubsection{Functional vs. anatomical alignment}

Localizer paradigm sucks compared to anatomical alignment when used to estimate
localizer $Z$-map (within-paradigm prediction) and movie PPA (cross-paradigm
prediction).
%
When audio PPA is estimated (cross-paradigm), localizer for alignment gets close
to anatomical alignment [but why].

%
Movie beats anatomical alignment when localizer or movie PPA is estimated (i.e.
the visual paradigms).
%
Movie gets as good as anatomical alignment when audio PPA is estimated.

%
Audio-description sucks---compared to anatomical alignment--for cross-paradigm
estimation.
%
Though, it gets close to the performance via anatomical alignment when more /
all segments are used for alignment.
%
For within-paradigm prediction, audio-description eventually outperforms
anatomical alignment (when more than 4-6 segments are used)
%

\todo[inline]{What might be the cause for that?}
%
In general, the outliers in audio PPA are difficult to estimate (via anatomical
alignment, too).
%
Maybe, the outliers fucked up the \ac{cfs}?


\paragraph{Conclusion on comparison of anatomical and functional alignment}

It's not that bad actually.


\subsubsection{Functional alignment: naturalistic stimuli \& partial alignment}

\todo[inline]{focus on what we did in the present study}
\todo[inline]{extend the idea to lots of "localizers" below in "Future studies";
but keep the "Vision" for the general discussion!}


\paragraph{Controlled paradigm vs. naturalistic stimuli}
%
Our results provide evidence that transformation matrices calculated based on
data from naturalistic stimuli [at least in case of the movie] promise an
increased validity for functional alignment over transformation matrices based
on data of a paradigm based on simplified stimuli.
%
This is the case for both within-paradigm prediction (e.g., audio-description
for alignment to estimate $Z$-maps from the audio-description's $t$-contrast)
and cross-paradigm prediction (e.g. audio-description for alignment to estimate
$Z$-maps from the visual localizer's $t$-contrast).

%
In case of cross-paradigm prediction, our results are different from findings of
\citet{haxby2011common} who found that within-paradigm prediction of $Z$-maps
from a non-naturalistic paradigm was as good or better than cross-paradigm
prediction via naturalistic stimulus for alignment.

%
However, they had a \ac{cfs} that was derived just from the non-naturalistic
paradigm, i.e. no multi-paradigm \ac{cfs}.
%
Maybe, our naturalistic stimuli (about 7000 TRs) fucked up our shared responses
in the 450 \acp{tr} within the SRM (imo, Fig.~\ref{fig:corr-vis-reg-srm}
suggests otherwise)


\paragraph{Partial Alignment}

\todo[inline]{"asymptotic relationship", "low ceiling", "fast saturation"?}

%
Results suggest that 15 [to 30] minutes of a movie (\ac{tr}=2s) of a movie
outperform an estimation via anatomical alignment.
%
However, there is no linear relationship between the quantity of data and the
prediction performance because 2 (or 3?) x 15 minutes of movie data did not lead
to significant performance increase over data from one (or 2?) movie segments.


\paragraph{The problem with the audio-description}

\begin{itemize}

    \item it is a "not-as-rich" stimulation as the movie?

    \item the auditory response is (too) different from the visual response?

    \item audio-description gave more room for individual variation of
        responses?

    \item some subjects where just not attending or even drove off to sleep?

\end{itemize}
%
We need---of course---further studies to assess the reliability of individual
differences in responses to stimulus features providing information about the
spatial layout of a naturalistic stimulus' scene.
%
Moreover, control confounding variables like alertness and attentiveness.


\subsection{Inference: calibration scan + database}

% A shared calibration scan across datasets could be used to transfer data
% between datasets, a procedure that is easier to accomplish than shared
% subjects across datasets \citep[cf.][an extension of the \ac{srm} for shared
% subjects across datasets]{zhang2018transfer}.

\todo[inline]{focus here more on what we did in the present study}

\todo[inline]{especially, "clinical context" belongs to general discussion}

\todo[inline]{text needs to match with general discussion's "Vision"}


\subsubsection{Intro}

\todo[inline]{define the terms "functional atlas" / "reference database" better}

\todo[inline]{"functional atlas" might be a little bold anyway}

%
Our results suggest that additional 15 minutes functional scanning using an
engaging naturalistic stimuli could provide sufficient data for a
\textit{calibration scan}.
%
Similar to a standard anatomical scan as part of main fMRI scan session, such an
calibration scan could align a subject to a \ac{cfs} derived from extensive
scans and analyses of other subjects' data.
%
The \ac{cfs} serves as an \textit{functional atlas} / \textit{reference
database}.


\subsubsection{The diagnostic run}
%
The diagnostic run should be based on a naturalistic stimulus because
naturalistic stimulus samples a broad range of brain states
\citep[cf.][]{guntupalli2016model, haxby2011common} and provide an increased
generalizability of transformations for functional alignment to the reference
database.
%
From a practical perspective, a naturalistic stimulus would lead to higher
engagement and better compliance \citep{vanderwal2015inscapes,
eickhoff2020towards} in, e.g. children or a patients/clinical population ``that
may have trouble maintaining attention during repetitions of a tedious localizer
task'' \citep{jiahui2020predicting}.

An engaging naturalistic stimulus as an calibration scan before the main
experiment would have the benefit of putting a study participant at ease and
letting the subject accommodate to the scanner environment.
%
Also possible but imo not as good: An engaging naturalistic stimulus after the
main experiment would not suffer less from fatigue than a one localizer or even
a localizer battery demanding voluntary attention and handling a repetitive task
(but prime the subject the stimulus features within the naturalistic stimulus).

%
The reference should---of course---be based on many subjects (maybe even
containing subgroups), and a collection of conducted paradigms that would enable
a mapping of that collection into individual persons.


\subsubsection{The database incl. dedicated paradigms}

\todo[inline]{repeat phrasing from introduction (cf. localizer paradigms)}

%
Because modeling of responses to specific stimulus features in a naturalistic
stimulus is fucking difficult, and the dedicated paradigms are, well, designed /
dedicated to probe specific brain process (and are the "gold standard").

%
The reference database could provide extent measurements from additional
paradigms that are dedicated and designed to probe specific brain functions like
category-selective regions \citep{stigliani2015temporal}, theory of mind
\citep{spunt2014validating}, or semantic processes \citep{fedorenko2010new,
fernandez2001language};
%
employ a range of dedicated stimuli and specific tasks to map processes of
``auditory and visual perception, motor actions, reading, language
comprehension, and mental calculation'' \citep{pinel2007fast}.


\subsubsection{Application: estimate regular pattern \& quantify deviant
pattern}

%
Functional patterns in response to a variety of paradigms could then be inferred
from an (independent) healthy or clinical subgroup and mapped into the voxel
space of an individual person, e.g.,
%
when additional functional scans are not possible due to time-limitations,
monetary constrains, scanner availability, fatigue, inadequate task demands, or
general compliance.
%
Another use case, is the assessment of a subject's actual (i.e. empirical)
functional pattern in respect to its similarity to  an estimated pattern
predicted from reference group of healthy individuals.
%
This would allow to quantify the deviation of the subject's empirical pattern to
the norm [or the similarity of a subject's empirical pattern to a non-normative
subgroup]


\paragraph{Example}

\todo[inline]{reduce abstract of \citet{yates2021emergence} to its gist}

%
For example, \citet{yates2021emergence} tested ``the presence, maturity, and
localization of adult functions in children using shared response modeling, a
machine learning approach for functional alignment.
%
After learning a lower-dimensional feature space from fMRI activity as adults
watched a movie, we translated these shared features into the anatomical brain
space of children 3--12 years old.
%
To evaluate functional maturity, we correlated this reconstructed activity with
childrens actual fMRI activity as they watched the same movie. We found
reliable correlations throughout cortex, even in the youngest children.
%
The strength of the correlation in the precuneus, inferior frontal gyrus, and
lateral occipital cortex predicted chronological age.
%
These age-related changes were driven by three types of developmental
trajectories: emergence from absence to presence, consistency in anatomical
expression, and reorganization from one anatomical region to another.
%
We also found evidence that the processing of pain-related events in the movie
underwent reorganization across childhood'' \citep{yates2021emergence}.


\subsection{Shortcomings \& future questions}

\subsubsection{Bigger sample size}
%
The SRM is computationally less demanding [= "computationally more efficient"?]
than hyperalignment, an advantage for scientists who want to replicate our
results but do not have access to a high-performance computer cluster [or
"high-throughput computer cluster" or simply "specialized hardware"?].
%
Moreover, it should scale pretty well (compared that to
\citep{jiahui2020predicting} 1-step alignment that needs pair-wise matrices for
subjects 'cause no \ac{cfs}; similarly, \citep{busch2021hybrid})\todo{check
Busch}.


\paragraph{...because our sample size was small with following effect}

\todo[inline]{cf. variability of present study's \acp{cfs}}

\todo[inline]{can/should that variability somehow be quantified?}

% what is the case
The correlations of shared responses within the \acp{cfs} created from $N-1$
training subjects varied across the folds of the cross-validation.
% interpretation
That means, a change of 1/13 of the data for every subject's analysis is causing
the estimates to vary [how much?].
% conclusion
Future studies, should create a \ac{cfs} based on data from more subjects and
investigate the relationship between number of participants, variability of
parameters, and estimation performance.



\subsubsection{Other ROIs than category-selective areas}

%
In the present study, we focused on the \ac{ppa} as an classic example of a
higher-visual area.


\paragraph{Studies on other functional areas}
%
However, studies have shown a varying degree of \textit{functional--anatomical
correspondence} between a brain function and its underlying anatomical location.
%
Previous studies that used either volume-based \citep{zhen2017quantifying,
zhen2015quantifying} or surface-based alignment \citep{rosenke2021probabilistic,
frost2012measuring} in order to estimate the most probable location of
functional area in a test subject have ``found a large variability in the degree
to which functional areas respect macro-anatomical boundaries across the
cortex'' \citep{frost2012measuring}.
%
``There is a strong structural-functional correspondence in some areas whilst in
others the spatial location of the functional area varies greatly across
subjects within a cortical area'' \citep{frost2012measuring}.
%
``Language areas were found to vary greatly across subjects whilst a high degree
of overlap was observed in sensory and motor areas'' \citep{frost2012measuring}.


``Generally, searchlights in the high-level visual areas and with strong
category-selectivity (e.g., ventral temporal cortex, lateral temporal cortex)
showed the highest mean correlation values, which often exceeded 0.8 (Figure 4,
Figure S6 \& 8). The lower mean correlations in other cortices (e.g.,
sensorimotor cortex) reflect low reliabilities of the localizer runs''
\citep{jiahui2022cross} (cf. also \citet{jiahui2020}, maybe also
\citet{feilong2022individualized}).

\paragraph{Example \ac{ppa} vs. \ac{ffa}}
%
For example, in domain of category-selective areas, the interindividual
variability varies across functional areas \citep{zhen2017quantifying,
zhen2015quantifying, frost2012measuring}:
%
``Scene-selective regions showed larger interindividual variability [after
nonlinear volume-based alignment] than the face-selective regions in spatial
topography'' \citep{zhen2017quantifying}.


\paragraph{Inference}

\todo[inline]{Mention whole-cortex (searchlight) alignment here already?
...instead of writing a dedicated subsection about "Other functional alignment
algorithms" below?}

\todo[inline]{topic for general discussion: what is the limit of "brain
functions" that we can estimate reliably using naturalistic stimuli (i.e.
executive functions, decision-making?)}

\todo[inline]{maybe, cite studies that did research on other brain functions
using naturalistic stimuli; or just cite a good review}

%
Hence, a new dataset should not just have more subjects but also more
localizers.
%
Then, future studies can investigate other functional domains.
%
Naturalistic stimuli have been used to investigate a variety of domains like XYZ
[check for, e.g., visual or auditory perception, spatial cognition; emotion;
music, speech or social perception], and possibly allow valid mapping of
functional data in all of these domains.


\subsubsection{\ac{srm} is problematic in case of atypical organization /
outliers}

\todo[inline]{example: papers / reviews about atypical language area topography
in patients (of various diseases)}

\todo[inline]{Check \citet{feilong2022individualized, jiahui2022cross,
turek2018capturing}; at least two of them have modeled an individual component.}

%
For every fold of the cross-validation, we calculated a \ac{cfs} from 13
subjects.
%
Especially in case of the audio-descriptions $Z$-maps, we observed variability
despite our cohort was homogeneous (young, healthy adults).
%
Estimating a atypical brain activation pattern has not been pretty successful in
the current study [which is somewhat more likely than that our modeling during
the audio-description was total garbage?].
%
Hence, our model and/or the matrices [?] are not super valid for the outliers in
our sample which limits the generalization of our model / matrices to people of
different ages (children or elderly) or a clinical population.
%
Our approach can be used to quantify the deviation of an empirically measured
pattern to a pattern estimated from a normative reference.
%

However, a drawback of the SRM algorithm we employed here is that it models
responses that are common across persons without an individual component.
%
A person's idiosyncratic responses are excluded from the shared response model
but ``are not necessarily noise and may in fact be highly reliable within
participants'' \citep{cohen2017computational}.
%
In order to predict a atypical pattern (and not just quantify the deviation from
norm), you need matching \ac{cfs} or---probably better---model has to have a
shared response + individual component + noise.


\paragraph{Templates that can probably be dropped}
%
``SRM can be used to isolate participant-unique responses by examining the
residuals after removing shared group responses, or it can be applied
hierarchically to the residuals to identify subgroups [\citet{chen2017shared}]
'' \citep{cohen2017computational}.
%
``In cases where each subject's unique response is of more interest than the
shared signal, SRM can be used to factor out the shared component thereby
isolating the idiosyncratic response for each subject
[\citep{chen2015reduced}]'' \citep{kumar2020brainiak}.
%
``Recognizing that signal exists beyond the average or shared response of a
group, such studies exploit idiosyncratic but stable responses to account for
previously unexplained variance in brain function, behavioral performance and
clinical measures [e.g., Finn (2015). Functional fingerprinting (based on
connectivity)]'' \citep{cohen2017computational}.




\subsubsection{Other functional alignment algorithms}

\todo[inline]{Evtl. kann man alle Punkte dieser Subsubsection bereits oben
verwursten; hier kommen die Punkte eh so ein wenig zusammenhangslos als
selbstzweck und ziehen allgemeine ganz am Ende die Ergebnisse runter (``a.k.a.
"warum hast du nicht gleich Algo XY benutzt?'')}


\subsubsection{ROI vs. whole-brain (i.e. searchlight)}

\todo[inline]{Maybe, shift that to "future studies on other ROIs"}

\todo[inline]{searchlight SRM \citep{zhang2016searchlight}}

\todo[inline]{negative: more parameters to vary and to assess}

``Searchlight functional alignment [\citep{zhang2016searchlight,
guntupalli2016model}] learns local transformations and aggregates them into a
single large-scale alignment.
%
The searchlight scheme [Kriegeskorte, 2006, Information-based functional brain
mapping], popular in brain imaging [Guntupalli et al., 2018; 2016], has been
used as a way to divide the cortex into small overlapping spheres of a field
radius.
%
This method allows researchers to remain agnostic as to the location of
functional or anatomical boundaries, such as those suggested by
parcellation-based approaches.
%
A local transform can then be learned in each sphere and the full alignment is
obtained by aggregating (e.g. summing as in \citep{guntupalli2016model} or
averaging) across overlapping transforms.
%
The aggregated transformation produced is no longer guaranteed to bear the type
of regularity (e.g orthogonality, isometry, or diffeomorphicity enforced during
the local neighborhood fit)'' \citep{bazeille2021empirical}.
%
``In the case of searchlight Procrustes, we selected searchlight parameters to
match those used in Guntupalli et al. (2016):
%
each searchlight had 5 voxel radius, with a 3 voxel distance between searchlight
centers'' \citep{bazeille2021empirical}.


\subsubsection{Time series vs connectivity-based}

\todo[inline]{kind of a killer cause you do not need intersection of
time series}

\todo[inline]{wtf did \citep{nastase2019leveraging} do?}

\todo[inline]{imo, this should not be a stand-alone paragraph; mention somewhere
before in discussion (future studies on other functional areas?) and possibly in
the intro that response-based functional alignment works better to predict
response}

\todo[inline]{Main message (for general discussion) regarding naturalistic
stimuli stays the same: naturalistic is better than resting-state; does not
matter if functional alignment based on response or connectivity (cf.
\citep{haxby2020hyperalignment}}


``Hyperalignment projects cortical pattern vectors into a common,
high-dimensional information space \citep{haxby2020hyperalignment}.
%
Derivation of this common space can be based on either neural response profiles
(e.g. data collected during tasks, such as movie viewing (Haxby et al., 2011))
or functional connectivity profiles files \citep{guntupalli2018computational}''
\citep{busch2021hybrid}.

``The number of voxels that can be considered simultaneously for functional BOLD
response time series alignment is limited by the number of timepoints in the
calibration scan (about 300-400 voxels for a 15min scan with a 2s TR,
corresponding to a local cortical neighborhood of about 1cm in diameter for a
standard resolution).
%
This limitation does not exist in this form for a functional alignment that is
based on connectivity vectors.
%
The length of these connectivity vectors is determined by the number of
reference (or seed) regions in the brain'' [project proposal].

% Kumar on Nastase's ugly mofo paper
``Estimating the SRM from functional connectivity data rather than response time
series circumvents the need for a single shared stimulus across subjects.
%
Connectivity SRM allows us to derive a single shared response space across
different stimuli with a shared connectivity profile
\citep{nastase2019leveraging}'' \citep{kumar2020brainiak}.

%
``The sampling of connectivity vector space is defined by the selection of
connectivity targets, but the richness and reliability of connectivity estimates
depends on the variety of brain states over which connectivity is estimated''
\citep{haxby2020hyperalignment}.

%
``Both connectivity hyperalignment and response hyperalignment increased ISCs
and bsMVPC classification accuracies significantly over anatomy-based alignment,
but each algorithm achieves better alignment for the information that it uses to
derive a common model, namely connectivity profiles and patterns of response,
respectively'' \citep{guntupalli2018computational}.


\subsubsection{Volume- vs surface-based}

\todo[inline]{An "issue" (at least for reviewers) that is not really an issue}

\todo[inline]{we are nearer on the raw data 'cause we work with voxels (surface
vertices need additional mapping of $Z$-maps calculated from smoothed data), and
both our anatomical and functional alignment use voxels as input (i.e. it is not
mixed)}

\todo[inline]{$Z$-maps of localizer were calculated in voxel space; cf. general
discussion (opportunity costs)}

\todo[inline]{funnily, most studies  that estimated functional areas compared
surface-based alignment to affine volume-based alignment; iirc,
\citet{rosenke2021probabilistic} was the exception}

We compare volume-based [nonlinear] anatomical alignment to volume-based
functional alignment.
%
Future work could compare surface-based alignment that respects cortical folding
structure -- that out-performs predictions based on [affine] volume-based
anatomical alignment \citep{weiner2018defining} -- to surface-based functional
alignment.


\subsubsection{Studyforrest dataset}

\paragraph{other localizer t-contrasts}

\todo[inline]{Does it make sense to discuss that?}

\todo[inline]{It's such a low-hanging fruit (more like bachelor thesis'
project); which might be good or bad for mentioning it)}

The studyforrest dataset's visual localizer \citep{sengupta2016extension} offers
other contrasts (and thus \acp{roi} masks) aimed at localizing the \ac{ffa} and
\ac{ofa} that are associated with face perception \citep{kanwisher1997ffa,
pitcher2011occipitalfacearea}, the \ac{eba} that is associated with the
perception of human bodies \citep{downing2001bodyarea}, and the \ac{loc} that is
associated with the perception of (small) objects (like tools or toys)
\citep{malach1995loc}.



\subsection{Conclusion}

God bless America!


\section{Data Availability}

\todo[inline]{all from PPA-Paper but with new GIN link leading to an empty repo}

% \href{https://gin.g-node.org/chaeusler/studyforrest-ppa-analysis}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-analysis}}

% new; PPA analysis
All fMRI data and results are available as Datalad \citep{halchenko2021datalad}
datasets, published to or linked from the \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).
% original
Raw data of the audio-description, movie and visual localizer were originally
published on the \emph{OpenfMRI} portal
(\url{https://legacy.openfmri.org/dataset/ds000113}; \citep{Hanke2014ds000113},
\space \url{https://legacy.openfmri.org/dataset/ds000113d};
\citep{hanke2016ds000113d}).
% visual localizer
Results from the localization of higher visual areas are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-visualrois}{\url{github.com/psychoinformatics-de/studyforrest-data-visualrois}}).
% raw data
The realigned participant-specific time series that were used in the current
analyses were derived from the raw data releases and are available as Datalad
datasets at \emph{GitHub}
(\href{https://github.com/psychoinformatics-de/studyforrest-data-aligned}{\url{github.com/psychoinformatics-de/studyforrest-data-aligned}}).
% OpenNeuro
The same data are available in a modified and merged form on OpenNeuro at
\url{https://openneuro.org/datasets/ds000113}.
% NeuroVault for z-maps of SRM
Unthresholded $Z$-maps of all contrasts can be found at
\href{https://identifiers.org/neurovault.collection:12340}{\url{neurovault.org/collections/12340}}.


\section*{Code Availability}

Scripts to generate the results as Datalad \citep{halchenko2021datalad} datasets
are available in a \emph{G-Node GIN} repository
(\href{https://gin.g-node.org/chaeusler/studyforrest-ppa-srm}{\url{gin.g-node.org/chaeusler/studyforrest-ppa-srm}}).



\section{Backup of texts regarding "done but not mentioned"}

\subsection{Calculate $Z$-maps mean in the common space already}
%
I also tested averaging $Z$-maps in the \ac{cfs} (i.e.: not in the test
subject's voxel space): similar results
%
In case of anatomical alignment, I did
not test averaging data in MNI152 space.

\subsection{Calculate $Z$-map from training subjects' TRs in FEAT}

iirc, I projected all subjects' localizer time series through
model space into test subject voxel space; then, calculated the contrast
with these data (s. scripts 'test/data\_denoise-vis.py' \&
'test/data\_srm-vis-to-ind.py').
%

The problem was: if one wants to test the different transformation matrices (I
only did it with one; imo, based on alignment using the whole audio-description)
it gets totally messy \& computational intensive.
%
Results were similar to the original procedure if not slightly worse.



\pagebreak

\section{Templates for Cronbach's $\alpha$}


\paragraph{\citet{jiahui2020predicting}}
%
``We compared the correlations between maps estimated from a participant's own
data and maps estimated from other participants' data to the reliability of the
localizer (across the four localizer runs).
%
The mean Cronbach's Alpha between the four localizer runs was 0.60 (N = 15, S.D.
= 0.14)'' \citep{jiahui2020predicting}.
%
``Results mean that if we scan each participant for another 4 localizer
runs, and compute the correlation between the two maps (4 runs vs. 4 runs), the
correlation would be 0.60 on average in the studyforrest''
\citep{jiahui2020predicting}.
%
``Cronbach's alpha indicates that the predicted contrast map based on
hyperalignment is close to or as good as the real contrast map based on four
localizer runs (studyforrest: t(14) = 0.61, p = 0.55; Grand Budapest: t(20) =
3.02, p = 0.007)'' \citep{jiahui2020predicting}.
%
``The predicted contrast map based on hyperalignment was better than the
contrast map based on data from three out of four localizer runs in other
participants (t(14) = 2.36, p = 0.03) and in Grand Budapest, the predicted
contrast map was comparable to the contrast map based on three localizer runs in
other participants (t(20) = 0.48, p = 0.63)''
\citep{jiahui2020predicting}.


\paragraph{\citet{jiahui2022cross}}
%
``Because the localizer task comprises several scanning runs, we calculated the
reliability of the localizer across runs with Cronbach's alpha to provide an
estimate of the noise ceiling for these correlations'' \citep{jiahui2022cross}.
%
``We compared the correlations between topographies estimated from a
participant's own localizer data and those from other participants' data to the
reliability of the localizer, calculated with Cronbach's alpha.
%
Predictions made with hyperalignment were close to and sometimes even exceeded
the reliability values (Figure 1B).
%
This indicates that the predicted category-selective topographies from other
participants' data using hyperalignment were as precise and sometimes even
better than the topographies estimated with their own localizer data''
\citep{jiahui2022cross}.


\paragraph{\citet{feilong2022individualized}}
%
``Due to the presence of noise in localizer data, the estimated face-selectivity
map is a combination of a true face-selectivity map of the participant and
some noise.
%
The component from the true map is supposed to be shared by all localizer
runs, and thus the data quality and the level of noise can be estimated based on
the similarity between the 4 maps (i.e., one from each run).
%
We used Cronbach's alpha to estimate the reliability  of the average map of the
4 runs.
%
We estimated the reliability of the localizer-based map using Cronbach's alpha,
which is the expected correlation between two average maps, each based on 4 runs
of independent data.
%
If we were to collect another 4 localizer runs from the participant and get a
new average map based on the 4 new runs (i.e., independent data), then the
expected correlation between the two average maps would be Cronbach's alpha.
%
In other words, if the correlation between the model-predicted map and the
localizer-based (average) map is higher than Cronbach's alpha, then the
model-predicted map is more accurate than the average map based on 4 runs''
\citep{feilong2022individualized}.

%
``Based on the similarity between these four maps, we computed the Cronbach's
alpha coefficient for each participant, which estimates the reliability of the
average map.
%
That is, if we were to scan the participant for another four localizer runs and
correlate the new average map with the current average map, the expected
correlation would be Cronbach's alpha.'' \citep{feilong2022individualized}

%
``For both datasets, the localizer-based and model-predicted face-selectivity
maps were highly correlated (Forrest: r = 0.618  0.089 [mean  SD], Raiders: r
= 0.716  0.074), and the correlations were higher than our previous
state-of-the-art model using the same dataset and hyperalignment (Jiahui et al.,
2020).
%
Across all participants, the average Cronbach's alpha was 0.606  0.126 for
Forrest, and 0.764  0.089 for Raiders.
%
For approximately a third of the participants (Forrest: 6 out of 15, 40\%;
Raiders: 6 out of 20, 30\%), the correlation exceeded the Cronbach's alpha of
localizer-based maps.
%
In other words, for these participants, the predicted map based on our model can
be more accurate than the map based on a typical localizer scanning session
comprising four runs'' \citep{feilong2022individualized}.

%
``Note that for the Forrest dataset, the similarity sometimes exceeded
Cronbach's alpha, which means the model-predicted map is more accurate than a
map based on 4 localizer runs (21 minutes).
%
The quality of localizer-based maps increases with more localizer data, which
can be estimated using the SpearmanBrown prediction formula [Brown, 1910;
Spearman, 1910].
%
Based on the SpearmanBrown prediction formula, we can estimate how Cronbach's
alpha changes with the amount of data (i.e., the number of localizer runs), and
correspondingly, how much localizer data is needed to achieve the quality of the
model-predicted map
%
For the Forrest dataset, the maps predicted by 15, 30, 60, and 120 minutes of
movie data were as accurate as 17, 22, 26, and 30 minutes of localizer data,
respectively.
%
For the Raiders dataset, the maps predicted by 15, 28, and 56 minutes of movie
data were as accurate as 10, 11 and 12 minutes of localizer data, respectively''
\citep{feilong2022individualized}


\pagebreak

\section{Templates from \citet{haxby2011common} on validity \& generalizability}

%
``The algorithm also can be applied to simpler, controlled experimental data but
our previous results showed that the sampling of response vectors from these
experiments is impoverished and produces a model representational space that
does not generalize well to new stimuli in other experiments
\citep{haxby2020hyperalignment}'' \citep{guntupalli2016model}.

%
``We base the derivation of the transformation matrices and the common space on
responses to the movie---a complex, naturalistic, dynamic stimulus.
%
Although the algorithm also can be applied to fMRI data from more controlled
experiments, we found that a common model based on such data has greatly
diminished general validity \citep{haxby2011common}, presumably because,
relative to a rich and dynamic naturalistic stimulus, such experiments sample an
impoverished range of brain states \citep{guntupalli2016model}''.

``The general validity of the model across the varied stimulus sets that we
tested could be achieved only when hyperalignment was based on responses to the
movie.
%
Common models based on responses to smaller, more controlled stimulus
sets---still images of a limited number of categories---were valid only for
restricted stimulus domains, indicating that these models captured only a
subspace of the substantially larger representational space in VT cortex''
\citep{haxby2011common}.

``We used a complex and dynamic natural stimulus---a full-length action
movie---to sample a diverse variety of representational states.
%
The results show that hyperalignment based on responses to this stimulus affords
a single model of VT cortex with general validity across a broad range of
stimuli, whereas hyperalignment based on responses to still images in more
controlled, conventional experiments does not.
%
Thus, by virtue of the rich diversity of a complex, natural stimulus, our model
of the representational space in VT cortex also has general validity across
stimuli''\citep{haxby2011common}.

``We also derived common models based on responses to the face and object
categories in ten subjects and on responses to the pictures of animals in 11
subjects.
%
These alternative common models afforded high levels of accuracy for BSC of the
stimulus categories used to derive the common space but did not generalize to
BSC for the movie time segments.
%
Thus, models based on hyperalignment of responses to a limited number of
stimulus categories align only a small subspace within the representational
space in VT cortex and are, therefore, inadequate as general models of that
space.
%
On the positive side, these results also show that hyperalignment can be used
for BSC of an fMRI experiment without data from movie viewing''
\citep{haxby2011common}.

%
``Further analyses revealed other desirable properties of the movie as a
stimulus for model derivation.
%
The movie evoked responses in VT cortex that were more distinctive than were
responses to the still images in the category perception experiments.
%
Moreover, the general validity of the model based on the responses to the movie
is not dependent on responses to stimuli that are in both the movie and the
category perception experiments but, rather, appears to rest on stimulus
properties that are more abstract and of more general utility''
\citep{haxby2011common}.

%
``We investigated whether hyperalignment of the face and object data and
hyperalignment of the animal species data would afford high levels of BSC
accuracy using only the data from those experiments.
%
In each experiment, we derived a common space based on all runs but one. We
transformed the data from all runs, including the left-out run, into this common
space.
%
We trained the classifier on those runs used for hyperalignment in all subjects
but one and tested the classifier on the data from the left-out run in the
left-out subject (i.e. the test data for determining classifier accuracy played
no role either in hyperalignment or in classifier training [Kriegeskorte et al.,
2009]).
%
BSC of face and object categories after hyperalignment based on data from that
experiment was equivalent to BSC after movie-based hyperalignment (62.9\% 
2.9\% versus 63.9\%  2.2\%, respectively; Figure 4).
%
BSC of the animal species after hyperalignment based on data from that
experiment was significantly better than BSC after movie-based hyperalignment
(76.2\%  3.7\% versus 68.0\%  2.8\%, respectively; p < 0.05; Figure 4).
%
Result suggests that the validity for a model of a specific subspace may be
enhanced by designing a stimulus paradigm that samples the brain states in that
subspace more extensively'' \citep{haxby2011common}.

%
``We next asked whether hyperalignment based on these simpler stimulus sets was
sufficient to derive a common space with general validity across a wider array
of complex stimuli.
%
We applied the hyperalignment parameters derived from the face and object data
to the movie data in the ten Princeton subjects and the hyperalignment
parameters derived from the animal species data to the movie data in the 11
Dartmouth subjects.
%
BSC of 18s movie time segments after hyperalignment based on category perception
experiment data was markedly worse than BSC after hyperalignment based on movie
data (17.6\%  1.3\% versus 65.8\%  2.7\% for Princeton subjects; 28.3\% 
2.8\% versus 74.9\%  4.1\% for Dartmouth subjects; p < 0.001 in both cases;
Figure 4).
%
Thus, hyperalignment of data using a set of stimuli that is less diverse than
the movie is effective, but the resultant common space has validity that is
limited to a small subspace of the representational space in VT cortex''
\citep{haxby2011common}.

%
``We also tested whether the general validity of the model space reflects
responses to stimuli that are in both the movie and the category perception
experiments or reflects stimulus properties that are not specific to these
stimuli.
%
We recomputed the common model after removing all movie time points in which a
monkey, a dog, an insect, or a bird appeared. We also removed time points for
the 30 s that followed such episodes to factor out effects of delayed
hemodynamic responses.
%
BSC of the face and object and animal species categories, including distinctions
among monkeys, dogs, insects, and birds, was not affected by removing these time
points from the movie data [65.0\%  1.9\% versus 64.8\%  2.3\% for faces and
objects; 67.1\%  3.0\% versus 67.6\%  3.1\% for animal species; Figure S4B].
%
This result suggests that the movie-based hyperalignment parameters that afford
generalization to these stimuli are not stimulus specific but, rather, reflect
stimulus properties that are more abstract and of more general utility for
object representations'' \citep{haxby2011common}.


\subsubsection{Shortcoming: leakage of test data in union of individual
\acp{ppa}}

\todo[inline]{yeah, whatever}
%
We used the union of individual \acp{ppa} as spatial constrain for $Z$-maps.
%
But we have a leakage of test data (test subject is in data for the mask).
%
We might miss some voxels (of some participants) at the borders of the \ac{roi},
because the subject-specific, binary masks are based on a ("titrated")
threshold.  \citep{sengupta2016extension}
%
In the future, an independent probabilistic atlas should be used, the \ac{roi}
dilated, [and a separate model calculated for each hemisphere].
